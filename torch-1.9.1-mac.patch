From 2418d58b9acd76034f4882e636e4cfed4e7ffd9e Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Tue, 26 Oct 2021 09:24:35 +0800
Subject: [PATCH 1/2] orlando - for fixing to enable pytorch 1.9.1 on macOS
 10.3.6

---
 aten/src/ATen/test/CMakeLists.txt  |   2 +-
 caffe2/CMakeLists.txt              |   3 +
 modules/detectron/CMakeLists.txt   |   6 +-
 test/cpp/api/CMakeLists.txt        |   6 +-
 torch/distributed/rpc/__init__.py  |   7 +-
 torch/distributed/rpc/constants.py |   7 +-
 torch/distributed/rpc/options.py   | 265 +++++++++++++++--------------
 7 files changed, 156 insertions(+), 140 deletions(-)

diff --git a/aten/src/ATen/test/CMakeLists.txt b/aten/src/ATen/test/CMakeLists.txt
index 24ab67da50..4d863c3029 100644
--- a/aten/src/ATen/test/CMakeLists.txt
+++ b/aten/src/ATen/test/CMakeLists.txt
@@ -18,7 +18,7 @@ list(APPEND ATen_CPU_TEST_SRCS
   ${CMAKE_CURRENT_SOURCE_DIR}/dlconvertor_test.cpp
   ${CMAKE_CURRENT_SOURCE_DIR}/native_test.cpp
   ${CMAKE_CURRENT_SOURCE_DIR}/scalar_tensor_test.cpp
-  ${CMAKE_CURRENT_SOURCE_DIR}/test_parallel.cpp
+  # ${CMAKE_CURRENT_SOURCE_DIR}/test_parallel.cpp
   ${CMAKE_CURRENT_SOURCE_DIR}/undefined_tensor_test.cpp
   ${CMAKE_CURRENT_SOURCE_DIR}/verify_api_visibility.cpp
   ${CMAKE_CURRENT_SOURCE_DIR}/thread_init_test.cpp
diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index 50ebb224ce..328551df56 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -1813,6 +1813,9 @@ if(BUILD_PYTHON)
   add_library(caffe2_pybind11_state MODULE ${Caffe2_CPU_PYTHON_SRCS})
   if(USE_NUMPY)
     target_compile_options(caffe2_pybind11_state PRIVATE "-DUSE_NUMPY")
+    # Orlando; refer to how to fix issue: ../caffe2/python/pybind_state.h:27:10: fatal error: 'numpy/arrayobject.h' file not found
+    find_package(Python3 REQUIRED COMPONENTS NumPy)
+    target_include_directories(caffe2_pybind11_state PRIVATE ${Python3_NumPy_INCLUDE_DIRS})
   endif()
   if(NOT MSVC)
     set_target_properties(caffe2_pybind11_state PROPERTIES COMPILE_FLAGS "-fvisibility=hidden")
diff --git a/modules/detectron/CMakeLists.txt b/modules/detectron/CMakeLists.txt
index 8041e71d35..7c2ece9b37 100644
--- a/modules/detectron/CMakeLists.txt
+++ b/modules/detectron/CMakeLists.txt
@@ -4,7 +4,11 @@ file(GLOB_RECURSE Detectron_HIP_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/*.hip)
 
 if(BUILD_CAFFE2_OPS)
   if(USE_OPENMP AND OPENMP_FOUND)
-    Set(OpenMP_link ${OpenMP_CXX_LIBRARIES})
+      if (${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
+        Set(OpenMP_link -Xpreprocessor -fopenmp /Users/llv23/opt/miniconda3/lib/libomp.dylib /Users/llv23/opt/miniconda3/lib/libgomp.dylib)
+      else()
+        Set(OpenMP_link ${OpenMP_CXX_LIBRARIES})
+      endif()
   endif()
 
   # Note(ilijar): Since Detectron ops currently have no
diff --git a/test/cpp/api/CMakeLists.txt b/test/cpp/api/CMakeLists.txt
index ebc3dd5192..59e723ee56 100644
--- a/test/cpp/api/CMakeLists.txt
+++ b/test/cpp/api/CMakeLists.txt
@@ -45,7 +45,11 @@ if(USE_CUDA)
 endif()
 
 add_executable(test_api ${TORCH_API_TEST_SOURCES})
-target_include_directories(test_api PRIVATE ${ATen_CPU_INCLUDE})
+if (${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
+  target_link_libraries(test_api PRIVATE torch gtest -Xpreprocessor -fopenmp /Users/llv23/opt/miniconda3/lib/libomp.dylib /Users/llv23/opt/miniconda3/lib/libgomp.dylib)
+else()
+  target_link_libraries(test_api PRIVATE torch gtest)
+endif()
 target_link_libraries(test_api PRIVATE torch gtest)
 
 if(USE_CUDA)
diff --git a/torch/distributed/rpc/__init__.py b/torch/distributed/rpc/__init__.py
index 5429da695e..d80359c251 100644
--- a/torch/distributed/rpc/__init__.py
+++ b/torch/distributed/rpc/__init__.py
@@ -24,6 +24,7 @@ if is_available() and not torch._C._rpc_init():
 
 if is_available():
     from torch._C._distributed_c10d import Store
+## For not using USE_TENSORPIPE for torch-1.9.1 hotfix on macOS
     from torch._C._distributed_rpc import (
         _disable_jit_rref_pickle,
         _enable_jit_rref_pickle,
@@ -48,17 +49,17 @@ if is_available():
         get_rpc_timeout,
         enable_gil_profiling,
         RpcBackendOptions,
-        _TensorPipeRpcBackendOptionsBase,
+        # _TensorPipeRpcBackendOptionsBase,
         ProcessGroupRpcBackendOptions,
         RpcAgent,
         PyRRef,
         ProcessGroupAgent,
-        TensorPipeAgent,
+        # TensorPipeAgent,
         RemoteProfilerManager,
         WorkerInfo,
         _DEFAULT_INIT_METHOD,
         _DEFAULT_NUM_SEND_RECV_THREADS,
-        _DEFAULT_NUM_WORKER_THREADS,
+        # _DEFAULT_NUM_WORKER_THREADS,
         _UNSET_RPC_TIMEOUT,
         _DEFAULT_RPC_TIMEOUT_SEC,
     )  # noqa: F401
diff --git a/torch/distributed/rpc/constants.py b/torch/distributed/rpc/constants.py
index e6d79e6e59..9f930ae00c 100644
--- a/torch/distributed/rpc/constants.py
+++ b/torch/distributed/rpc/constants.py
@@ -3,7 +3,8 @@ from datetime import timedelta
 from torch._C._distributed_rpc import (
     _DEFAULT_INIT_METHOD,
     _DEFAULT_NUM_SEND_RECV_THREADS,
-    _DEFAULT_NUM_WORKER_THREADS,
+    ## For not using USE_TENSORPIPE for torch-1.9.1 hotfix on macOS
+    # _DEFAULT_NUM_WORKER_THREADS,
     _DEFAULT_RPC_TIMEOUT_SEC,
     _UNSET_RPC_TIMEOUT,
 )
@@ -17,7 +18,9 @@ DEFAULT_SHUTDOWN_TIMEOUT: float = 5.0
 # For ProcessGroupAgent.
 DEFAULT_NUM_SEND_RECV_THREADS: int = _DEFAULT_NUM_SEND_RECV_THREADS
 # For TensorPipeAgent.
-DEFAULT_NUM_WORKER_THREADS: int = _DEFAULT_NUM_WORKER_THREADS
+## For not using USE_TENSORPIPE for torch-1.9.1 hotfix on macOS
+# DEFAULT_NUM_WORKER_THREADS: int = _DEFAULT_NUM_WORKER_THREADS
+DEFAULT_NUM_WORKER_THREADS: int = 16
 # Ensure that we don't time out when there are long periods of time without
 # any operations against the underlying ProcessGroup.
 DEFAULT_PROCESS_GROUP_TIMEOUT: timedelta = timedelta(milliseconds=2 ** 31 - 1)
diff --git a/torch/distributed/rpc/options.py b/torch/distributed/rpc/options.py
index 0c32a57731..ffb512eb30 100644
--- a/torch/distributed/rpc/options.py
+++ b/torch/distributed/rpc/options.py
@@ -1,4 +1,5 @@
-from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase
+## For not using USE_TENSORPIPE for torch-1.9.1 hotfix on macOS
+# from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase
 from . import constants as rpc_contants
 
 import torch
@@ -39,134 +40,134 @@ def _to_device_list(devices: List[DeviceType]) -> List[torch.device]:
     return list(map(_to_device, devices))
 
 
-
-class TensorPipeRpcBackendOptions(_TensorPipeRpcBackendOptionsBase):
-    r"""
-    The backend options for
-    :class:`~torch.distributed.rpc.TensorPipeAgent`, derived from
-    :class:`~torch.distributed.rpc.RpcBackendOptions`.
-
-    Args:
-        num_worker_threads (int, optional): The number of threads in the
-            thread-pool used by
-            :class:`~torch.distributed.rpc.TensorPipeAgent` to execute
-            requests (default: 16).
-        rpc_timeout (float, optional): The default timeout, in seconds,
-            for RPC requests (default: 60 seconds). If the RPC has not
-            completed in this timeframe, an exception indicating so will
-            be raised. Callers can override this timeout for individual
-            RPCs in :meth:`~torch.distributed.rpc.rpc_sync` and
-            :meth:`~torch.distributed.rpc.rpc_async` if necessary.
-        init_method (str, optional): The URL to initialize the distributed
-            store used for rendezvous. It takes any value accepted for the
-            same argument of :meth:`~torch.distributed.init_process_group`
-            (default: ``env://``).
-        device_maps (Dict[str, Dict], optional): Device placement mappings from
-            this worker to the callee. Key is the callee worker name and value
-            the dictionary (``Dict`` of ``int``, ``str``, or ``torch.device``)
-            that maps this worker's devices to the callee worker's devices.
-            (default: ``None``)
-        devices (List[int, str, or ``torch.device``], optional): all local
-            CUDA devices used by RPC agent. By Default, it will be initialized
-            to all local devices from its own ``device_maps`` and corresponding
-            devices from its peers' ``device_maps``. When processing CUDA RPC
-            requests, the agent will properly synchronize CUDA streams for
-            all devices in this ``List``.
-    """
-    def __init__(
-        self,
-        *,
-        num_worker_threads: int = rpc_contants.DEFAULT_NUM_WORKER_THREADS,
-        rpc_timeout: float = rpc_contants.DEFAULT_RPC_TIMEOUT_SEC,
-        init_method: str = rpc_contants.DEFAULT_INIT_METHOD,
-        device_maps: Optional[Dict[str, Dict[DeviceType, DeviceType]]] = None,
-        devices: Optional[List[DeviceType]] = None,
-        _transports: List = None,
-        _channels: List = None,
-    ):
-        full_device_maps = (
-            {} if device_maps is None else
-            {k : _to_device_map(v) for k, v in device_maps.items()}
-        )
-        full_device_list = (
-            [] if devices is None else
-            _to_device_list(devices)
-        )
-        super().__init__(
-            num_worker_threads,
-            _transports,
-            _channels,
-            rpc_timeout,
-            init_method,
-            full_device_maps,
-            full_device_list,
-        )
-
-    def set_device_map(self, to: str, device_map: Dict[DeviceType, DeviceType]):
-        r"""
-        Set device mapping between each RPC caller and callee pair. This
-        function can be called multiple times to incrementally add
-        device placement configurations.
-
-        Args:
-            worker_name (str): Callee name.
-            device_map (Dict of int, str, or torch.device): Device placement
-                mappings from this worker to the callee. This map must be
-                invertible.
-
-        Example::
-            >>> # both workers
-            >>> def add(x, y):
-            >>>     print(x)  # tensor([1., 1.], device='cuda:1')
-            >>>     return x + y, (x + y).to(2)
-            >>>
-            >>> # on worker 0
-            >>> options = TensorPipeRpcBackendOptions(
-            >>>     num_worker_threads=8,
-            >>>     device_maps={"worker1": {0: 1}}
-            >>>     # maps worker0's cuda:0 to worker1's cuda:1
-            >>> )
-            >>> options.set_device_map("worker1", {1: 2})
-            >>> # maps worker0's cuda:1 to worker1's cuda:2
-            >>>
-            >>> rpc.init_rpc(
-            >>>     "worker0",
-            >>>     rank=0,
-            >>>     world_size=2,
-            >>>     backend=rpc.BackendType.TENSORPIPE,
-            >>>     rpc_backend_options=options
-            >>> )
-            >>>
-            >>> x = torch.ones(2)
-            >>> rets = rpc.rpc_sync("worker1", add, args=(x.to(0), 1))
-            >>> # The first argument will be moved to cuda:1 on worker1. When
-            >>> # sending the return value back, it will follow the invert of
-            >>> # the device map, and hence will be moved back to cuda:0 and
-            >>> # cuda:1 on worker0
-            >>> print(rets[0])  # tensor([2., 2.], device='cuda:0')
-            >>> print(rets[1])  # tensor([2., 2.], device='cuda:1')
-        """
-        full_device_map = _to_device_map(device_map)
-        curr_device_maps = super().device_maps
-
-        if to in curr_device_maps:
-            for k, v in full_device_map.items():
-                if k in curr_device_maps[to] and v != curr_device_maps[to][k]:
-                    raise ValueError(
-                        "`set_device_map` only supports 1-to-1 mapping, trying"
-                        f" to map {k} to {v} and {curr_device_maps[to][k]}"
-                    )
-
-        super()._set_device_map(to, full_device_map)
-
-    def set_devices(self, devices: List[DeviceType]):
-        r"""
-        Set local devices used by the TensorPipe RPC agent. When processing
-        CUDA RPC requests, the TensorPipe RPC agent will properly synchronize
-        CUDA streams for all devices in this ``List``.
-
-        Args:
-            devices (List of int, str, or torch.device): local devices used by
-                the TensorPipe RPC agent.
-        """
-        self.devices = _to_device_list(devices)
+## For not using USE_TENSORPIPE for torch-1.9.1 hotfix on macOS
+# class TensorPipeRpcBackendOptions(_TensorPipeRpcBackendOptionsBase):
+#     r"""
+#     The backend options for
+#     :class:`~torch.distributed.rpc.TensorPipeAgent`, derived from
+#     :class:`~torch.distributed.rpc.RpcBackendOptions`.
+
+#     Args:
+#         num_worker_threads (int, optional): The number of threads in the
+#             thread-pool used by
+#             :class:`~torch.distributed.rpc.TensorPipeAgent` to execute
+#             requests (default: 16).
+#         rpc_timeout (float, optional): The default timeout, in seconds,
+#             for RPC requests (default: 60 seconds). If the RPC has not
+#             completed in this timeframe, an exception indicating so will
+#             be raised. Callers can override this timeout for individual
+#             RPCs in :meth:`~torch.distributed.rpc.rpc_sync` and
+#             :meth:`~torch.distributed.rpc.rpc_async` if necessary.
+#         init_method (str, optional): The URL to initialize the distributed
+#             store used for rendezvous. It takes any value accepted for the
+#             same argument of :meth:`~torch.distributed.init_process_group`
+#             (default: ``env://``).
+#         device_maps (Dict[str, Dict], optional): Device placement mappings from
+#             this worker to the callee. Key is the callee worker name and value
+#             the dictionary (``Dict`` of ``int``, ``str``, or ``torch.device``)
+#             that maps this worker's devices to the callee worker's devices.
+#             (default: ``None``)
+#         devices (List[int, str, or ``torch.device``], optional): all local
+#             CUDA devices used by RPC agent. By Default, it will be initialized
+#             to all local devices from its own ``device_maps`` and corresponding
+#             devices from its peers' ``device_maps``. When processing CUDA RPC
+#             requests, the agent will properly synchronize CUDA streams for
+#             all devices in this ``List``.
+#     """
+#     def __init__(
+#         self,
+#         *,
+#         num_worker_threads: int = rpc_contants.DEFAULT_NUM_WORKER_THREADS,
+#         rpc_timeout: float = rpc_contants.DEFAULT_RPC_TIMEOUT_SEC,
+#         init_method: str = rpc_contants.DEFAULT_INIT_METHOD,
+#         device_maps: Optional[Dict[str, Dict[DeviceType, DeviceType]]] = None,
+#         devices: Optional[List[DeviceType]] = None,
+#         _transports: List = None,
+#         _channels: List = None,
+#     ):
+#         full_device_maps = (
+#             {} if device_maps is None else
+#             {k : _to_device_map(v) for k, v in device_maps.items()}
+#         )
+#         full_device_list = (
+#             [] if devices is None else
+#             _to_device_list(devices)
+#         )
+#         super().__init__(
+#             num_worker_threads,
+#             _transports,
+#             _channels,
+#             rpc_timeout,
+#             init_method,
+#             full_device_maps,
+#             full_device_list,
+#         )
+
+#     def set_device_map(self, to: str, device_map: Dict[DeviceType, DeviceType]):
+#         r"""
+#         Set device mapping between each RPC caller and callee pair. This
+#         function can be called multiple times to incrementally add
+#         device placement configurations.
+
+#         Args:
+#             worker_name (str): Callee name.
+#             device_map (Dict of int, str, or torch.device): Device placement
+#                 mappings from this worker to the callee. This map must be
+#                 invertible.
+
+#         Example::
+#             >>> # both workers
+#             >>> def add(x, y):
+#             >>>     print(x)  # tensor([1., 1.], device='cuda:1')
+#             >>>     return x + y, (x + y).to(2)
+#             >>>
+#             >>> # on worker 0
+#             >>> options = TensorPipeRpcBackendOptions(
+#             >>>     num_worker_threads=8,
+#             >>>     device_maps={"worker1": {0: 1}}
+#             >>>     # maps worker0's cuda:0 to worker1's cuda:1
+#             >>> )
+#             >>> options.set_device_map("worker1", {1: 2})
+#             >>> # maps worker0's cuda:1 to worker1's cuda:2
+#             >>>
+#             >>> rpc.init_rpc(
+#             >>>     "worker0",
+#             >>>     rank=0,
+#             >>>     world_size=2,
+#             >>>     backend=rpc.BackendType.TENSORPIPE,
+#             >>>     rpc_backend_options=options
+#             >>> )
+#             >>>
+#             >>> x = torch.ones(2)
+#             >>> rets = rpc.rpc_sync("worker1", add, args=(x.to(0), 1))
+#             >>> # The first argument will be moved to cuda:1 on worker1. When
+#             >>> # sending the return value back, it will follow the invert of
+#             >>> # the device map, and hence will be moved back to cuda:0 and
+#             >>> # cuda:1 on worker0
+#             >>> print(rets[0])  # tensor([2., 2.], device='cuda:0')
+#             >>> print(rets[1])  # tensor([2., 2.], device='cuda:1')
+#         """
+#         full_device_map = _to_device_map(device_map)
+#         curr_device_maps = super().device_maps
+
+#         if to in curr_device_maps:
+#             for k, v in full_device_map.items():
+#                 if k in curr_device_maps[to] and v != curr_device_maps[to][k]:
+#                     raise ValueError(
+#                         "`set_device_map` only supports 1-to-1 mapping, trying"
+#                         f" to map {k} to {v} and {curr_device_maps[to][k]}"
+#                     )
+
+#         super()._set_device_map(to, full_device_map)
+
+#     def set_devices(self, devices: List[DeviceType]):
+#         r"""
+#         Set local devices used by the TensorPipe RPC agent. When processing
+#         CUDA RPC requests, the TensorPipe RPC agent will properly synchronize
+#         CUDA streams for all devices in this ``List``.
+
+#         Args:
+#             devices (List of int, str, or torch.device): local devices used by
+#                 the TensorPipe RPC agent.
+#         """
+#         self.devices = _to_device_list(devices)
-- 
2.17.2 (Apple Git-113)


From b7a8a8d3968e64d6a985242cd3c884d61478be4d Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Tue, 26 Oct 2021 21:26:02 +0800
Subject: [PATCH 2/2] orlando - for fixing issue of gloo

---
 aten/src/ATen/cuda/detail/LazyNVRTC.cpp | 6 ++++++
 torch/distributed/rpc/__init__.py       | 3 ++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/aten/src/ATen/cuda/detail/LazyNVRTC.cpp b/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
index efdca84838..3223cc39c4 100644
--- a/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
+++ b/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
@@ -13,6 +13,8 @@ namespace _stubs {
 at::DynamicLibrary& getCUDALibrary() {
 #if defined(_WIN32)
   static at::DynamicLibrary lib("nvcuda.dll");
+#elif defined(__APPLE__) && defined(__MACH__)
+  static at::DynamicLibrary lib("libcuda.dylib");
 #else
   static at::DynamicLibrary lib("libcuda.so.1");
 #endif
@@ -25,6 +27,8 @@ static std::string getLibVersion() {
   //
   // In the following, MAJOR and MINOR denote the major and minor versions of the CUDA Toolkit.
   // e.g. for CUDA 11.2, MAJOR is "11" and MINOR is "2".
+  // MacOS:
+  //  - In apple we add /usr/local/cuda/lib/libnvrtc.10.1.dylib and /usr/local/cuda/lib/libcuda.dylib
   //
   // Linux:
   //   - In CUDA toolkits prior to CUDA 11.3, the soname was set to "MAJOR.MINOR".
@@ -63,6 +67,8 @@ static std::string getLibVersion() {
 static std::string getLibName() {
 #if defined(_WIN32)
   return std::string("nvrtc64_") + getLibVersion() + "_0.dll";
+#elif defined(__APPLE__) && defined(__MACH__)
+  return std::string("libnvrtc.") + getLibVersion() + ".dylib";
 #else
   return std::string("libnvrtc.so.") + getLibVersion();
 #endif
diff --git a/torch/distributed/rpc/__init__.py b/torch/distributed/rpc/__init__.py
index d80359c251..3beb409958 100644
--- a/torch/distributed/rpc/__init__.py
+++ b/torch/distributed/rpc/__init__.py
@@ -71,7 +71,8 @@ if is_available():
     import torch.distributed.autograd as dist_autograd
 
     from .backend_registry import BackendType
-    from .options import TensorPipeRpcBackendOptions  # noqa: F401
+## For not using USE_TENSORPIPE for torch-1.9.1 hotfix on macOS
+    # from .options import TensorPipeRpcBackendOptions  # noqa: F401
     from .server_process_global_profiler import (
         _server_process_global_profile,
     )
-- 
2.17.2 (Apple Git-113)

