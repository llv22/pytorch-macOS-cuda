From a43e62343d50dce1719c4bd919494092759d3173 Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Thu, 16 Mar 2023 21:12:47 -0700
Subject: [PATCH 1/5] orlando - for fixing cuda c++14 compiling

---
 CMakeLists.txt                                |  2 +-
 c10/CMakeLists.txt                            |  2 +-
 .../FindCUDA/select_compute_arch.cmake        | 68 +++++++++----------
 cmake/public/cuda.cmake                       | 10 +--
 functorch/CMakeLists.txt                      |  2 +-
 5 files changed, 42 insertions(+), 42 deletions(-)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 471fc8a8d3d..97029735a66 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -37,7 +37,7 @@ if(env_cxx_standard GREATER -1)
       WARNING "C++ standard version definition detected in environment variable."
       "PyTorch requires -std=c++17. Please remove -std=c++ settings in your environment.")
 endif()
-set(CMAKE_CXX_STANDARD 17 CACHE STRING "The C++ standard whose features are requested to build this target.")
+set(CMAKE_CXX_STANDARD 14 CACHE STRING "The C++ standard whose features are requested to build this target.")
 set(CMAKE_C_STANDARD   11 CACHE STRING "The C standard whose features are requested to build this target.")
 
 # ---[ Utils
diff --git a/c10/CMakeLists.txt b/c10/CMakeLists.txt
index b8f65822f95..9c1c4c41cd2 100644
--- a/c10/CMakeLists.txt
+++ b/c10/CMakeLists.txt
@@ -1,7 +1,7 @@
 cmake_minimum_required(VERSION 3.18 FATAL_ERROR)
 project(c10 CXX)
 
-set(CMAKE_CXX_STANDARD 17 CACHE STRING "The C++ standard whose features are requested to build this target.")
+set(CMAKE_CXX_STANDARD 14 CACHE STRING "The C++ standard whose features are requested to build this target.")
 set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
 
 # Main build file for the C10 library.
diff --git a/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake b/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake
index 33c484e1029..8a8450ee289 100644
--- a/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake
+++ b/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake
@@ -27,50 +27,50 @@ endif()
 # See: https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list
 
 # This list will be used for CUDA_ARCH_NAME = All option
-set(CUDA_KNOWN_GPU_ARCHITECTURES  "Kepler" "Maxwell")
+set(CUDA_KNOWN_GPU_ARCHITECTURES  "Pascal")
 
 # This list will be used for CUDA_ARCH_NAME = Common option (enabled by default)
-set(CUDA_COMMON_GPU_ARCHITECTURES "3.5" "5.0")
+set(CUDA_COMMON_GPU_ARCHITECTURES "6.1")
 
 # This list is used to filter CUDA archs when autodetecting
-set(CUDA_ALL_GPU_ARCHITECTURES "3.5" "5.0")
+set(CUDA_ALL_GPU_ARCHITECTURES "6.1")
 
-if(CUDA_VERSION VERSION_GREATER "10.5")
-  list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Ampere")
-  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.0")
-  list(APPEND CUDA_ALL_GPU_ARCHITECTURES "8.0")
+# if(CUDA_VERSION VERSION_GREATER "10.5")
+#   list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Ampere")
+#   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.0")
+#   list(APPEND CUDA_ALL_GPU_ARCHITECTURES "8.0")
 
-  if(CUDA_VERSION VERSION_LESS "11.1")
-    set(CUDA_LIMIT_GPU_ARCHITECTURE "8.0")
-    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.0+PTX")
-  endif()
-endif()
+#   if(CUDA_VERSION VERSION_LESS "11.1")
+#     set(CUDA_LIMIT_GPU_ARCHITECTURE "8.0")
+#     list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.0+PTX")
+#   endif()
+# endif()
 
-if(NOT CUDA_VERSION VERSION_LESS "11.1")
-  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.6")
-  list(APPEND CUDA_ALL_GPU_ARCHITECTURES "8.6")
-  set(CUDA_LIMIT_GPU_ARCHITECUTRE "8.6")
+# if(NOT CUDA_VERSION VERSION_LESS "11.1")
+#   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.6")
+#   list(APPEND CUDA_ALL_GPU_ARCHITECTURES "8.6")
+#   set(CUDA_LIMIT_GPU_ARCHITECUTRE "8.6")
 
-  if(CUDA_VERSION VERSION_LESS "11.8")
-    set(CUDA_LIMIT_GPU_ARCHITECTURE "8.9")
-    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.6+PTX")
-  endif()
-endif()
+#   if(CUDA_VERSION VERSION_LESS "11.8")
+#     set(CUDA_LIMIT_GPU_ARCHITECTURE "8.9")
+#     list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.6+PTX")
+#   endif()
+# endif()
 
-if(NOT CUDA_VERSION VERSION_LESS "11.8")
-  list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Ada")
-  list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Hopper")
-  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.9")
-  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "9.0")
-  list(APPEND CUDA_ALL_GPU_ARCHITECTURES "8.9")
-  list(APPEND CUDA_ALL_GPU_ARCHITECTURES "9.0")
+# if(NOT CUDA_VERSION VERSION_LESS "11.8")
+#   list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Ada")
+#   list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Hopper")
+#   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.9")
+#   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "9.0")
+#   list(APPEND CUDA_ALL_GPU_ARCHITECTURES "8.9")
+#   list(APPEND CUDA_ALL_GPU_ARCHITECTURES "9.0")
 
-  if(CUDA_VERSION VERSION_LESS "12.0")
-    set(CUDA_LIMIT_GPU_ARCHITECTURE "9.0")
-    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.9+PTX")
-    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "9.0+PTX")
-  endif()
-endif()
+#   if(CUDA_VERSION VERSION_LESS "12.0")
+#     set(CUDA_LIMIT_GPU_ARCHITECTURE "9.0")
+#     list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.9+PTX")
+#     list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "9.0+PTX")
+#   endif()
+# endif()
 
 ################################################################################################
 # A function for automatic detection of GPUs installed  (if autodetection is enabled)
diff --git a/cmake/public/cuda.cmake b/cmake/public/cuda.cmake
index df40ff7d2da..38a06ab5aff 100644
--- a/cmake/public/cuda.cmake
+++ b/cmake/public/cuda.cmake
@@ -45,14 +45,14 @@ if("${CMAKE_CXX_COMPILER_ID}" MATCHES "Clang")
   set(CMAKE_CUDA_HOST_COMPILER "${CMAKE_C_COMPILER}")
 endif()
 enable_language(CUDA)
-set(CMAKE_CUDA_STANDARD ${CMAKE_CXX_STANDARD})
+set(CMAKE_CUDA_STANDARD 14)
 set(CMAKE_CUDA_STANDARD_REQUIRED ON)
 
 message(STATUS "Caffe2: CUDA detected: " ${CUDA_VERSION})
 message(STATUS "Caffe2: CUDA nvcc is: " ${CUDA_NVCC_EXECUTABLE})
 message(STATUS "Caffe2: CUDA toolkit directory: " ${CUDA_TOOLKIT_ROOT_DIR})
-if(CUDA_VERSION VERSION_LESS 11.0)
-  message(FATAL_ERROR "PyTorch requires CUDA 11.0 or above.")
+if(CUDA_VERSION VERSION_LESS 10.1)
+  message(FATAL_ERROR "PyTorch requires CUDA 10.1 or above.")
 endif()
 
 if(CUDA_FOUND)
@@ -273,8 +273,8 @@ if(CAFFE2_USE_CUDNN)
       "Cannot find cuDNN library. Turning the option off")
     set(CAFFE2_USE_CUDNN OFF)
   else()
-    if(CUDNN_VERSION VERSION_LESS "8.0.0")
-      message(FATAL_ERROR "PyTorch requires cuDNN 8 and above.")
+    if(CUDNN_VERSION VERSION_LESS "7.6.5")
+      message(FATAL_ERROR "PyTorch requires cuDNN 7.6.5 and above.")
     endif()
   endif()
 
diff --git a/functorch/CMakeLists.txt b/functorch/CMakeLists.txt
index 3e578079859..1fd0944ebd5 100644
--- a/functorch/CMakeLists.txt
+++ b/functorch/CMakeLists.txt
@@ -1,6 +1,6 @@
 cmake_minimum_required(VERSION 3.18)
 project(functorch)
-set(CMAKE_CXX_STANDARD 17)
+set(CMAKE_CXX_STANDARD 14)
 
 include(GNUInstallDirs)
 include(CMakePackageConfigHelpers)
-- 
2.17.2 (Apple Git-113)


From 17c9239923123fc379db5d0a87aadc46537744d3 Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Thu, 16 Mar 2023 21:55:16 -0700
Subject: [PATCH 2/5] orlando - for commiting changes

---
 .gitmodules                                   |  3 +-
 CMakeLists.txt                                |  3 ++
 aten/src/ATen/CMakeLists.txt                  |  8 +++++
 aten/src/ATen/native/ReduceOps.cpp            |  8 +++++
 .../ATen/native/cpu/AdaptiveMaxPoolKernel.cpp | 16 ++++++++-
 aten/src/ATen/native/cpu/MaxPoolKernel.cpp    | 16 ++++++++-
 aten/src/ATen/native/cpu/MaxPooling.cpp       | 14 ++++++++
 aten/src/ATen/native/cuda/EmbeddingBag.cu     |  3 +-
 c10/util/safe_numerics.h                      | 10 ++++++
 caffe2/CMakeLists.txt                         | 12 +++++--
 caffe2/core/macros.h.in                       |  2 ++
 caffe2/mpi/mpi_ops_gpu.cc                     | 26 +++++++++++++--
 cmake/Dependencies.cmake                      | 33 ++++++++++++++++---
 cmake/Summary.cmake                           |  1 +
 test/cpp/api/CMakeLists.txt                   |  6 +++-
 .../csrc/distributed/c10d/ProcessGroupMPI.cpp | 17 +++++++---
 16 files changed, 160 insertions(+), 18 deletions(-)

diff --git a/.gitmodules b/.gitmodules
index 282746ed0b5..d5ac854e8e1 100644
--- a/.gitmodules
+++ b/.gitmodules
@@ -129,7 +129,8 @@
 [submodule "third_party/tensorpipe"]
     ignore = dirty
     path = third_party/tensorpipe
-    url = https://github.com/pytorch/tensorpipe.git
+    branch = torch-1.12.0
+    url = https://github.com/llv22/tensorpipe-macos-cuda.git
 [submodule "third_party/cudnn_frontend"]
 	path = third_party/cudnn_frontend
 	url = https://github.com/NVIDIA/cudnn-frontend.git
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 97029735a66..51f7464a3ef 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -306,6 +306,9 @@ option(USE_DISTRIBUTED "Use distributed" ON)
 cmake_dependent_option(
     USE_MPI "Use MPI for Caffe2. Only available if USE_DISTRIBUTED is on." ON
     "USE_DISTRIBUTED" OFF)
+cmake_dependent_option(
+  USE_CUDA_MPI "Force CUDA-Aware MPI for Caffe2. Only available if USE_DISTRIBUTED and USE_MPI is on." OFF
+  "USE_DISTRIBUTED AND USE_MPI" OFF)
 cmake_dependent_option(
     USE_UCC "Use UCC. Only available if USE_DISTRIBUTED is on." OFF
     "USE_DISTRIBUTED" OFF)
diff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt
index 96fc29782b2..988c8b92d6c 100644
--- a/aten/src/ATen/CMakeLists.txt
+++ b/aten/src/ATen/CMakeLists.txt
@@ -293,6 +293,11 @@ if(USE_TBB)
   list(APPEND ATen_CPU_DEPENDENCY_LIBS TBB::tbb)
 endif()
 
+if(USE_OPENMP)
+  message("ATen is compiled with OPEN_MP (/Users/llv23/opt/miniconda3/lib/libomp.dylib)")
+  list(APPEND ATen_CPU_DEPENDENCY_LIBS /Users/llv23/opt/miniconda3/lib/libomp.dylib)
+endif()
+
 if(BLAS_FOUND)
   if($ENV{TH_BINARY_BUILD})
     message(STATUS "TH_BINARY_BUILD detected. Enabling special linkage.")
@@ -442,11 +447,13 @@ if(USE_CUDA AND NOT USE_ROCM)
       ${CUDA_TOOLKIT_ROOT_DIR}/lib64/libcusparse_static.a
       ${CUDA_TOOLKIT_ROOT_DIR}/lib64/libcurand_static.a
       ${CUDA_TOOLKIT_ROOT_DIR}/lib64/libcufft_static_nocallback.a
+      /Users/llv23/opt/miniconda3/lib/libomp.dylib # test parallel for symbol _omp_in_parallel
       )
    if(NOT BUILD_LAZY_CUDA_LINALG)
      list(APPEND ATen_CUDA_DEPENDENCY_LIBS
        ${CUDA_TOOLKIT_ROOT_DIR}/lib64/libcusolver_static.a
        ${CUDA_TOOLKIT_ROOT_DIR}/lib64/liblapack_static.a     # needed for libcusolver_static
+       /Users/llv23/opt/miniconda3/lib/libomp.dylib # test parallel for symbol _omp_in_parallel
        )
    endif()
   else()
@@ -454,6 +461,7 @@ if(USE_CUDA AND NOT USE_ROCM)
       ${CUDA_LIBRARIES}
       ${CUDA_cusparse_LIBRARY}
       ${CUDA_curand_LIBRARY}
+      /Users/llv23/opt/miniconda3/lib/libomp.dylib # test parallel for symbol _omp_in_parallel
       )
    if(NOT BUILD_LAZY_CUDA_LINALG)
      list(APPEND ATen_CUDA_DEPENDENCY_LIBS
diff --git a/aten/src/ATen/native/ReduceOps.cpp b/aten/src/ATen/native/ReduceOps.cpp
index 91bf3985617..bc1d5611b00 100644
--- a/aten/src/ATen/native/ReduceOps.cpp
+++ b/aten/src/ATen/native/ReduceOps.cpp
@@ -757,6 +757,14 @@ template<typename T>
 inline typename std::enable_if<!std::is_integral<T>::value, bool>::type isnan_(T x) {
   return std::isnan(x);
 }
+#elif defined(__APPLE__) && defined(__MACH__)
+template<typename T>
+inline bool isnan_(T x) {
+  return std::isnan(x);
+}
+inline bool isnan_(const c10::BFloat16 x) {
+  return std::isnan(x.x);
+}
 #else
 template<typename T>
 inline bool isnan_(T x) {
diff --git a/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp b/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
index 2464a6a1441..865a93f309e 100644
--- a/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
@@ -12,6 +12,16 @@ namespace at::native {
 
 namespace {
 
+#if defined(__APPLE__) && defined(__MACH__)
+template<typename T>
+inline bool isnan_(T x) {
+  return std::isnan(x);
+}
+inline bool isnan_(const c10::BFloat16 x) {
+  return std::isnan(x.x);
+}
+#endif
+
 template <typename scalar_t, typename accscalar_t>
 void cpu_adaptive_max_pool(
     const Tensor& output_,
@@ -56,7 +66,11 @@ void cpu_adaptive_max_pool(
             for (int64_t iw = iw0; iw < iw1; iw ++) {
               int64_t index = ih * input_width + iw;
               scalar_t val = input_ptr[index];
-              if ((val > maxval) || std::isnan(val)) {
+#if defined(__APPLE__) && defined(__MACH__)
+          if ((val > maxval) || isnan_(val)) {
+#else
+          if ((val > maxval) || std::isnan(val)) {
+#endif
                 maxval = val;
                 maxindex = index;
               }
diff --git a/aten/src/ATen/native/cpu/MaxPoolKernel.cpp b/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
index 90d0993ea03..3da7e6fd98b 100644
--- a/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
@@ -13,6 +13,16 @@ namespace at::native {
 
 namespace {
 
+#if defined(__APPLE__) && defined(__MACH__)
+template<typename T>
+inline bool isnan_(T x) {
+  return std::isnan(x);
+}
+inline bool isnan_(const c10::BFloat16 x) {
+  return std::isnan(x.x);
+}
+#endif
+
 template <typename scalar_t, typename accscalar_t>
 void cpu_max_pool(
     const Tensor& output_,
@@ -64,7 +74,11 @@ void cpu_max_pool(
         for (int64_t iw = iw0; iw < iw1; iw += dilationW) {
           int64_t index = ih * input_width + iw;
           accscalar_t val = accscalar_t(input_ptr[index]);
-          if ((val > maxval) || std::isnan(val)) {
+#if defined(__APPLE__) && defined(__MACH__)
+          if ((val > maxval) || isnan_(val)) {
+#else
+           if ((val > maxval) || std::isnan(val)) {
+#endif
             maxval = val;
             maxindex = index;
           }
diff --git a/aten/src/ATen/native/cpu/MaxPooling.cpp b/aten/src/ATen/native/cpu/MaxPooling.cpp
index 1f3029456ed..4a6b4b557ac 100644
--- a/aten/src/ATen/native/cpu/MaxPooling.cpp
+++ b/aten/src/ATen/native/cpu/MaxPooling.cpp
@@ -10,6 +10,16 @@ namespace at::native {
 
 namespace {
 
+#if defined(__APPLE__) && defined(__MACH__)
+template<typename T>
+inline bool isnan_(T x) {
+  return std::isnan(x);
+  }
+inline bool isnan_(const c10::BFloat16 x) {
+  return std::isnan(x.x);
+}
+#endif
+
 template <typename scalar_t>
 inline void max_pool1d_kernel(
     scalar_t* C10_RESTRICT op,
@@ -21,7 +31,11 @@ inline void max_pool1d_kernel(
     int64_t ij = p.index(kj, oj);
     for (; oj < oe; ++oj, ij += p.SJ) {
       scalar_t val = ip[ij];
+#if defined(__APPLE__) && defined(__MACH__)
+      bool update_max = isnan_(val) || op[oj] < val;
+#else
       bool update_max = std::isnan(val) || op[oj] < val;
+#endif
       op[oj] = update_max ? val : op[oj];
     }
   }
diff --git a/aten/src/ATen/native/cuda/EmbeddingBag.cu b/aten/src/ATen/native/cuda/EmbeddingBag.cu
index 7a1e2663b49..5f448463ce4 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBag.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBag.cu
@@ -189,7 +189,8 @@ Tensor embedding_bag_backward_cuda_sum_avg(
   Tensor count;
 
   AT_DISPATCH_INDEX_TYPES(indices.scalar_type(), "embedding_bag_backward_cuda_sum_avg", [&] () {
-    auto range = at::arange(num_indices, indices.options());
+    //https://github.com/pytorch/pytorch/issues/42271
+    auto range = at::arange(c10::Scalar((int64_t)num_indices), indices.options());
     // int64_t nbits = cuda::cub::get_num_bits(num_weights);
     cuda::cub::radix_sort_pairs(
       indices.data_ptr<index_t>(), sorted_indices.data_ptr<index_t>(),
diff --git a/c10/util/safe_numerics.h b/c10/util/safe_numerics.h
index e5c249dd1d2..77e1e6feced 100644
--- a/c10/util/safe_numerics.h
+++ b/c10/util/safe_numerics.h
@@ -19,7 +19,12 @@ namespace c10 {
 
 C10_ALWAYS_INLINE bool add_overflows(uint64_t a, uint64_t b, uint64_t* out) {
 #if C10_HAS_BUILTIN_OVERFLOW()
+// https://clang.llvm.org/docs/LanguageExtensions.html#checked-arithmetic-builtins
+#if defined(__APPLE__) && defined(__MACH__)
+  return __builtin_uaddll_overflow(a, b, out);
+#else
   return __builtin_add_overflow(a, b, out);
+#endif
 #else
   unsigned long long tmp;
 #if defined(_M_IX86) || defined(_M_X64)
@@ -36,7 +41,12 @@ C10_ALWAYS_INLINE bool add_overflows(uint64_t a, uint64_t b, uint64_t* out) {
 
 C10_ALWAYS_INLINE bool mul_overflows(uint64_t a, uint64_t b, uint64_t* out) {
 #if C10_HAS_BUILTIN_OVERFLOW()
+// https://clang.llvm.org/docs/LanguageExtensions.html#checked-arithmetic-builtins
+#if defined(__APPLE__) && defined(__MACH__)
+  return __builtin_umulll_overflow(a, b, out);
+#else
   return __builtin_mul_overflow(a, b, out);
+#endif
 #else
   *out = a * b;
   // This test isnt exact, but avoids doing integer division
diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index 221e3f32b29..5daef846321 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -96,8 +96,8 @@ if(INTERN_BUILD_ATEN_OPS)
   list(APPEND Caffe2_GPU_INCLUDE ${ATen_CUDA_INCLUDE})
   list(APPEND Caffe2_HIP_INCLUDE ${ATen_HIP_INCLUDE})
   list(APPEND Caffe2_VULKAN_INCLUDE ${ATen_VULKAN_INCLUDE})
-  list(APPEND Caffe2_DEPENDENCY_LIBS ${ATen_CPU_DEPENDENCY_LIBS})
-  list(APPEND Caffe2_CUDA_DEPENDENCY_LIBS ${ATen_CUDA_DEPENDENCY_LIBS})
+  list(APPEND Caffe2_DEPENDENCY_LIBS ${ATen_CPU_DEPENDENCY_LIBS} /Users/llv23/opt/miniconda3/lib/libomp.dylib)
+  list(APPEND Caffe2_CUDA_DEPENDENCY_LIBS ${ATen_CUDA_DEPENDENCY_LIBS} /Users/llv23/opt/miniconda3/lib/libomp.dylib)
   list(APPEND Caffe2_HIP_DEPENDENCY_LIBS ${ATen_HIP_DEPENDENCY_LIBS})
   list(APPEND Caffe2_DEPENDENCY_INCLUDE ${ATen_THIRD_PARTY_INCLUDE})
 endif()
@@ -1256,6 +1256,9 @@ if(USE_DISTRIBUTED)
         "${TORCH_SRC_DIR}/csrc/distributed/c10d/ProcessGroupMPI.cpp"
         PROPERTIES COMPILE_FLAGS -Wno-deprecated-declarations)
     endif()
+    if(USE_CUDA_MPI)
+      add_definitions(-DUSE_CUDA_MPI=1)
+    endif()
     target_compile_definitions(torch_cpu PUBLIC USE_C10D_MPI)
   endif()
   # Pass USE_RPC in order to reduce use of
@@ -1601,7 +1604,7 @@ if(BUILD_MOBILE_TEST)
   foreach(test_src ${ATen_MOBILE_TEST_SRCS})
     get_filename_component(test_name ${test_src} NAME_WE)
     add_executable(${test_name} "${test_src}")
-    target_link_libraries(${test_name} torch_library gtest_main)
+    target_link_libraries(${test_name} torch_library gtest_main /Users/llv23/opt/miniconda3/lib/libomp.dylib)
     target_include_directories(${test_name} PRIVATE $<INSTALL_INTERFACE:include>)
     target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)
     target_include_directories(${test_name} PRIVATE ${ATen_CPU_INCLUDE})
@@ -1836,6 +1839,9 @@ if(BUILD_PYTHON)
   target_compile_definitions(torch_python PRIVATE BUILD_CAFFE2)
   if(USE_NUMPY)
     target_compile_options(caffe2_pybind11_state PRIVATE "-DUSE_NUMPY")
+    # Orlando; refer to how to fix issue: ../caffe2/python/pybind_state.h:27:10: fatal error: 'numpy/arrayobject.h' file not found
+    find_package(Python3 REQUIRED COMPONENTS NumPy)
+    target_include_directories(caffe2_pybind11_state PRIVATE ${Python3_NumPy_INCLUDE_DIRS})
     target_link_libraries(caffe2_pybind11_state  PRIVATE numpy::numpy)
   endif()
   if(NOT MSVC)
diff --git a/caffe2/core/macros.h.in b/caffe2/core/macros.h.in
index 2d9f03e94c0..d82430522ce 100644
--- a/caffe2/core/macros.h.in
+++ b/caffe2/core/macros.h.in
@@ -25,6 +25,7 @@ static_assert(
 
 #cmakedefine CAFFE2_BUILD_SHARED_LIBS
 #cmakedefine CAFFE2_FORCE_FALLBACK_CUDA_MPI
+#cmakedefine CAFFE2_USE_CUDA_MPI
 #cmakedefine CAFFE2_HAS_MKL_DNN
 #cmakedefine CAFFE2_HAS_MKL_SGEMM_PACK
 #cmakedefine CAFFE2_PERF_WITH_AVX
@@ -66,6 +67,7 @@ static_assert(
   {"CUDNN_VERSION", "${CUDNN_VERSION}"}, \
   {"USE_NCCL", "${USE_NCCL}"}, \
   {"USE_MPI", "${USE_MPI}"}, \
+  {"USE_CUDA_MPI", "${USE_CUDA_MPI}"}, \
   {"USE_GFLAGS", "${USE_GFLAGS}"}, \
   {"USE_GLOG", "${USE_GLOG}"}, \
   {"USE_GLOO", "${USE_GLOI}"}, \
diff --git a/caffe2/mpi/mpi_ops_gpu.cc b/caffe2/mpi/mpi_ops_gpu.cc
index bb645a5c785..1fd149d2d22 100644
--- a/caffe2/mpi/mpi_ops_gpu.cc
+++ b/caffe2/mpi/mpi_ops_gpu.cc
@@ -32,9 +32,23 @@ namespace caffe2 {
 #define CAFFE2_HAS_CUDA_MPI_ALLREDUCE 1
 #else // CAFFE2_OMPI_VERSION >= 10805
 #define CAFFE2_HAS_CUDA_MPI_ALLREDUCE 0
-#endif // CAFFE2_OMPI_VERSION >= 10805
 #endif // CAFFE2_OMPI_VERSION >= 2000
-#else // !OPEN_MPI
+#elif MVAPICH2_NUMVERSION // !OPEN_MPI
+#define CAFFE2_MV2_VERSION MVAPICH2_NUMVERSION
+#if CAFFE2_MV2_VERSION >= 20305300
+#include "mpi-ext.h"
+#if MPIX_CUDA_AWARE_SUPPORT
+#define CAFFE2_HAS_CUDA_MPI_BASICS 1
+#define CAFFE2_HAS_CUDA_MPI_ALLREDUCE 1
+#endif // MPIX_CUDA_AWARE_SUPPORT
+#else //CAFFE2_MV2_VERSION >= 235
+// In the case of MVAPICH2-GDR before 2.3.5, we don't have compile-time flags
+// // to figure out if CUDA is supported; as a result, we will assume that the
+// // user has built MVAPICH2-GDR with CUDA support.
+#define CAFFE2_HAS_CUDA_MPI_BASICS 1
+#define CAFFE2_HAS_CUDA_MPI_ALLREDUCE 1
+#endif //CAFFE2_MV2_VERSION >= 235
+#else // !OPEN_MPI && !MVAPICH_GDR
 // We have not really tested against other MPI environments, so let's go for a
 // safe path and basically say we don't have cuda-aware functions.
 #define CAFFE2_HAS_CUDA_MPI_BASICS 0
@@ -49,6 +63,14 @@ namespace caffe2 {
 #define CAFFE2_HAS_CUDA_MPI_ALLREDUCE 0
 #endif // CAFFE2_FORCE_FALLBACK_CUDA_MPI
 
+// We allow a macro to force using CUDA functions
+#ifdef CAFFE2_USE_CUDA_MPI
+#undef CAFFE2_HAS_CUDA_MPI_BASICS
+#undef CAFFE2_HAS_CUDA_MPI_ALLREDUCE
+#define CAFFE2_HAS_CUDA_MPI_BASICS 1
+#define CAFFE2_HAS_CUDA_MPI_ALLREDUCE 1
+#endif // CAFFE2_FORCE_CUDA_MPI
+
 REGISTER_CUDA_OPERATOR(
     MPICreateCommonWorld,
     MPICreateCommonWorldOp<CUDAContext>);
diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake
index 8c0e3c24bc5..2b0590799b7 100644
--- a/cmake/Dependencies.cmake
+++ b/cmake/Dependencies.cmake
@@ -1081,6 +1081,7 @@ if(BUILD_PYTHON)
 
   # These should fill in the rest of the variables, like versions, but resepct
   # the variables we set above
+  set(CMAKE_PREFIX_PATH "/Users/llv23/opt/miniconda3")
   set(Python_ADDITIONAL_VERSIONS ${PYTHON_VERSION} 3.8)
   find_package(PythonInterp 3.0)
   find_package(PythonLibs 3.0)
@@ -1169,13 +1170,37 @@ if(USE_MPI)
       execute_process(COMMAND ${OMPI_INFO}
                       OUTPUT_VARIABLE _output)
       if(_output MATCHES "smcuda")
-        message(STATUS "Found OpenMPI with CUDA support built.")
       else()
-        message(WARNING "OpenMPI found, but it is not built with CUDA support.")
-        set(CAFFE2_FORCE_FALLBACK_CUDA_MPI 1)
+        if(USE_CUDA_MPI)
+          if(USE_CUDA)
+            message(WARNING "OpenMPI with CUDA support not found, but forcing anyway.")
+          else()
+            message(WARNING "Force building for OpenMPI with CUDA.")
+          endif()
+          set(CAFFE2_USE_CUDA_MPI 1)
+        else()
+          message(WARNING "OpenMPI found, but it is not built with CUDA support.")
+          set(CAFFE2_FORCE_FALLBACK_CUDA_MPI 1)
+        endif()
       endif()
+    else()
+      find_program(MV2_INFO NAMES mpiname HINTS ${MPI_CXX_LIBRARIES}/../bin)
+      if(MV2_INFO)
+          execute_process(COMMAND ${MV2_INFO} "-a" OUTPUT_VARIABLE _output)
+          if(_output MATCHES "enable-cuda")
+              message(STATUS "Found MVAPICH2 with CUDA support built.")
+          else()
+              if(USE_CUDA_MPI)
+                  message(WARNING "MVAPICH2 with CUDA support not found, but forcing anyway.")
+                  set(CAFFE2_USE_CUDA_MPI 1)
+              else()
+                  message(WARNING "MVAPICH2 found, but it is not built with CUDA SUPPORT.")
+                  set(CAFFE2_FORCE_FALLBACK_CUDA_MPI 1)
+              endif()
+          endif()
+      endif()   
     endif()
-  else()
+      else()
     message(WARNING "Not compiling with MPI. Suppress this warning with -DUSE_MPI=OFF")
     caffe2_update_option(USE_MPI OFF)
   endif()
diff --git a/cmake/Summary.cmake b/cmake/Summary.cmake
index eba48dff57a..eb8df349b78 100644
--- a/cmake/Summary.cmake
+++ b/cmake/Summary.cmake
@@ -183,6 +183,7 @@ function(caffe2_print_configuration_summary)
   message(STATUS "  USE_DISTRIBUTED       : ${USE_DISTRIBUTED}")
   if(${USE_DISTRIBUTED})
     message(STATUS "    USE_MPI               : ${USE_MPI}")
+    message(STATUS "    USE_CUDA_MPI          : ${USE_CUDA_MPI}")
     message(STATUS "    USE_GLOO              : ${USE_GLOO}")
     message(STATUS "    USE_GLOO_WITH_OPENSSL : ${USE_GLOO_WITH_OPENSSL}")
     message(STATUS "    USE_TENSORPIPE        : ${USE_TENSORPIPE}")
diff --git a/test/cpp/api/CMakeLists.txt b/test/cpp/api/CMakeLists.txt
index 6b801a07318..b58b3dae145 100644
--- a/test/cpp/api/CMakeLists.txt
+++ b/test/cpp/api/CMakeLists.txt
@@ -48,7 +48,11 @@ endif()
 
 add_executable(test_api ${TORCH_API_TEST_SOURCES})
 target_include_directories(test_api PRIVATE ${ATen_CPU_INCLUDE})
-target_link_libraries(test_api PRIVATE torch gtest)
+if (${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
+  target_link_libraries(test_api PRIVATE torch gtest -Xpreprocessor -fopenmp /Users/llv23/opt/miniconda3/lib/libomp.dylib /Users/llv23/opt/miniconda3/lib/libgomp.dylib)
+else()
+  target_link_libraries(test_api PRIVATE torch gtest)
+endif()
 if(NOT MSVC)
   target_compile_options_if_supported(test_api -Wno-unused-variable)
 endif()
diff --git a/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp b/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp
index 14daa6faf48..7423675864d 100644
--- a/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp
+++ b/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp
@@ -9,7 +9,13 @@
 #include <c10/core/DeviceGuard.h>
 #include <c10/util/irange.h>
 
-#if defined(OPEN_MPI) && OPEN_MPI
+// #if defined(OPEN_MPI) && OPEN_MPI
+#ifndef OPEN_MPI
+#define OPEN_MPI 0
+#endif
+//Build flag USE_CUDA_MPI forces CUDA-Aware MPI support and removes run-time checks. 
+//USE_CUDA_MPI is meant for older MPI libraries that don't support MPIX_Query_cuda_support()
+#if (OPEN_MPI || (defined(MVAPICH2_NUMVERSION) && (MVAPICH2_NUMVERSION >= 20205300))) && !USE_CUDA_MPI
 #include <mpi-ext.h> // Needed for CUDA-aware check
 #endif
 
@@ -48,17 +54,20 @@ std::map<at::ScalarType, MPI_Datatype> mpiDatatype = {
 
 // Checking CUDA-aware MPI support, currently we only support CUDA aware
 // MPI ops through Open MPI
+// MPI ops through Open MPI and MVAPICH2-GDR
 bool cudaAwareMpiCheck() {
 // Run time check
-#if defined(MPIX_CUDA_AWARE_SUPPORT)
+#if !defined(USE_CUDA_MPI) && defined(MPIX_CUDA_AWARE_SUPPORT)
   if (MPIX_Query_cuda_support() == 1) {
     return true;
   } else {
     return false;
   }
-#else // !defined(MPIX_CUDA_AWARE_SUPPORT)
+#elif defined(USE_CUDA_MPI)
+  return true;
+#else // defined(USE_CUDA_MPI)
   return false;
-#endif // MPIX_CUDA_AWARE_SUPPORT
+#endif // MPIX_CUDA_AWARE_SUPPORT && !USE_CUDA_MPI
 }
 
 // Checking the input tensor's validity
-- 
2.17.2 (Apple Git-113)


From 03db1976d5b4374a8d9db6a547b025a27e9ac313 Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Fri, 17 Mar 2023 09:58:05 -0700
Subject: [PATCH 3/5] orlando - for updates

---
 c10/util/BFloat16-inl.h | 7 ++++---
 c10/util/BFloat16.h     | 4 ++--
 third_party/pocketfft   | 2 +-
 3 files changed, 7 insertions(+), 6 deletions(-)

diff --git a/c10/util/BFloat16-inl.h b/c10/util/BFloat16-inl.h
index c2ad4d20377..007dd651851 100644
--- a/c10/util/BFloat16-inl.h
+++ b/c10/util/BFloat16-inl.h
@@ -37,7 +37,7 @@ inline C10_HOST_DEVICE BFloat16::BFloat16(float value)
 
 /// Implicit conversions
 inline C10_HOST_DEVICE BFloat16::operator float() const {
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(__CUDACC__) && !defined(USE_ROCM) && CUDA_VERSION >= 11000
   return __bfloat162float(*reinterpret_cast<const __nv_bfloat16*>(&x));
 #elif defined(__SYCL_DEVICE_ONLY__) && \
     defined(SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS)
@@ -47,7 +47,7 @@ inline C10_HOST_DEVICE BFloat16::operator float() const {
 #endif
 }
 
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(__CUDACC__) && !defined(USE_ROCM) && CUDA_VERSION >= 11000
 inline C10_HOST_DEVICE BFloat16::BFloat16(const __nv_bfloat16& value) {
   x = *reinterpret_cast<const unsigned short*>(&value);
 }
@@ -70,7 +70,8 @@ inline C10_HOST_DEVICE BFloat16::operator sycl::ext::oneapi::bfloat16() const {
 
 #if defined(__CUDACC__) || defined(__HIPCC__)
 inline C10_DEVICE BFloat16 __ldg(const BFloat16* ptr) {
-#if !defined(USE_ROCM) && defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && \
+    defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
   return __ldg(reinterpret_cast<const __nv_bfloat16*>(ptr));
 #else
   return *ptr;
diff --git a/c10/util/BFloat16.h b/c10/util/BFloat16.h
index cf20f7a3b90..fcfdb28ec11 100644
--- a/c10/util/BFloat16.h
+++ b/c10/util/BFloat16.h
@@ -7,7 +7,7 @@
 #include <cmath>
 #include <cstring>
 
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(__CUDACC__) && !defined(USE_ROCM) && CUDA_VERSION >= 11000
 #include <cuda_bf16.h>
 #endif
 
@@ -99,7 +99,7 @@ struct alignas(2) BFloat16 {
   inline C10_HOST_DEVICE BFloat16(float value);
   inline C10_HOST_DEVICE operator float() const;
 
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(__CUDACC__) && !defined(USE_ROCM) && CUDA_VERSION >= 11000
   inline C10_HOST_DEVICE BFloat16(const __nv_bfloat16& value);
   explicit inline C10_HOST_DEVICE operator __nv_bfloat16() const;
 #endif
diff --git a/third_party/pocketfft b/third_party/pocketfft
index ea778e37710..f800d91ba69 160000
--- a/third_party/pocketfft
+++ b/third_party/pocketfft
@@ -1 +1 @@
-Subproject commit ea778e37710c07723435b1be58235996d1d43a5a
+Subproject commit f800d91ba695b6e19ae2687dd60366900b928002
-- 
2.17.2 (Apple Git-113)


From 4764007d13aec53227fbe81f126d3ecc7aacd72b Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Sat, 18 Mar 2023 12:35:25 -0700
Subject: [PATCH 4/5] orlando - for finalizing torch 2.0.0 on macOS

---
 aten/src/ATen/ConjugateFallback.cpp           |  4 +--
 aten/src/ATen/cuda/Atomic.cuh                 |  5 +--
 aten/src/ATen/cuda/CUDABlas.cpp               | 33 ++++++++++++++-----
 aten/src/ATen/cuda/CUDADataType.h             |  6 ++--
 aten/src/ATen/cuda/CUDASparseDescriptors.cpp  |  4 +--
 aten/src/ATen/cuda/CUDASparseDescriptors.h    | 10 ++++--
 aten/src/ATen/cuda/CublasHandlePool.cpp       |  4 +--
 aten/src/ATen/native/cpu/Activation.cpp       |  4 +--
 .../ATen/native/cpu/AdaptiveAvgPoolKernel.cpp |  4 +--
 .../ATen/native/cpu/AdaptiveMaxPoolKernel.cpp |  4 +--
 aten/src/ATen/native/cpu/AvgPoolKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/BinaryOpsKernel.cpp  |  4 +--
 aten/src/ATen/native/cpu/BlasKernel.cpp       |  4 +--
 aten/src/ATen/native/cpu/CatKernel.cpp        |  4 +--
 .../ATen/native/cpu/ChannelShuffleKernel.cpp  |  4 +--
 aten/src/ATen/native/cpu/ComplexKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/CopyKernel.cpp       |  4 +--
 aten/src/ATen/native/cpu/CrossKernel.cpp      |  4 +--
 .../ATen/native/cpu/DepthwiseConvKernel.cpp   |  4 +--
 .../src/ATen/native/cpu/DistanceOpsKernel.cpp |  4 +--
 .../ATen/native/cpu/DistributionKernels.cpp   |  4 +--
 aten/src/ATen/native/cpu/FillKernel.cpp       |  4 +--
 .../cpu/FunctionOfAMatrixUtilsKernel.cpp      |  4 +--
 .../src/ATen/native/cpu/GridSamplerKernel.cpp |  4 +--
 aten/src/ATen/native/cpu/HistogramKernel.cpp  |  4 +--
 aten/src/ATen/native/cpu/IndexKernel.cpp      |  4 +--
 .../ATen/native/cpu/LinearAlgebraKernel.cpp   |  4 +--
 aten/src/ATen/native/cpu/MaxPoolKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/MaxPooling.cpp       |  4 +--
 aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp  |  4 +--
 .../src/ATen/native/cpu/MultinomialKernel.cpp |  4 +--
 .../ATen/native/cpu/PixelShuffleKernel.cpp    |  4 +--
 .../ATen/native/cpu/PointwiseOpsKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/PowKernel.cpp        |  4 +--
 .../ATen/native/cpu/RangeFactoriesKernel.cpp  |  4 +--
 .../ATen/native/cpu/ReduceAllOpsKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/ReduceOpsKernel.cpp  |  4 +--
 aten/src/ATen/native/cpu/ReduceUtils.h        |  4 +--
 aten/src/ATen/native/cpu/RenormKernel.cpp     |  4 +--
 .../ATen/native/cpu/SampledAddmmKernel.cpp    |  4 +--
 .../ATen/native/cpu/ScatterGatherKernel.cpp   |  4 +--
 aten/src/ATen/native/cpu/SoftMaxKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/SortingKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/SparseFactories.cpp  |  4 +--
 aten/src/ATen/native/cpu/SpmmReduceKernel.h   |  4 +--
 aten/src/ATen/native/cpu/StackKernel.cpp      |  4 +--
 aten/src/ATen/native/cpu/SumKernel.cpp        |  4 +--
 .../ATen/native/cpu/TensorCompareKernel.cpp   |  4 +--
 aten/src/ATen/native/cpu/UnaryOpsKernel.cpp   |  4 +--
 aten/src/ATen/native/cpu/Unfold2d.cpp         |  4 +--
 .../ATen/native/cpu/UnfoldBackwardKernel.cpp  |  4 +--
 aten/src/ATen/native/cpu/UpSampleKernel.cpp   |  4 +--
 .../ATen/native/cpu/UpSampleMoreKernel.cpp    |  4 +--
 aten/src/ATen/native/cpu/WeightNormKernel.cpp |  4 +--
 aten/src/ATen/native/cpu/airy_ai.cpp          |  4 +--
 .../src/ATen/native/cpu/batch_norm_kernel.cpp |  4 +--
 .../src/ATen/native/cpu/group_norm_kernel.cpp |  4 +--
 .../src/ATen/native/cpu/layer_norm_kernel.cpp |  4 +--
 aten/src/ATen/native/cpu/radix_sort.h         |  8 ++---
 .../native/cpu/scaled_modified_bessel_k0.cpp  |  4 +--
 .../native/cpu/scaled_modified_bessel_k1.cpp  |  4 +--
 .../ATen/native/cpu/spherical_bessel_j0.cpp   |  4 +--
 aten/src/ATen/native/cuda/AbsKernel.cu        |  4 +--
 aten/src/ATen/native/cuda/Activation.cpp      |  4 +--
 .../ATen/native/cuda/ActivationEluKernel.cu   |  4 +--
 .../ATen/native/cuda/ActivationGeluKernel.cu  |  4 +--
 .../ATen/native/cuda/ActivationGluKernel.cu   |  5 ++-
 .../native/cuda/ActivationHardshrinkKernel.cu |  4 +--
 .../cuda/ActivationHardsigmoidKernel.cu       |  4 +--
 .../native/cuda/ActivationHardswishKernel.cu  |  4 +--
 .../native/cuda/ActivationHardtanhKernel.cu   |  4 +--
 .../native/cuda/ActivationLeakyReluKernel.cu  |  4 +--
 .../native/cuda/ActivationLogSigmoidKernel.cu |  4 +--
 .../ATen/native/cuda/ActivationMishKernel.cu  |  4 +--
 .../ATen/native/cuda/ActivationPreluKernel.cu |  4 +--
 .../ATen/native/cuda/ActivationSiluKernel.cu  |  4 +--
 .../native/cuda/ActivationSoftplusKernel.cu   |  4 +--
 .../native/cuda/ActivationSoftshrinkKernel.cu |  4 +--
 .../native/cuda/ActivationThresholdKernel.cu  |  4 +--
 .../native/cuda/AdaptiveAveragePooling.cu     |  4 +--
 .../native/cuda/AdaptiveAveragePooling3d.cu   |  4 +--
 .../ATen/native/cuda/AdaptiveMaxPooling2d.cu  |  5 ++-
 .../ATen/native/cuda/AdaptiveMaxPooling3d.cu  |  4 +--
 aten/src/ATen/native/cuda/AmpKernels.cu       |  4 +--
 aten/src/ATen/native/cuda/AveragePool2d.cu    |  4 +--
 aten/src/ATen/native/cuda/AveragePool3d.cu    |  4 +--
 .../native/cuda/BinaryBitwiseOpsKernels.cu    |  4 +--
 .../ATen/native/cuda/BinaryDivFloorKernel.cu  |  4 +--
 .../ATen/native/cuda/BinaryDivTrueKernel.cu   |  4 +--
 .../ATen/native/cuda/BinaryDivTruncKernel.cu  |  4 +--
 .../native/cuda/BinaryGeometricKernels.cu     |  4 +--
 .../native/cuda/BinaryLogicalOpsKernels.cu    |  4 +--
 .../cuda/BinaryMiscBackwardOpsKernels.cu      |  4 +--
 .../ATen/native/cuda/BinaryMiscOpsKernels.cu  |  4 +--
 aten/src/ATen/native/cuda/BinaryMulKernel.cu  |  4 +--
 .../ATen/native/cuda/BinaryRemainderKernel.cu |  4 +--
 .../ATen/native/cuda/BinaryShiftOpsKernels.cu |  4 +--
 aten/src/ATen/native/cuda/Blas.cpp            |  4 +--
 aten/src/ATen/native/cuda/Bucketization.cu    |  4 +--
 aten/src/ATen/native/cuda/CUDAScalar.cu       |  4 +--
 aten/src/ATen/native/cuda/Col2Im.cu           |  4 +--
 aten/src/ATen/native/cuda/CompareEQKernel.cu  |  4 +--
 aten/src/ATen/native/cuda/CompareKernels.cu   |  4 +--
 aten/src/ATen/native/cuda/ComplexKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/ConvolutionMM2d.cu  |  4 +--
 aten/src/ATen/native/cuda/Copy.cu             |  4 +--
 aten/src/ATen/native/cuda/CopysignKernel.cu   |  4 +--
 aten/src/ATen/native/cuda/CrossKernel.cu      |  4 +--
 aten/src/ATen/native/cuda/CumminmaxKernel.cu  |  4 +--
 aten/src/ATen/native/cuda/CumprodKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/CumsumKernel.cu     |  4 +--
 aten/src/ATen/native/cuda/DepthwiseConv2d.cu  |  4 +--
 aten/src/ATen/native/cuda/DepthwiseConv3d.cu  |  4 +--
 aten/src/ATen/native/cuda/DilatedMaxPool2d.cu |  4 +--
 aten/src/ATen/native/cuda/DilatedMaxPool3d.cu |  4 +--
 aten/src/ATen/native/cuda/DistanceKernel.cu   |  4 +--
 .../ATen/native/cuda/DistributionBernoulli.cu |  4 +--
 .../native/cuda/DistributionCauchyKernel.cu   |  4 +--
 .../cuda/DistributionExponentialKernel.cu     |  4 +--
 .../cuda/DistributionGeometricKernel.cu       |  4 +--
 .../cuda/DistributionLogNormalKernel.cu       |  4 +--
 .../ATen/native/cuda/DistributionNormal.cu    |  4 +--
 .../native/cuda/DistributionRandomKernel.cu   |  4 +--
 .../ATen/native/cuda/DistributionUniform.cu   |  4 +--
 aten/src/ATen/native/cuda/Distributions.cpp   |  4 +--
 aten/src/ATen/native/cuda/Distributions.cu    |  4 +--
 aten/src/ATen/native/cuda/Dropout.cu          |  4 +--
 aten/src/ATen/native/cuda/Embedding.cu        |  4 +--
 .../native/cuda/EmbeddingBackwardKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/EmbeddingBag.cu     |  4 +--
 aten/src/ATen/native/cuda/Equal.cpp           |  4 +--
 aten/src/ATen/native/cuda/FillKernel.cu       |  4 +--
 .../ATen/native/cuda/ForeachBinaryOpList.cu   |  4 +--
 .../ATen/native/cuda/ForeachBinaryOpScalar.cu |  4 +--
 .../native/cuda/ForeachBinaryOpScalarList.cu  |  4 +--
 .../ATen/native/cuda/ForeachPointwiseOp.cu    |  4 +--
 aten/src/ATen/native/cuda/ForeachReduceOp.cu  |  4 +--
 aten/src/ATen/native/cuda/ForeachTernaryOp.cu |  4 +--
 aten/src/ATen/native/cuda/ForeachUnaryOp.cu   |  4 +--
 .../ATen/native/cuda/FractionalMaxPool2d.cu   |  4 +--
 .../ATen/native/cuda/FractionalMaxPool3d.cu   |  4 +--
 .../cuda/FunctionOfAMatrixUtilsKernel.cu      |  4 +--
 aten/src/ATen/native/cuda/FusedAdamKernel.cu  |  4 +--
 aten/src/ATen/native/cuda/GcdLcmKernel.cu     |  4 +--
 aten/src/ATen/native/cuda/GridSampler.cpp     |  4 +--
 aten/src/ATen/native/cuda/GridSampler.cu      |  4 +--
 aten/src/ATen/native/cuda/IGammaKernel.cu     |  4 +--
 aten/src/ATen/native/cuda/Im2Col.cu           |  4 +--
 aten/src/ATen/native/cuda/IndexKernel.cpp     |  4 +--
 aten/src/ATen/native/cuda/IndexKernel.cu      |  4 +--
 aten/src/ATen/native/cuda/Indexing.cu         |  4 +--
 aten/src/ATen/native/cuda/KernelUtils.cuh     |  6 ++--
 .../ATen/native/cuda/LegacyThrustHelpers.cu   |  4 +--
 aten/src/ATen/native/cuda/Lerp.cu             |  4 +--
 aten/src/ATen/native/cuda/LinearAlgebra.cu    |  4 +--
 .../ATen/native/cuda/LinearAlgebraStubs.cpp   |  4 +--
 aten/src/ATen/native/cuda/LogAddExpKernel.cu  |  4 +--
 .../ATen/native/cuda/LogcumsumexpKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/Loss.cu             |  4 +--
 aten/src/ATen/native/cuda/LossCTC.cu          |  4 +--
 .../native/cuda/MaxMinElementwiseKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/MaxUnpooling.cu     |  4 +--
 .../native/cuda/MultiLabelMarginCriterion.cu  |  4 +--
 aten/src/ATen/native/cuda/MultiMarginLoss.cu  |  4 +--
 .../src/ATen/native/cuda/MultinomialKernel.cu |  4 +--
 aten/src/ATen/native/cuda/NLLLoss2d.cu        |  4 +--
 .../cuda/NaiveConvolutionTranspose2d.cu       |  4 +--
 .../cuda/NaiveConvolutionTranspose3d.cu       |  4 +--
 .../native/cuda/NaiveDilatedConvolution.cu    |  4 +--
 aten/src/ATen/native/cuda/Nonzero.cu          |  4 +--
 aten/src/ATen/native/cuda/Normalization.cu    |  4 +--
 aten/src/ATen/native/cuda/Normalization.cuh   |  2 +-
 .../ATen/native/cuda/PointwiseOpsKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/PowKernel.cu        |  4 +--
 aten/src/ATen/native/cuda/RNN.cu              |  4 +--
 aten/src/ATen/native/cuda/Randperm.cu         |  4 +--
 aten/src/ATen/native/cuda/RangeFactories.cu   |  4 +--
 aten/src/ATen/native/cuda/RecordStream.cu     |  4 +--
 aten/src/ATen/native/cuda/Reduce.cu           |  4 +--
 .../ATen/native/cuda/ReduceAMinMaxKernel.cu   |  4 +--
 .../ATen/native/cuda/ReduceArgMaxKernel.cu    |  4 +--
 .../ATen/native/cuda/ReduceArgMinKernel.cu    |  4 +--
 .../src/ATen/native/cuda/ReduceLogicKernel.cu |  4 +--
 .../ATen/native/cuda/ReduceMaxValuesKernel.cu |  4 +--
 .../ATen/native/cuda/ReduceMinValuesKernel.cu |  4 +--
 .../ATen/native/cuda/ReduceMomentKernel.cu    |  4 +--
 aten/src/ATen/native/cuda/ReduceNormKernel.cu |  4 +--
 aten/src/ATen/native/cuda/ReduceOps.cpp       |  4 +--
 .../ATen/native/cuda/ReduceSumProdKernel.cu   |  4 +--
 aten/src/ATen/native/cuda/ReflectionPad.cu    |  4 +--
 aten/src/ATen/native/cuda/RenormKernel.cu     |  4 +--
 aten/src/ATen/native/cuda/Repeat.cu           |  4 +--
 .../ATen/native/cuda/ReplicationPadding.cu    |  4 +--
 aten/src/ATen/native/cuda/Resize.cpp          |  4 +--
 aten/src/ATen/native/cuda/RreluWithNoise.cu   |  4 +--
 aten/src/ATen/native/cuda/ScanKernels.cpp     |  4 +--
 .../ATen/native/cuda/ScatterGatherKernel.cu   |  4 +--
 aten/src/ATen/native/cuda/SegmentReduce.cu    |  4 +--
 aten/src/ATen/native/cuda/Shape.cu            |  4 +--
 aten/src/ATen/native/cuda/SoftMax.cu          |  4 +--
 aten/src/ATen/native/cuda/Sort.cpp            |  4 +--
 aten/src/ATen/native/cuda/Sort.cu             |  4 +--
 aten/src/ATen/native/cuda/SortImpl.cu         |  4 +--
 aten/src/ATen/native/cuda/SortStable.cu       |  4 +--
 aten/src/ATen/native/cuda/Sorting.cpp         |  4 +--
 aten/src/ATen/native/cuda/Sorting.cu          |  4 +--
 .../cuda/SparseBinaryOpIntersectionKernel.cu  |  4 +--
 aten/src/ATen/native/cuda/SparseMM.cu         |  4 +--
 aten/src/ATen/native/cuda/SpectralOps.cpp     |  5 +--
 aten/src/ATen/native/cuda/SpectralOps.cu      |  4 +--
 aten/src/ATen/native/cuda/StepKernel.cu       |  4 +--
 aten/src/ATen/native/cuda/TensorCompare.cpp   |  4 +--
 aten/src/ATen/native/cuda/TensorCompare.cu    |  4 +--
 aten/src/ATen/native/cuda/TensorFactories.cu  |  4 +--
 .../src/ATen/native/cuda/TensorModeKernel.cpp |  4 +--
 aten/src/ATen/native/cuda/TensorModeKernel.cu |  4 +--
 aten/src/ATen/native/cuda/TensorShapeCUDA.cpp |  4 +--
 aten/src/ATen/native/cuda/TensorTopK.cpp      |  4 +--
 aten/src/ATen/native/cuda/TensorTopK.cu       |  6 ++--
 .../ATen/native/cuda/TensorTransformations.cu |  4 +--
 aten/src/ATen/native/cuda/TriangularOps.cu    |  4 +--
 .../ATen/native/cuda/UnaryComplexKernels.cu   |  4 +--
 .../ATen/native/cuda/UnaryFractionKernels.cu  |  4 +--
 .../src/ATen/native/cuda/UnaryGammaKernels.cu |  4 +--
 .../native/cuda/UnaryGeometricAcosKernel.cu   |  4 +--
 .../native/cuda/UnaryGeometricAcoshKernel.cu  |  4 +--
 .../native/cuda/UnaryGeometricAsinKernel.cu   |  4 +--
 .../native/cuda/UnaryGeometricAsinhKernel.cu  |  4 +--
 .../native/cuda/UnaryGeometricAtanKernel.cu   |  4 +--
 .../native/cuda/UnaryGeometricAtanhKernel.cu  |  4 +--
 .../native/cuda/UnaryGeometricCosKernel.cu    |  4 +--
 .../native/cuda/UnaryGeometricCoshKernel.cu   |  4 +--
 .../native/cuda/UnaryGeometricSinKernel.cu    |  4 +--
 .../native/cuda/UnaryGeometricSinhKernel.cu   |  4 +--
 .../native/cuda/UnaryGeometricTanKernel.cu    |  4 +--
 .../native/cuda/UnaryGeometricTanhKernel.cu   |  4 +--
 aten/src/ATen/native/cuda/UnaryLogKernels.cu  |  4 +--
 aten/src/ATen/native/cuda/UnaryOpsKernel.cu   |  4 +--
 aten/src/ATen/native/cuda/UnarySignKernels.cu |  4 +--
 .../ATen/native/cuda/UnarySpecialOpsKernel.cu |  4 +--
 .../ATen/native/cuda/UnfoldBackwardKernel.cu  |  4 +--
 aten/src/ATen/native/cuda/Unique.cu           |  7 ++--
 aten/src/ATen/native/cuda/UniqueCub.cu        |  4 +--
 .../src/ATen/native/cuda/UpSampleBicubic2d.cu |  4 +--
 .../ATen/native/cuda/UpSampleBilinear2d.cu    |  4 +--
 aten/src/ATen/native/cuda/UpSampleLinear1d.cu |  4 +--
 .../src/ATen/native/cuda/UpSampleNearest1d.cu |  4 +--
 .../src/ATen/native/cuda/UpSampleNearest2d.cu |  4 +--
 .../src/ATen/native/cuda/UpSampleNearest3d.cu |  4 +--
 .../ATen/native/cuda/UpSampleTrilinear3d.cu   |  4 +--
 .../cuda/ValidateCompressedIndicesKernel.cu   |  4 +--
 aten/src/ATen/native/cuda/WeightNorm.cu       |  4 +--
 aten/src/ATen/native/cuda/ZetaKernel.cu       |  4 +--
 aten/src/ATen/native/cuda/airy_ai.cu          |  4 +--
 aten/src/ATen/native/cuda/bessel_j0.cu        |  4 +--
 aten/src/ATen/native/cuda/bessel_j1.cu        |  4 +--
 aten/src/ATen/native/cuda/bessel_y0.cu        |  4 +--
 aten/src/ATen/native/cuda/bessel_y1.cu        |  4 +--
 .../native/cuda/chebyshev_polynomial_t.cu     |  4 +--
 .../native/cuda/chebyshev_polynomial_u.cu     |  4 +--
 .../native/cuda/chebyshev_polynomial_v.cu     |  4 +--
 .../native/cuda/chebyshev_polynomial_w.cu     |  4 +--
 .../native/cuda/fused_adam_amsgrad_impl.cu    |  4 +--
 aten/src/ATen/native/cuda/fused_adam_impl.cu  |  4 +--
 .../src/ATen/native/cuda/group_norm_kernel.cu |  4 +--
 .../ATen/native/cuda/hermite_polynomial_h.cu  |  4 +--
 .../ATen/native/cuda/hermite_polynomial_he.cu |  4 +--
 .../ATen/native/cuda/laguerre_polynomial_l.cu |  4 +--
 .../src/ATen/native/cuda/layer_norm_kernel.cu |  4 +--
 .../ATen/native/cuda/legendre_polynomial_p.cu |  4 +--
 .../native/cuda/linalg/BatchLinearAlgebra.cpp |  4 +--
 .../cuda/linalg/BatchLinearAlgebraLib.cpp     |  4 +--
 .../ATen/native/cuda/modified_bessel_i0.cu    |  4 +--
 .../ATen/native/cuda/modified_bessel_i1.cu    |  4 +--
 .../ATen/native/cuda/modified_bessel_k0.cu    |  4 +--
 .../ATen/native/cuda/modified_bessel_k1.cu    |  4 +--
 .../native/cuda/scaled_modified_bessel_k0.cu  |  4 +--
 .../native/cuda/scaled_modified_bessel_k1.cu  |  4 +--
 .../cuda/shifted_chebyshev_polynomial_t.cu    |  4 +--
 .../cuda/shifted_chebyshev_polynomial_u.cu    |  4 +--
 .../cuda/shifted_chebyshev_polynomial_v.cu    |  4 +--
 .../cuda/shifted_chebyshev_polynomial_w.cu    |  4 +--
 .../ATen/native/cuda/spherical_bessel_j0.cu   |  4 +--
 aten/src/ATen/native/mkldnn/RNN.cpp           |  8 ++---
 aten/src/ATen/native/mps/OperationUtils.mm    |  2 +-
 aten/src/ATen/native/mps/TensorFactory.cpp    |  4 +--
 .../ATen/native/mps/operations/Activation.mm  |  4 +--
 .../native/mps/operations/AdaptivePooling.mm  |  4 +--
 .../ATen/native/mps/operations/BinaryOps.mm   |  4 +--
 aten/src/ATen/native/mps/operations/Blas.mm   |  4 +--
 .../ATen/native/mps/operations/ConstantOps.mm |  4 +--
 .../ATen/native/mps/operations/Convolution.mm |  4 +--
 aten/src/ATen/native/mps/operations/Copy.mm   |  4 +--
 .../ATen/native/mps/operations/CrossKernel.mm |  4 +--
 .../native/mps/operations/Distributions.mm    |  4 +--
 aten/src/ATen/native/mps/operations/Eye.mm    |  4 +--
 .../ATen/native/mps/operations/Indexing.mm    |  4 +--
 .../src/ATen/native/mps/operations/Inverse.mm |  4 +--
 aten/src/ATen/native/mps/operations/Linear.mm |  4 +--
 .../native/mps/operations/LinearAlgebra.mm    |  4 +--
 .../src/ATen/native/mps/operations/LossOps.mm |  4 +--
 .../native/mps/operations/Normalization.mm    |  4 +--
 aten/src/ATen/native/mps/operations/Pad.mm    |  4 +--
 .../native/mps/operations/PointwiseOps.mm     |  4 +--
 .../src/ATen/native/mps/operations/Pooling.mm |  4 +--
 .../native/mps/operations/RangeFactories.mm   |  4 +--
 .../ATen/native/mps/operations/ReduceOps.mm   |  4 +--
 aten/src/ATen/native/mps/operations/Repeat.mm |  2 +-
 aten/src/ATen/native/mps/operations/RnnOps.mm |  2 +-
 aten/src/ATen/native/mps/operations/Scalar.mm |  4 +--
 .../native/mps/operations/ScatterGather.mm    |  4 +--
 aten/src/ATen/native/mps/operations/Shape.mm  |  4 +--
 .../src/ATen/native/mps/operations/SoftMax.mm |  4 +--
 aten/src/ATen/native/mps/operations/Sort.mm   |  2 +-
 .../ATen/native/mps/operations/SummaryOps.mm  |  4 +--
 .../native/mps/operations/TensorCompare.mm    |  4 +--
 .../native/mps/operations/TriangularOps.mm    |  4 +--
 .../ATen/native/mps/operations/UnaryOps.mm    |  4 +--
 aten/src/ATen/native/mps/operations/Unique.mm |  4 +--
 .../ATen/native/mps/operations/UpSample.mm    |  4 +--
 aten/src/ATen/native/mps/operations/View.mm   |  4 +--
 .../native/sparse/cuda/SparseBlasImpl.cpp     |  5 +--
 .../sparse/cuda/SparseCUDATensorMath.cu       | 11 +++++--
 third_party/nvfuser/CMakeLists.txt            |  7 +++-
 third_party/nvfuser/csrc/executor.cpp         |  4 +--
 325 files changed, 697 insertions(+), 660 deletions(-)

diff --git a/aten/src/ATen/ConjugateFallback.cpp b/aten/src/ATen/ConjugateFallback.cpp
index cb029d97230..80f9b58d144 100644
--- a/aten/src/ATen/ConjugateFallback.cpp
+++ b/aten/src/ATen/ConjugateFallback.cpp
@@ -1,7 +1,7 @@
 #include <ATen/native/MathBitsFallback.h>
 #include <ATen/native/MathBitFallThroughLists.h>
 
-namespace at::native {
+namespace at { namespace native {
 struct ConjFallback : MathOpFallback {
   ConjFallback() : MathOpFallback(DispatchKey::Conjugate, "conjugate") {}
   bool is_bit_set(const Tensor& tensor) override {
@@ -62,4 +62,4 @@ TORCH_LIBRARY_IMPL(aten, Conjugate, m) {
   TORCH_VIEW_FNS_NATIVE_FN_REGISTRATION(m)
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/cuda/Atomic.cuh b/aten/src/ATen/cuda/Atomic.cuh
index 37033eb17fb..f20466e4650 100644
--- a/aten/src/ATen/cuda/Atomic.cuh
+++ b/aten/src/ATen/cuda/Atomic.cuh
@@ -6,7 +6,8 @@
 
 #include <ATen/NumericUtils.h>
 
-#if !(defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))
+// #if !(defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))) && CUDA_VERSION >= 11000
+#if CUDA_VERSION >= 11000
 #include <cuda_bf16.h>
 #endif
 
@@ -223,7 +224,7 @@ static inline  __device__ at::Half gpuAtomicAdd(at::Half *address, at::Half val)
 }
 
 static inline __device__ at::BFloat16 gpuAtomicAdd(at::BFloat16 *address, at::BFloat16 val) {
-#if defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))
+#if defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))) || CUDA_VERSION < 11000
 return AtomicFPOp<at::BFloat16>()(address, val,
                                   [](at::BFloat16 bsum, at::BFloat16 val) {
                                     return bsum + val;
diff --git a/aten/src/ATen/cuda/CUDABlas.cpp b/aten/src/ATen/cuda/CUDABlas.cpp
index 659ef114120..7f5e464126d 100644
--- a/aten/src/ATen/cuda/CUDABlas.cpp
+++ b/aten/src/ATen/cuda/CUDABlas.cpp
@@ -15,6 +15,10 @@
 #include <cublasLt.h>
 #endif
 
+// refer to http://www.jcuda.org/jcuda/jcublas/doc/constant-values.html#jcuda.jcublas.cublasMath.CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION
+// for more details
+static int CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION = 16;
+
 #ifdef USE_ROCM
 #define PYTORCH_ROCBLAS_VERSION_DECIMAL (ROCBLAS_VERSION_MAJOR * 100 + ROCBLAS_VERSION_MINOR)
 #define USE_GEMM_FLAGS_FP16_ALT_IMPL (PYTORCH_ROCBLAS_VERSION_DECIMAL >= 242)
@@ -286,6 +290,7 @@ void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half)) {
 
 template <>
 void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16)) {
+#if defined(USE_ROCM) || defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   // See Note [Writing Nondeterministic Operations]
   globalContext().alertCuBLASConfigNotDeterministic();
   BGEMM_CHECK_ARGVALUES(at::BFloat16);
@@ -295,15 +300,16 @@ void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16)) {
   const float falpha = alpha;
   const float fbeta = beta;
   _cublasAdjustLdLevel3(transa, transb, m, n, k, &lda, &ldb, &ldc);
+#endif
 
-  #if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && CUDA_VERSION >= 11000
     TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedExFix(handle,
                                     opa, opb, (int)m, (int)n, (int)k,
                                     (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea,
                                     b, CUDA_R_16BF, (int)ldb, strideb,
                                     (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec,
                                     (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP));
-  #else
+#elif defined(ROCM_VERSION) && ROCM_VERSION >= 21000
     TORCH_CUDABLAS_CHECK(rocblas_gemm_strided_batched_ex(handle, opa, opb, (int)m, (int)n, (int)k,
                                    (void*)&falpha, a, rocblas_datatype_bf16_r, (int)lda, stridea,
                                    b, rocblas_datatype_bf16_r, (int)ldb, strideb,
@@ -311,7 +317,9 @@ void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16)) {
                                    c, rocblas_datatype_bf16_r, (int)ldc, stridec,
                                    (int) num_batches, rocblas_datatype_f32_r, rocblas_gemm_algo_standard,
                                    0, 0, NULL, NULL));
-  #endif // !defined(USE_ROCM)
+#else
+  AT_ERROR("Cublas_bfdot requires CUDA 11.0+");
+#endif
 }
 
 template <>
@@ -418,12 +426,18 @@ void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half)) {
 #else
   cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
   if (prop->major >= 5) {
+#if defined(CUDA_VERSION) && CUDA_VERSION < 11000
+    // On CUDA versions prior to 11, users are required to set the math mode to CUBLAS_TENSOR_OP_MATH
+    // manually to be able to use tensor cores for FP16. On CUDA 11, this is no longer required.
+    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
+#else
     cublasMath_t cublas_flags = CUBLAS_DEFAULT_MATH;
     if (!at::globalContext().allowFP16ReductionCuBLAS()) {
       cublas_flags = static_cast<cublasMath_t>(cublas_flags | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION);
     }
     // Disallow fp16 reductions that could lead to unexpected overflow issues.
     TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, cublas_flags));
+#endif  // defined(CUDA_VERSION) && CUDA_VERSION < 11000
     TORCH_CUDABLAS_CHECK(cublasGemmEx(
         handle,
         opa,
@@ -506,7 +520,8 @@ void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
 }
 #endif
 
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) 
+// #if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
   globalContext().alertCuBLASConfigNotDeterministic();
@@ -531,14 +546,14 @@ void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
       k,
       &falpha,
       a,
-      CUDA_R_16BF,
+      CUDA_R_16F,
       lda,
       b,
-      CUDA_R_16BF,
+      CUDA_R_16F,
       ldb,
       &fbeta,
       c,
-      CUDA_R_16BF,
+      CUDA_R_16F,
       ldc,
       CUDA_R_32F,
       CUBLAS_GEMM_DFALT_TENSOR_OP));
@@ -546,7 +561,7 @@ void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
 }
 #endif // !defined(USE_ROCM)
 
-#if !defined(USE_ROCM) && !defined(_MSC_VER)
+#if !defined(USE_ROCM) && !defined(_MSC_VER) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 
 namespace {
 // Following the pattern of CuSparseDescriptor
@@ -1130,7 +1145,7 @@ void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half)) {
 
 template <>
 void dot<at::BFloat16>(CUDABLAS_DOT_ARGTYPES(at::BFloat16)) {
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   TORCH_CUDABLAS_CHECK(cublasDotEx(
       handle,
       n,
diff --git a/aten/src/ATen/cuda/CUDADataType.h b/aten/src/ATen/cuda/CUDADataType.h
index 97dec2e3cda..34e9142c651 100644
--- a/aten/src/ATen/cuda/CUDADataType.h
+++ b/aten/src/ATen/cuda/CUDADataType.h
@@ -45,7 +45,7 @@ template<> inline cudaDataType getCudaDataType<int>() {
 }
 #endif
 
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && CUDA_VERSION >= 11000
 template<> inline cudaDataType getCudaDataType<int16_t>() {
   return CUDA_R_16I;
 }
@@ -60,7 +60,7 @@ template<> inline cudaDataType getCudaDataType<at::BFloat16>() {
 inline cudaDataType ScalarTypeToCudaDataType(const c10::ScalarType& scalar_type) {
   switch (scalar_type) {
 // HIP doesn't define integral types
-#ifndef USE_ROCM
+#if !defined(USE_ROCM) && CUDA_VERSION >= 11000
     case c10::ScalarType::Byte:
       return CUDA_R_8U;
     case c10::ScalarType::Char:
@@ -80,7 +80,7 @@ inline cudaDataType ScalarTypeToCudaDataType(const c10::ScalarType& scalar_type)
       return CUDA_C_32F;
     case c10::ScalarType::ComplexDouble:
       return CUDA_C_64F;
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && CUDA_VERSION >= 11000
     case c10::ScalarType::Short:
       return CUDA_R_16I;
     case c10::ScalarType::Long:
diff --git a/aten/src/ATen/cuda/CUDASparseDescriptors.cpp b/aten/src/ATen/cuda/CUDASparseDescriptors.cpp
index 7c505955576..7b9c4d44d9e 100644
--- a/aten/src/ATen/cuda/CUDASparseDescriptors.cpp
+++ b/aten/src/ATen/cuda/CUDASparseDescriptors.cpp
@@ -26,7 +26,7 @@ void check_supported_cuda_type(cudaDataType cuda_type) {
         prop->minor,
         ")");
   }
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   if (cuda_type == CUDA_R_16BF) {
     cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
     TORCH_CHECK(
@@ -177,7 +177,7 @@ CuSparseSpMatCsrDescriptor::CuSparseSpMatCsrDescriptor(const Tensor& input, int6
       value_type // data type of values
       ));
 
-#if AT_USE_HIPSPARSE_GENERIC_52_API() || !defined(USE_ROCM)
+#if (AT_USE_HIPSPARSE_GENERIC_52_API() || !defined(USE_ROCM)) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   if (ndim == 3 && batch_offset == -1) {
     int batch_count =
         at::native::cuda_int_cast(at::native::batchCount(input), "batch_count");
diff --git a/aten/src/ATen/cuda/CUDASparseDescriptors.h b/aten/src/ATen/cuda/CUDASparseDescriptors.h
index f2681d143db..efab381e737 100644
--- a/aten/src/ATen/cuda/CUDASparseDescriptors.h
+++ b/aten/src/ATen/cuda/CUDASparseDescriptors.h
@@ -177,11 +177,13 @@ class TORCH_CUDA_CPP_API CuSparseSpMatCsrDescriptor
 
   std::tuple<int64_t, int64_t, int64_t> get_size() {
     int64_t rows, cols, nnz;
+#if CUDA_VERSION >= 11000
     TORCH_CUDASPARSE_CHECK(cusparseSpMatGetSize(
         this->descriptor(),
         &rows,
         &cols,
         &nnz));
+#endif
     return std::make_tuple(rows, cols, nnz);
   }
 
@@ -193,11 +195,13 @@ class TORCH_CUDA_CPP_API CuSparseSpMatCsrDescriptor
     TORCH_INTERNAL_ASSERT_DEBUG_ONLY(crow_indices.is_contiguous());
     TORCH_INTERNAL_ASSERT_DEBUG_ONLY(col_indices.is_contiguous());
     TORCH_INTERNAL_ASSERT_DEBUG_ONLY(values.is_contiguous());
+#if CUDA_VERSION >= 11000
     TORCH_CUDASPARSE_CHECK(cusparseCsrSetPointers(
         this->descriptor(),
         crow_indices.data_ptr(),
         col_indices.data_ptr(),
         values.data_ptr()));
+#endif
   }
 
 #if AT_USE_CUSPARSE_GENERIC_SPSV()
@@ -241,13 +245,15 @@ class TORCH_CUDA_CPP_API CuSparseSpSMDescriptor
  public:
   CuSparseSpSMDescriptor() {
     cusparseSpSMDescr_t raw_descriptor;
+#if CUDA_VERSION >= 11000
     TORCH_CUDASPARSE_CHECK(cusparseSpSM_createDescr(&raw_descriptor));
+#endif
     descriptor_.reset(raw_descriptor);
   }
 };
 #endif
-
-#if (defined(USE_ROCM) && ROCM_VERSION >= 50200) || !defined(USE_ROCM)
+ 
+#if (defined(USE_ROCM) && ROCM_VERSION >= 50200) || ( !defined(USE_ROCM) && CUDA_VERSION >= 11000 )
 class TORCH_CUDA_CPP_API CuSparseSpGEMMDescriptor
     : public CuSparseDescriptor<cusparseSpGEMMDescr, &cusparseSpGEMM_destroyDescr> {
  public:
diff --git a/aten/src/ATen/cuda/CublasHandlePool.cpp b/aten/src/ATen/cuda/CublasHandlePool.cpp
index 68a4951bcce..58e8cc66238 100644
--- a/aten/src/ATen/cuda/CublasHandlePool.cpp
+++ b/aten/src/ATen/cuda/CublasHandlePool.cpp
@@ -101,7 +101,7 @@ cublasHandle_t getCurrentCUDABlasHandle() {
   auto handle = myPoolWindow->reserve(device);
   auto stream = c10::cuda::getCurrentCUDAStream();
   TORCH_CUDABLAS_CHECK(cublasSetStream(handle, stream));
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   // cublasSetWorkspace not available on CUDA 10.2
   cudaStream_t _stream = stream;
   auto key = std::make_tuple(static_cast<void *>(handle), static_cast<void *>(_stream));
@@ -111,7 +111,7 @@ cublasHandle_t getCurrentCUDABlasHandle() {
   }
   TORCH_CUDABLAS_CHECK(cublasSetWorkspace(handle, workspace_it->second.get(), getChosenWorkspaceSize()));
 #endif
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   // On CUDA >= 11, and architecture >= Ampere, cuBLAS can use TF32 to speedup
   // FP32 data type calculations based on the value of the allow_tf32 flag.
   // To enable TF32, set the math mode of the handle to CUBLAS_TF32_TENSOR_OP_MATH.
diff --git a/aten/src/ATen/native/cpu/Activation.cpp b/aten/src/ATen/native/cpu/Activation.cpp
index f679cafb67d..42af4bfc066 100644
--- a/aten/src/ATen/native/cpu/Activation.cpp
+++ b/aten/src/ATen/native/cpu/Activation.cpp
@@ -19,7 +19,7 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -1402,4 +1402,4 @@ REGISTER_DISPATCH(mish_backward_stub, &mish_backward_kernel);
 REGISTER_DISPATCH(prelu_stub, &prelu_kernel);
 REGISTER_DISPATCH(prelu_backward_stub, &prelu_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp b/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp
index b1069d2cc2a..b121e239025 100644
--- a/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp
@@ -8,7 +8,7 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -413,4 +413,4 @@ void adapative_avg_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(adaptive_avg_pool2d_kernel, &adaptive_avg_pool2d_kernel_impl);
 REGISTER_DISPATCH(adaptive_avg_pool2d_backward_kernel, &adapative_avg_pool2d_backward_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp b/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
index 865a93f309e..123bc4e6fa7 100644
--- a/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
@@ -8,7 +8,7 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -499,4 +499,4 @@ void adaptive_max_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(adaptive_max_pool2d_kernel, &adaptive_max_pool2d_kernel_impl);
 REGISTER_DISPATCH(adaptive_max_pool2d_backward_kernel, &adaptive_max_pool2d_backward_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/AvgPoolKernel.cpp b/aten/src/ATen/native/cpu/AvgPoolKernel.cpp
index 7a28b977f9d..df51715e163 100644
--- a/aten/src/ATen/native/cpu/AvgPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AvgPoolKernel.cpp
@@ -7,7 +7,7 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -549,4 +549,4 @@ void avg_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(avg_pool2d_kernel, &avg_pool2d_kernel_impl);
 REGISTER_DISPATCH(avg_pool2d_backward_kernel, &avg_pool2d_backward_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp b/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
index a9e8cf2243f..1a46f075e10 100644
--- a/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
@@ -15,7 +15,7 @@
 #include <c10/util/TypeSafeSignMath.h>
 #include <c10/util/copysign.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -1270,4 +1270,4 @@ REGISTER_DISPATCH(shifted_chebyshev_polynomial_u_stub, &shifted_chebyshev_polyno
 REGISTER_DISPATCH(shifted_chebyshev_polynomial_v_stub, &shifted_chebyshev_polynomial_v_kernel);
 REGISTER_DISPATCH(shifted_chebyshev_polynomial_w_stub, &shifted_chebyshev_polynomial_w_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/BlasKernel.cpp b/aten/src/ATen/native/cpu/BlasKernel.cpp
index e143d8a6546..70becd5a85a 100644
--- a/aten/src/ATen/native/cpu/BlasKernel.cpp
+++ b/aten/src/ATen/native/cpu/BlasKernel.cpp
@@ -4,7 +4,7 @@
 #include <c10/util/irange.h>
 #include <c10/util/Unroll.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace cpublas {
 namespace {
 
@@ -247,4 +247,4 @@ REGISTER_DISPATCH(cpublas::gemm_stub, &cpublas::cpublas_gemm_impl);
 REGISTER_DISPATCH(cpublas::axpy_stub, &cpublas::cpublas_axpy_impl);
 REGISTER_DISPATCH(cpublas::copy_stub, &cpublas::cpublas_copy_impl);
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cpu/CatKernel.cpp b/aten/src/ATen/native/cpu/CatKernel.cpp
index 3641b02fbbe..c4fa1bb0540 100644
--- a/aten/src/ATen/native/cpu/CatKernel.cpp
+++ b/aten/src/ATen/native/cpu/CatKernel.cpp
@@ -7,7 +7,7 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -65,4 +65,4 @@ void cat_serial_kernel(const Tensor& result, const MaterializedITensorListRef& t
 
 REGISTER_DISPATCH(cat_serial_stub, &cat_serial_kernel);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp b/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp
index d2494970c9b..57bd4f3badc 100644
--- a/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp
+++ b/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp
@@ -8,7 +8,7 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -113,4 +113,4 @@ void channel_shuffle_kernel_impl(
 
 REGISTER_DISPATCH(channel_shuffle_kernel, &channel_shuffle_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/ComplexKernel.cpp b/aten/src/ATen/native/cpu/ComplexKernel.cpp
index e9e72ff758d..b9b45b4ad3a 100644
--- a/aten/src/ATen/native/cpu/ComplexKernel.cpp
+++ b/aten/src/ATen/native/cpu/ComplexKernel.cpp
@@ -4,7 +4,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void complex_kernel(TensorIterator& iter) {
@@ -28,4 +28,4 @@ void polar_kernel(TensorIterator& iter) {
 REGISTER_DISPATCH(complex_stub, &complex_kernel);
 REGISTER_DISPATCH(polar_stub, &polar_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/CopyKernel.cpp b/aten/src/ATen/native/cpu/CopyKernel.cpp
index adfb550b2a6..eab97486be3 100644
--- a/aten/src/ATen/native/cpu/CopyKernel.cpp
+++ b/aten/src/ATen/native/cpu/CopyKernel.cpp
@@ -8,7 +8,7 @@
 #include <ATen/TensorIteratorInternal.h>
 #include <ATen/Parallel.h>
 
-namespace at::native {
+namespace at { namespace native {
 inline namespace CPU_CAPABILITY {
 void neg_kernel(TensorIteratorBase &iter);
 void conj_kernel(TensorIteratorBase &iter);
@@ -270,4 +270,4 @@ void copy_kernel(TensorIterator& iter, bool /*non_blocking*/) {
 
 REGISTER_DISPATCH(copy_stub, &copy_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/CrossKernel.cpp b/aten/src/ATen/native/cpu/CrossKernel.cpp
index 97249807bb8..c33af3ae750 100644
--- a/aten/src/ATen/native/cpu/CrossKernel.cpp
+++ b/aten/src/ATen/native/cpu/CrossKernel.cpp
@@ -11,7 +11,7 @@
 #include <ATen/Parallel.h>
 #include <ATen/TensorIterator.h>
 #include <c10/util/irange.h>
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template<typename scalar_t>
@@ -78,4 +78,4 @@ static void cross_kernel_impl(const Tensor& result, const Tensor& a, const Tenso
 
 REGISTER_DISPATCH(cross_stub, &cross_kernel_impl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp b/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp
index 8688bd688b0..2f1bbb3fb6d 100644
--- a/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp
+++ b/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp
@@ -15,7 +15,7 @@
 #include <arm_neon.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 struct Arguments final {
@@ -312,4 +312,4 @@ Tensor _convolution_depthwise3x3_winograd(
 
 REGISTER_DISPATCH(convolution_depthwise3x3_winograd_stub, &_convolution_depthwise3x3_winograd);
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp b/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp
index f2346759cbc..5f4dd09c603 100644
--- a/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp
@@ -10,7 +10,7 @@
 #include <ATen/cpu/vec/functional.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template<typename scalar_t>
@@ -448,4 +448,4 @@ REGISTER_DISPATCH(pdist_backward_stub, &pdist_backward_kernel_impl);
 REGISTER_DISPATCH(cdist_stub, &cdist_kernel_impl);
 REGISTER_DISPATCH(cdist_backward_stub, &cdist_backward_kernel_impl);
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cpu/DistributionKernels.cpp b/aten/src/ATen/native/cpu/DistributionKernels.cpp
index 5b9d844b7a3..2fd869d9d03 100644
--- a/aten/src/ATen/native/cpu/DistributionKernels.cpp
+++ b/aten/src/ATen/native/cpu/DistributionKernels.cpp
@@ -23,7 +23,7 @@
 #include <cpuinfo.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 static void cauchy_kernel(TensorIteratorBase& iter, double median, double sigma, c10::optional<Generator> gen) {
@@ -239,4 +239,4 @@ REGISTER_DISPATCH(random_from_to_stub, &random_from_to_kernel);
 REGISTER_DISPATCH(random_full_64_bits_range_stub, &random_full_64_bits_range_kernel);
 REGISTER_DISPATCH(random_stub, &random_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/FillKernel.cpp b/aten/src/ATen/native/cpu/FillKernel.cpp
index a04ffdbf904..fab989a94cc 100644
--- a/aten/src/ATen/native/cpu/FillKernel.cpp
+++ b/aten/src/ATen/native/cpu/FillKernel.cpp
@@ -9,7 +9,7 @@
 #include <ATen/native/Fill.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 
@@ -58,4 +58,4 @@ void fill_kernel(TensorIterator& iter, const Scalar& value_scalar) {
 
 REGISTER_DISPATCH(fill_stub, &fill_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp b/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp
index 92cf41c309e..de3be1587e5 100644
--- a/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp
+++ b/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp
@@ -11,7 +11,7 @@
 #define RESTRICT __restrict__
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -54,4 +54,4 @@ void _compute_linear_combination_cpu_kernel(
 
 REGISTER_DISPATCH(_compute_linear_combination_stub, &_compute_linear_combination_cpu_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/GridSamplerKernel.cpp b/aten/src/ATen/native/cpu/GridSamplerKernel.cpp
index c65bdfcd6d6..c80c5d2f000 100644
--- a/aten/src/ATen/native/cpu/GridSamplerKernel.cpp
+++ b/aten/src/ATen/native/cpu/GridSamplerKernel.cpp
@@ -14,7 +14,7 @@
 #include <cstring>
 #include <type_traits>
 
-namespace at::native { namespace {
+namespace at { namespace native { namespace {
 
 /**  NOTE [ Grid Sample CPU Kernels ]
  *
@@ -1319,4 +1319,4 @@ REGISTER_DISPATCH(grid_sampler_2d_cpu_kernel, &grid_sampler_2d_cpu_kernel_impl);
 REGISTER_DISPATCH(grid_sampler_2d_backward_cpu_kernel, &grid_sampler_2d_backward_cpu_kernel_impl);
 
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cpu/HistogramKernel.cpp b/aten/src/ATen/native/cpu/HistogramKernel.cpp
index e640aef8dde..83011aa2e9a 100644
--- a/aten/src/ATen/native/cpu/HistogramKernel.cpp
+++ b/aten/src/ATen/native/cpu/HistogramKernel.cpp
@@ -19,7 +19,7 @@
 #include <numeric>
 #include <functional>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -288,4 +288,4 @@ REGISTER_DISPATCH(histogramdd_stub, &histogramdd_kernel_impl);
 
 REGISTER_DISPATCH(histogramdd_linear_stub, &histogramdd_linear_kernel_impl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/IndexKernel.cpp b/aten/src/ATen/native/cpu/IndexKernel.cpp
index 7ac9c3ff607..ddcbf09ad90 100644
--- a/aten/src/ATen/native/cpu/IndexKernel.cpp
+++ b/aten/src/ATen/native/cpu/IndexKernel.cpp
@@ -15,7 +15,7 @@
 #include <c10/util/irange.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using namespace vec;
@@ -797,4 +797,4 @@ REGISTER_DISPATCH(masked_select_stub, &masked_select_kernel);
 REGISTER_DISPATCH(masked_scatter_stub, &masked_scatter_kernel);
 REGISTER_DISPATCH(flip_stub, &flip_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp b/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp
index f93394bb5eb..d67769dead4 100644
--- a/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp
+++ b/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp
@@ -8,7 +8,7 @@
 #include <ATen/native/cpu/Loops.h>
 #include <c10/util/irange.h>
 
-namespace at::native { namespace {
+namespace at { namespace native { namespace {
 
 void addr_kernel(TensorIterator &iter,
                  const Scalar& beta, const Scalar& alpha) {
@@ -86,4 +86,4 @@ void addr_kernel(TensorIterator &iter,
 } // anonymous namespace
 
 REGISTER_DISPATCH(addr_stub, &addr_kernel);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/MaxPoolKernel.cpp b/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
index 3da7e6fd98b..3c3c7e74620 100644
--- a/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
@@ -9,7 +9,7 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -522,4 +522,4 @@ void max_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(max_pool2d_kernel, &max_pool2d_kernel_impl);
 REGISTER_DISPATCH(max_pool2d_backward_kernel, &max_pool2d_backward_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/MaxPooling.cpp b/aten/src/ATen/native/cpu/MaxPooling.cpp
index 4a6b4b557ac..a5a2b9bc003 100644
--- a/aten/src/ATen/native/cpu/MaxPooling.cpp
+++ b/aten/src/ATen/native/cpu/MaxPooling.cpp
@@ -6,7 +6,7 @@
 #include <ATen/native/MaxPooling.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -70,4 +70,4 @@ void max_pool1d_impl(
 
 REGISTER_DISPATCH(max_pool1d_stub, &max_pool1d_impl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp b/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
index 23148589808..566f1359160 100644
--- a/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
@@ -9,7 +9,7 @@
 
 #include <c10/util/Optional.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -269,4 +269,4 @@ void max_unpool3d_kernel_impl(
 REGISTER_DISPATCH(max_unpool2d_kernel, &max_unpool2d_kernel_impl);
 REGISTER_DISPATCH(max_unpool3d_kernel, &max_unpool3d_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/MultinomialKernel.cpp b/aten/src/ATen/native/cpu/MultinomialKernel.cpp
index e92b0c73d2b..bdab8043ce7 100644
--- a/aten/src/ATen/native/cpu/MultinomialKernel.cpp
+++ b/aten/src/ATen/native/cpu/MultinomialKernel.cpp
@@ -15,7 +15,7 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template <typename scalar_t>
@@ -240,4 +240,4 @@ static void multinomial_with_replacement_kernel_impl(
 REGISTER_DISPATCH(
     multinomial_with_replacement_stub,
     &multinomial_with_replacement_kernel_impl);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp b/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp
index 263596faa69..0045edd2fea 100644
--- a/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp
+++ b/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp
@@ -8,7 +8,7 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -250,4 +250,4 @@ void pixel_unshuffle_kernel_impl(
 REGISTER_DISPATCH(pixel_shuffle_kernel, &pixel_shuffle_kernel_impl);
 REGISTER_DISPATCH(pixel_unshuffle_kernel, &pixel_unshuffle_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp b/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp
index 4d50fff6f7a..52d6983b971 100644
--- a/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp
@@ -6,7 +6,7 @@
 #include <ATen/native/cpu/Loops.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 static void addcmul_cpu_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -241,4 +241,4 @@ REGISTER_DISPATCH(smooth_l1_backward_stub, &smooth_l1_backward_cpu_kernel);
 REGISTER_DISPATCH(huber_backward_stub, &huber_backward_cpu_kernel);
 REGISTER_DISPATCH(mse_backward_stub, &mse_backward_cpu_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/PowKernel.cpp b/aten/src/ATen/native/cpu/PowKernel.cpp
index 74dc944a0f0..bade9772f69 100644
--- a/aten/src/ATen/native/cpu/PowKernel.cpp
+++ b/aten/src/ATen/native/cpu/PowKernel.cpp
@@ -9,7 +9,7 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 inline namespace CPU_CAPABILITY {
 
@@ -151,4 +151,4 @@ void pow_tensor_scalar_kernel(
 REGISTER_DISPATCH(pow_tensor_tensor_stub, &CPU_CAPABILITY::pow_tensor_tensor_kernel);
 REGISTER_DISPATCH(pow_tensor_scalar_stub, &CPU_CAPABILITY::pow_tensor_scalar_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp b/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp
index 28adc7040cf..78b16efb93b 100644
--- a/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp
+++ b/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp
@@ -13,7 +13,7 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using namespace vec;
@@ -74,4 +74,4 @@ static void linspace_kernel(TensorIterator& iter, const Scalar& scalar_start, co
 REGISTER_DISPATCH(arange_stub, &arange_kernel);
 REGISTER_DISPATCH(linspace_stub, &linspace_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp b/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp
index b3ae7b7b97a..916ad0a0a1c 100644
--- a/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp
@@ -14,7 +14,7 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using namespace vec;
@@ -228,4 +228,4 @@ REGISTER_DISPATCH(min_all_stub, &min_all_kernel_impl);
 REGISTER_DISPATCH(max_all_stub, &max_all_kernel_impl);
 REGISTER_DISPATCH(aminmax_allreduce_stub, &aminmax_allreduce_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp b/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp
index 7ce3c1506a1..deda4108234 100644
--- a/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp
@@ -23,7 +23,7 @@
 #include <c10/util/irange.h>
 #include <ATen/AccumulateType.h>
 
-namespace at::native { namespace {
+namespace at { namespace native { namespace {
 
 using namespace vec;
 
@@ -536,4 +536,4 @@ REGISTER_DISPATCH(cumprod_stub, &cumprod_cpu_kernel);
 REGISTER_DISPATCH(cumsum_stub, &cumsum_cpu_kernel);
 REGISTER_DISPATCH(logcumsumexp_stub, &logcumsumexp_cpu_kernel);
 
-}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/ReduceUtils.h b/aten/src/ATen/native/cpu/ReduceUtils.h
index 68b19d5b5b9..583dcddfbf7 100644
--- a/aten/src/ATen/native/cpu/ReduceUtils.h
+++ b/aten/src/ATen/native/cpu/ReduceUtils.h
@@ -7,7 +7,7 @@
 #include <ATen/native/ReductionType.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 inline namespace CPU_CAPABILITY {
 
 using namespace vec;
@@ -157,4 +157,4 @@ inline void write(scalar_t* out, int64_t count, int64_t K) {
 }
 
 } // namespace CPU_CAPABILITY
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/RenormKernel.cpp b/aten/src/ATen/native/cpu/RenormKernel.cpp
index f684d59328e..57138c7a94c 100644
--- a/aten/src/ATen/native/cpu/RenormKernel.cpp
+++ b/aten/src/ATen/native/cpu/RenormKernel.cpp
@@ -7,7 +7,7 @@
 
 #include <ATen/Dispatch.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
@@ -35,4 +35,4 @@ void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
 
 REGISTER_DISPATCH(renorm_scale_factor_stub, &renorm_scale_factor_impl);
 
-}  // namespace at::native
+} } // namespace at::native
diff --git a/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp b/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp
index 731f91c349e..804c3df0b35 100644
--- a/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp
+++ b/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp
@@ -9,7 +9,7 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -96,4 +96,4 @@ void sampled_addmm_sparse_csr_kernel(
 
 REGISTER_DISPATCH(sampled_addmm_sparse_csr_stub, &sampled_addmm_sparse_csr_kernel);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp
index 849ed43bfb5..7675fd41166 100644
--- a/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp
+++ b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp
@@ -13,7 +13,7 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -857,4 +857,4 @@ REGISTER_DISPATCH(scatter_add_expanded_index_stub, &scatter_add_expanded_index_k
 REGISTER_DISPATCH(scatter_reduce_expanded_index_stub, &scatter_reduce_expanded_index_kernel);
 REGISTER_DISPATCH(gather_expanded_index_stub, &gather_expanded_index_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/SoftMaxKernel.cpp b/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
index 337ddb546ff..a20d93b6726 100644
--- a/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
+++ b/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
@@ -26,7 +26,7 @@
 //
 // We use a chunk size such that it'd fit in L1D.
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 template <typename scalar_t>
@@ -1294,4 +1294,4 @@ REGISTER_DISPATCH(softmax_backward_kernel, &softmax_backward_kernel_impl);
 REGISTER_DISPATCH(
     log_softmax_backward_kernel,
     &log_softmax_backward_kernel_impl);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/SortingKernel.cpp b/aten/src/ATen/native/cpu/SortingKernel.cpp
index 2d98c31d331..8d32353edbb 100644
--- a/aten/src/ATen/native/cpu/SortingKernel.cpp
+++ b/aten/src/ATen/native/cpu/SortingKernel.cpp
@@ -11,7 +11,7 @@
 #include <c10/core/WrapDimMinimal.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -182,4 +182,4 @@ static void topk_kernel(
 REGISTER_DISPATCH(sort_stub, &sort_kernel);
 REGISTER_DISPATCH(topk_stub, &topk_kernel);
 
-} //at::native
+}} //at::native
diff --git a/aten/src/ATen/native/cpu/SparseFactories.cpp b/aten/src/ATen/native/cpu/SparseFactories.cpp
index cbe1abe7716..f177e10ab42 100644
--- a/aten/src/ATen/native/cpu/SparseFactories.cpp
+++ b/aten/src/ATen/native/cpu/SparseFactories.cpp
@@ -8,7 +8,7 @@
 #include <c10/core/ScalarType.h>
 #include <c10/util/Exception.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 void _spdiags_kernel_cpu(
@@ -62,4 +62,4 @@ void _spdiags_kernel_cpu(
 
 REGISTER_DISPATCH(spdiags_kernel_stub, &_spdiags_kernel_cpu)
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/SpmmReduceKernel.h b/aten/src/ATen/native/cpu/SpmmReduceKernel.h
index cbcbf3c63d9..ae4cf069abf 100644
--- a/aten/src/ATen/native/cpu/SpmmReduceKernel.h
+++ b/aten/src/ATen/native/cpu/SpmmReduceKernel.h
@@ -4,7 +4,7 @@
 #include <ATen/native/DispatchStub.h>
 #include <ATen/native/ReductionType.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 using spmm_reduce_fn = void(*)(const Tensor&, const Tensor&, const Tensor&, const Tensor&, const Tensor&, ReductionType op);
 using spmm_reduce_arg_fn = void(*)(const Tensor&, const Tensor&, const Tensor&, const Tensor&, const Tensor&, const Tensor&, ReductionType op);
@@ -19,4 +19,4 @@ DECLARE_DISPATCH(spmm_reduce_backward_input_arg_fn, spmm_reduce_backward_input_a
 DECLARE_DISPATCH(spmm_reduce_backward_other_fn, spmm_reduce_backward_other_stub);
 DECLARE_DISPATCH(spmm_reduce_backward_input_arg_fn, spmm_reduce_backward_other_arg_stub);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/StackKernel.cpp b/aten/src/ATen/native/cpu/StackKernel.cpp
index 999b0d07b9e..e93dd5bde8a 100644
--- a/aten/src/ATen/native/cpu/StackKernel.cpp
+++ b/aten/src/ATen/native/cpu/StackKernel.cpp
@@ -6,7 +6,7 @@
 #include <ATen/native/cpu/StackKernel.h>
 #include <ATen/native/cpu/SerialStackImpl.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -21,4 +21,4 @@ void stack_serial_kernel(Tensor& result, TensorList tensors, int64_t dim) {
 
 REGISTER_DISPATCH(stack_serial_stub, &stack_serial_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/SumKernel.cpp b/aten/src/ATen/native/cpu/SumKernel.cpp
index b5bad819bda..d6bd5c4ee64 100644
--- a/aten/src/ATen/native/cpu/SumKernel.cpp
+++ b/aten/src/ATen/native/cpu/SumKernel.cpp
@@ -9,7 +9,7 @@
 
 #include <algorithm>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 // Load vector from a smaller type (more elements) to a larger type (fewer elements),
@@ -639,4 +639,4 @@ REGISTER_DISPATCH(nansum_stub, &nansum_kernel_impl);
 REGISTER_NO_AVX512_DISPATCH(nansum_stub);
 #endif
 
-}  // namespace at::native
+} } // namespace at::native
diff --git a/aten/src/ATen/native/cpu/TensorCompareKernel.cpp b/aten/src/ATen/native/cpu/TensorCompareKernel.cpp
index 7449017cfe6..79e7840a835 100644
--- a/aten/src/ATen/native/cpu/TensorCompareKernel.cpp
+++ b/aten/src/ATen/native/cpu/TensorCompareKernel.cpp
@@ -26,7 +26,7 @@
 #include <ATen/ops/result_type.h>
 #endif
 
-namespace at::native { namespace {
+namespace at { namespace native { namespace {
 
 template <typename scalar_t, typename scalar_t_2 = int64_t, typename loop1d_t>
 static inline void compare_base_kernel_core(
@@ -404,4 +404,4 @@ REGISTER_DISPATCH(clamp_min_scalar_stub, &clamp_min_scalar_kernel_impl);
 REGISTER_DISPATCH(clamp_max_scalar_stub, &clamp_max_scalar_kernel_impl);
 REGISTER_DISPATCH(isin_default_stub, &isin_default_kernel_cpu);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp b/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp
index 292c2e6b7ed..d87bf8fea1f 100644
--- a/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp
@@ -28,7 +28,7 @@
 #include <mkl.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 inline namespace CPU_CAPABILITY {
 
@@ -802,4 +802,4 @@ IMPLEMENT_FLOAT_KERNEL(trunc)
 // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables,modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)
 IMPLEMENT_FLOAT_KERNEL(lgamma)
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/Unfold2d.cpp b/aten/src/ATen/native/cpu/Unfold2d.cpp
index a117a66c5a0..f2a9a264f75 100644
--- a/aten/src/ATen/native/cpu/Unfold2d.cpp
+++ b/aten/src/ATen/native/cpu/Unfold2d.cpp
@@ -8,7 +8,7 @@
 #include <ATen/native/cpu/utils.h>
 #include <cmath>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -448,4 +448,4 @@ void unfolded2d_copy_kernel(
 REGISTER_DISPATCH(unfolded2d_copy_stub, &unfolded2d_copy_kernel);
 REGISTER_DISPATCH(unfolded2d_acc_stub, &unfolded2d_acc_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp b/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp
index 35049ce21d2..07fef9a4f57 100644
--- a/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp
+++ b/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp
@@ -53,7 +53,7 @@
 // and then the corresponding value of grad_in[...,i_in_dim,...,i_in_last_dim]
 // gets added up to grad_out[...,i_out_dim,...].
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -149,4 +149,4 @@ void unfold_backward_cpu_kernel(
 
 REGISTER_DISPATCH(unfold_backward_stub, &unfold_backward_cpu_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/UpSampleKernel.cpp b/aten/src/ATen/native/cpu/UpSampleKernel.cpp
index 1f471d495df..b1e6fb8f425 100644
--- a/aten/src/ATen/native/cpu/UpSampleKernel.cpp
+++ b/aten/src/ATen/native/cpu/UpSampleKernel.cpp
@@ -18,7 +18,7 @@
 #include <ATen/ops/ones.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using scale_t = std::vector<c10::optional<double>>;
@@ -1981,4 +1981,4 @@ REGISTER_DISPATCH(upsample_trilinear3d_kernel, &upsample_trilinear3d_kernel_impl
 REGISTER_DISPATCH(upsample_bicubic2d_kernel, &upsample_bicubic2d_kernel_impl);
 REGISTER_DISPATCH(_upsample_bicubic2d_aa_kernel, &upsample_bicubic2d_aa_kernel_impl);
 REGISTER_DISPATCH(_upsample_bicubic2d_aa_backward_kernel, &upsample_bicubic2d_aa_backward_kernel_impl);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp b/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp
index 8a2be2738ec..18509db568b 100644
--- a/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp
+++ b/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp
@@ -9,7 +9,7 @@
 #include <c10/util/irange.h>
 #include <ATen/cpu/vec/vec.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using scale_t = std::vector<c10::optional<double>>;
@@ -598,4 +598,4 @@ REGISTER_DISPATCH(upsample_linear1d_backward_kernel, &upsample_linear1d_backward
 REGISTER_DISPATCH(upsample_bilinear2d_backward_kernel, &upsample_bilinear2d_backward_kernel_impl);
 REGISTER_DISPATCH(upsample_trilinear3d_backward_kernel, &upsample_trilinear3d_backward_kernel_impl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/WeightNormKernel.cpp b/aten/src/ATen/native/cpu/WeightNormKernel.cpp
index cace911114e..831ce18332f 100644
--- a/aten/src/ATen/native/cpu/WeightNormKernel.cpp
+++ b/aten/src/ATen/native/cpu/WeightNormKernel.cpp
@@ -10,7 +10,7 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -446,4 +446,4 @@ void weight_norm_backward_kernel(
 REGISTER_DISPATCH(weight_norm_stub, &weight_norm_kernel);
 REGISTER_DISPATCH(weight_norm_backward_stub, &weight_norm_backward_kernel);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cpu/airy_ai.cpp b/aten/src/ATen/native/cpu/airy_ai.cpp
index ee75717b8df..2c2f8fe0772 100644
--- a/aten/src/ATen/native/cpu/airy_ai.cpp
+++ b/aten/src/ATen/native/cpu/airy_ai.cpp
@@ -7,7 +7,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at { namespace native {
 inline namespace CPU_CAPABILITY {
 static void airy_ai_kernel(TensorIteratorBase& iterator) {
     TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +21,4 @@ static void airy_ai_kernel(TensorIteratorBase& iterator) {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_airy_ai_stub, &CPU_CAPABILITY::airy_ai_kernel);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/batch_norm_kernel.cpp b/aten/src/ATen/native/cpu/batch_norm_kernel.cpp
index 156e98969af..7c8b22210e2 100644
--- a/aten/src/ATen/native/cpu/batch_norm_kernel.cpp
+++ b/aten/src/ATen/native/cpu/batch_norm_kernel.cpp
@@ -21,7 +21,7 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using namespace vec;
@@ -1297,4 +1297,4 @@ REGISTER_DISPATCH(batch_norm_cpu_stub, &batch_norm_cpu_kernel);
 REGISTER_DISPATCH(batch_norm_cpu_collect_stats_stub, &batch_norm_cpu_collect_stats_kernel);
 REGISTER_DISPATCH(batch_norm_cpu_backward_stub, &batch_norm_cpu_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/group_norm_kernel.cpp b/aten/src/ATen/native/cpu/group_norm_kernel.cpp
index 5f02ef17d8b..f50e746f2db 100644
--- a/aten/src/ATen/native/cpu/group_norm_kernel.cpp
+++ b/aten/src/ATen/native/cpu/group_norm_kernel.cpp
@@ -21,7 +21,7 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -1409,4 +1409,4 @@ void GroupNormBackwardKernelImpl(
 REGISTER_DISPATCH(GroupNormKernel, &GroupNormKernelImpl);
 REGISTER_DISPATCH(GroupNormBackwardKernel, &GroupNormBackwardKernelImpl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/layer_norm_kernel.cpp b/aten/src/ATen/native/cpu/layer_norm_kernel.cpp
index 3171f3ff04f..2d9a51e0760 100644
--- a/aten/src/ATen/native/cpu/layer_norm_kernel.cpp
+++ b/aten/src/ATen/native/cpu/layer_norm_kernel.cpp
@@ -19,7 +19,7 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -620,4 +620,4 @@ void LayerNormBackwardKernelImpl(
 REGISTER_DISPATCH(LayerNormKernel, &LayerNormKernelImpl);
 REGISTER_DISPATCH(LayerNormBackwardKernel, &LayerNormBackwardKernelImpl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/radix_sort.h b/aten/src/ATen/native/cpu/radix_sort.h
index 94faff28ad1..079f93a6ee4 100644
--- a/aten/src/ATen/native/cpu/radix_sort.h
+++ b/aten/src/ATen/native/cpu/radix_sort.h
@@ -3,7 +3,7 @@
 
 #if !AT_PARALLEL_OPENMP
 
-namespace at::native {
+namespace at { namespace native {
 
 constexpr bool is_radix_sort_available() { return false; }
 
@@ -18,14 +18,14 @@ std::pair<K*, V*> radix_sort_parallel(
   TORCH_CHECK(false, "radix_sort_parallel: ATen is not compiled with OpenMP support");
 }
 
-} // at::native
+}} // at::native
 
 #else
 
 #include <omp.h>
 #include <c10/util/llvmMathExtras.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -191,6 +191,6 @@ std::pair<K*, V*> radix_sort_parallel(
                           : std::make_pair(tmp_key_buf, tmp_value_buf));
 }
 
-} // at::native
+}} // at::native
 
 #endif
diff --git a/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp b/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp
index c706b225daf..2f3058d7261 100644
--- a/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp
+++ b/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp
@@ -7,7 +7,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at { namespace native {
 inline namespace CPU_CAPABILITY {
     static void scaled_modified_bessel_k0_kernel(TensorIteratorBase& iterator) {
         TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +21,4 @@ inline namespace CPU_CAPABILITY {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_scaled_modified_bessel_k0_stub, &CPU_CAPABILITY::scaled_modified_bessel_k0_kernel);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp b/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp
index d2d8de71581..14967f30405 100644
--- a/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp
+++ b/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp
@@ -7,7 +7,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at { namespace native {
 inline namespace CPU_CAPABILITY {
     static void scaled_modified_bessel_k1_kernel(TensorIteratorBase& iterator) {
         TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +21,4 @@ inline namespace CPU_CAPABILITY {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_scaled_modified_bessel_k1_stub, &CPU_CAPABILITY::scaled_modified_bessel_k1_kernel);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp b/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp
index 351ab8670be..7504f31bbbb 100644
--- a/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp
+++ b/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp
@@ -7,7 +7,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at { namespace native {
 inline namespace CPU_CAPABILITY {
     static void spherical_bessel_j0_kernel(TensorIteratorBase& iterator) {
         TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +21,4 @@ inline namespace CPU_CAPABILITY {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_spherical_bessel_j0_stub, &CPU_CAPABILITY::spherical_bessel_j0_kernel);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AbsKernel.cu b/aten/src/ATen/native/cuda/AbsKernel.cu
index 980bd663734..74eaceffba2 100644
--- a/aten/src/ATen/native/cuda/AbsKernel.cu
+++ b/aten/src/ATen/native/cuda/AbsKernel.cu
@@ -6,7 +6,7 @@
 #include <ATen/native/DispatchStub.h>
 #include <ATen/native/TensorIterator.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename scalar_t>
 struct AbsFunctor {
@@ -48,4 +48,4 @@ void abs_kernel_cuda(TensorIteratorBase& iter) {
 
   REGISTER_DISPATCH(abs_stub, &abs_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Activation.cpp b/aten/src/ATen/native/cuda/Activation.cpp
index 633a5f386a8..d5e79e9643d 100644
--- a/aten/src/ATen/native/cuda/Activation.cpp
+++ b/aten/src/ATen/native/cuda/Activation.cpp
@@ -20,7 +20,7 @@
 #include <ATen/ops/log_sigmoid_forward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 // -----------------------------------
 // glu backward
@@ -105,4 +105,4 @@ TORCH_IMPL_FUNC(gelu_backward_out_cuda) (
   GeluBackwardCUDAKernelImpl(*this, get_gelutype_enum(approximate));
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationEluKernel.cu b/aten/src/ATen/native/cuda/ActivationEluKernel.cu
index 3f68b521c00..b734625520a 100644
--- a/aten/src/ATen/native/cuda/ActivationEluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationEluKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void elu_kernel(
@@ -83,4 +83,4 @@ void elu_backward_kernel(
 REGISTER_DISPATCH(elu_stub, &elu_kernel);
 REGISTER_DISPATCH(elu_backward_stub, &elu_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationGeluKernel.cu b/aten/src/ATen/native/cuda/ActivationGeluKernel.cu
index 47012f980fc..020ad12a40f 100644
--- a/aten/src/ATen/native/cuda/ActivationGeluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationGeluKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 void GeluCUDAKernelImpl(TensorIteratorBase& it, GeluType approximate) {
   if (approximate == GeluType::Tanh) {
@@ -85,4 +85,4 @@ void GeluBackwardCUDAKernelImpl(TensorIteratorBase& it, GeluType approximate) {
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationGluKernel.cu b/aten/src/ATen/native/cuda/ActivationGluKernel.cu
index 15ac2a50c91..b3946af55f8 100644
--- a/aten/src/ATen/native/cuda/ActivationGluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationGluKernel.cu
@@ -16,8 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
-
+namespace at { namespace native {
 // -----------------------------------
 // glu forward
 // -----------------------------------
@@ -138,4 +137,4 @@ void launch_glu_backward_kernel(
 REGISTER_DISPATCH(glu_stub, &glu_kernel);
 REGISTER_DISPATCH(glu_jvp_stub, &glu_jvp_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu b/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu
index 3e2ca62e274..dcb1b75e406 100644
--- a/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void hardshrink_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -36,4 +36,4 @@ void hardshrink_kernel(TensorIteratorBase& iter, const Scalar& value) {
 
 REGISTER_DISPATCH(hardshrink_stub, &hardshrink_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu b/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu
index f69b5c5daed..98fc833c55c 100644
--- a/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void hardsigmoid_kernel(TensorIteratorBase& iter) {
@@ -71,4 +71,4 @@ void hardsigmoid_backward_kernel(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(hardsigmoid_stub, &hardsigmoid_kernel);
 REGISTER_DISPATCH(hardsigmoid_backward_stub, &hardsigmoid_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu b/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu
index 38011e9ed60..3fdd9b6a0ad 100644
--- a/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void hardswish_kernel(TensorIterator& iter) {
@@ -60,4 +60,4 @@ void hardswish_backward_kernel(TensorIterator& iter) {
 REGISTER_DISPATCH(hardswish_stub, &hardswish_kernel);
 REGISTER_DISPATCH(hardswish_backward_stub, &hardswish_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu b/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu
index 30bb909d58e..17bd0b21d0c 100644
--- a/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void hardtanh_backward_kernel(
@@ -42,4 +42,4 @@ void hardtanh_backward_kernel(
 
 REGISTER_DISPATCH(hardtanh_backward_stub, &hardtanh_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu b/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu
index 6b848df333a..a962f9f9dc0 100644
--- a/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void leaky_relu_kernel(TensorIteratorBase& iter, const Scalar& negval_) {
@@ -59,4 +59,4 @@ void leaky_relu_backward_kernel(
 REGISTER_DISPATCH(leaky_relu_stub, &leaky_relu_kernel);
 REGISTER_DISPATCH(leaky_relu_backward_stub, &leaky_relu_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu b/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu
index eb34d9d4633..61174089a0a 100644
--- a/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 // -----------------------------------
 // log_sigmoid forward
@@ -61,4 +61,4 @@ void log_sigmoid_backward_kernel(TensorIterator& iter) {
 
 REGISTER_DISPATCH(log_sigmoid_backward_stub, &log_sigmoid_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationMishKernel.cu b/aten/src/ATen/native/cuda/ActivationMishKernel.cu
index e259e64fc08..ba08ee4b841 100644
--- a/aten/src/ATen/native/cuda/ActivationMishKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationMishKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void mish_kernel(TensorIteratorBase& iter) {
@@ -61,4 +61,4 @@ void mish_backward_kernel(TensorIterator& iter) {
 REGISTER_DISPATCH(mish_stub, &mish_kernel);
 REGISTER_DISPATCH(mish_backward_stub, &mish_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationPreluKernel.cu b/aten/src/ATen/native/cuda/ActivationPreluKernel.cu
index d6b73317738..46c91925541 100644
--- a/aten/src/ATen/native/cuda/ActivationPreluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationPreluKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 // -----------------------------------
 // prelu
@@ -45,4 +45,4 @@ void prelu_backward_kernel(TensorIterator &iter) {
 REGISTER_DISPATCH(prelu_stub, &prelu_kernel);
 REGISTER_DISPATCH(prelu_backward_stub, &prelu_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationSiluKernel.cu b/aten/src/ATen/native/cuda/ActivationSiluKernel.cu
index e9b495ca70b..6b1e91d815e 100644
--- a/aten/src/ATen/native/cuda/ActivationSiluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationSiluKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void silu_kernel(TensorIteratorBase& iter) {
@@ -56,4 +56,4 @@ void silu_backward_kernel(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(silu_stub, &silu_kernel);
 REGISTER_DISPATCH(silu_backward_stub, &silu_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu b/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu
index 054e42139b0..373eb3ce8a4 100644
--- a/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void softplus_kernel(
@@ -71,4 +71,4 @@ void softplus_backward_kernel(
 REGISTER_DISPATCH(softplus_stub, &softplus_kernel);
 REGISTER_DISPATCH(softplus_backward_stub, &softplus_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu b/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu
index a07d0d69a38..e9d1dbb19b7 100644
--- a/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void softshrink_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -55,4 +55,4 @@ void shrink_backward_kernel(TensorIteratorBase& iter, const Scalar& value) {
 REGISTER_DISPATCH(softshrink_stub, &softshrink_kernel);
 REGISTER_DISPATCH(shrink_backward_stub, &shrink_backward_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu b/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu
index 68baa5133e7..bec92ede530 100644
--- a/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template <typename scalar_t>
@@ -49,4 +49,4 @@ static void threshold_kernel_cuda(
 
 REGISTER_DISPATCH(threshold_stub, &threshold_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu b/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu
index b25f3872523..be4adc62349 100644
--- a/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu
@@ -36,7 +36,7 @@
 #define CUDA_MAX_THREADS 1024 // this is safe, in reality 256 is our limit
 #define BLOCK_STRIDE 2 // increasing block_stride to lower # of blocks launched
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -799,7 +799,7 @@ namespace {
     return gradInput;
   }
 
-} // namespace at::native
+}} // namespace at::native
 
 #undef BLOCK_STRIDE
 #undef CUDA_MAX_THREADS
diff --git a/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu b/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu
index 4bc45f30569..e2b48e85be0 100644
--- a/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu
@@ -25,7 +25,7 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -542,4 +542,4 @@ Tensor adaptive_avg_pool3d_backward_cuda(
   return gradInput;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu b/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu
index bf25a098cb2..53941f45329 100644
--- a/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu
@@ -23,8 +23,7 @@
 #include <cmath>
 
 
-namespace at::native {
-
+namespace at { namespace native {
 namespace {
 
 __device__ inline int64_t start_index(int64_t a, int64_t b, int64_t c) {
@@ -471,4 +470,4 @@ TORCH_IMPL_FUNC(adaptive_max_pool2d_backward_out_cuda)
     gradInput.copy_(gradInput_c);
   }
  }
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu b/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu
index 0b57d9e5101..46d6da070fa 100644
--- a/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu
@@ -23,7 +23,7 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -481,4 +481,4 @@ TORCH_IMPL_FUNC(adaptive_max_pool3d_backward_out_cuda)
         });
   }
  }
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AmpKernels.cu b/aten/src/ATen/native/cuda/AmpKernels.cu
index ab2bf66d517..111b0b0b091 100644
--- a/aten/src/ATen/native/cuda/AmpKernels.cu
+++ b/aten/src/ATen/native/cuda/AmpKernels.cu
@@ -35,7 +35,7 @@ static __host__ __device__ __forceinline__ int isfinite_ensure_cuda_math(float v
 }
 }
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 // Single-tensor fallback for _amp_foreach_non_finite_check_and_unscale_cuda_.
@@ -245,4 +245,4 @@ Tensor& _amp_update_scale_cuda_(Tensor& current_scale,
   return current_scale;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AveragePool2d.cu b/aten/src/ATen/native/cuda/AveragePool2d.cu
index 9a5399b9e00..919aff08331 100644
--- a/aten/src/ATen/native/cuda/AveragePool2d.cu
+++ b/aten/src/ATen/native/cuda/AveragePool2d.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/avg_pool2d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -455,4 +455,4 @@ TORCH_IMPL_FUNC(avg_pool2d_backward_out_cuda) (
   );
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/AveragePool3d.cu b/aten/src/ATen/native/cuda/AveragePool3d.cu
index a722236ea57..811584ef2c2 100644
--- a/aten/src/ATen/native/cuda/AveragePool3d.cu
+++ b/aten/src/ATen/native/cuda/AveragePool3d.cu
@@ -21,7 +21,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -603,4 +603,4 @@ TORCH_IMPL_FUNC(avg_pool3d_backward_out_cuda) (
   }
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu
index f0a498b0647..bf2cee778eb 100644
--- a/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu
@@ -8,7 +8,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename scalar_t>
 struct BitwiseAndFunctor {
@@ -78,4 +78,4 @@ REGISTER_DISPATCH(bitwise_or_stub, &bitwise_or_kernel_cuda);
 REGISTER_DISPATCH(bitwise_xor_stub, &bitwise_xor_kernel_cuda);
 
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu b/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu
index bd4d243f618..190ecf67aa5 100644
--- a/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu
@@ -14,7 +14,7 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at { namespace native {
 namespace binary_internal {
 
 void div_floor_kernel_cuda(TensorIteratorBase& iter) {
@@ -107,4 +107,4 @@ void div_floor_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(div_floor_stub, &binary_internal::div_floor_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu b/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu
index aa955a9c7e5..86ddacf115e 100644
--- a/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu
@@ -13,7 +13,7 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at { namespace native {
 namespace binary_internal {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char div_name[] = "div_kernel";
@@ -58,4 +58,4 @@ void div_true_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(div_true_stub, &binary_internal::div_true_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu b/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu
index 5e906a000b0..8fa3132c41d 100644
--- a/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu
@@ -12,7 +12,7 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at { namespace native {
 namespace binary_internal {
 
 void div_trunc_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +50,4 @@ void div_trunc_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(div_trunc_stub, &binary_internal::div_trunc_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu b/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu
index e734a66e931..a46e3e9e1b8 100644
--- a/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu
@@ -8,7 +8,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void atan2_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(
@@ -36,4 +36,4 @@ void hypot_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(atan2_stub, &atan2_kernel_cuda);
 REGISTER_DISPATCH(hypot_stub, &hypot_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu
index eaa01ac1acc..7c7cd6e869f 100644
--- a/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu
@@ -9,7 +9,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char logical_and_name[] = "logical_and_kernel";
 void logical_and_kernel_cuda(TensorIterator& iter) {
@@ -125,4 +125,4 @@ REGISTER_DISPATCH(logical_or_stub, &logical_or_kernel_cuda);
 REGISTER_DISPATCH(logical_xor_stub, &logical_xor_kernel_cuda);
 
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu
index 75d5991f93d..4c6d045fa86 100644
--- a/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu
@@ -13,7 +13,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char sigmoid_backward_name[] = "sigmoid_backward";
 void sigmoid_backward_kernel_cuda(TensorIteratorBase& iter) {
@@ -128,4 +128,4 @@ REGISTER_DISPATCH(sigmoid_backward_stub, &sigmoid_backward_kernel_cuda);
 REGISTER_DISPATCH(logit_backward_stub, &logit_backward_kernel_cuda);
 REGISTER_DISPATCH(tanh_backward_stub, &tanh_backward_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu
index 85be724c7a3..703436a1d49 100644
--- a/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu
@@ -10,7 +10,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void smooth_l1_kernel_cuda(TensorIteratorBase& iter, double beta) {
   AT_DISPATCH_FLOATING_TYPES_AND_HALF(iter.dtype(), "smooth_l1_cuda", [&iter, beta]() {
@@ -78,4 +78,4 @@ REGISTER_DISPATCH(xlog1py_stub, &xlog1py_kernel_cuda);
 // DO NOT ADD ANY NEW KERNELS HERE
 // CUDA compilation times grow quickly.  It's perfectly acceptable to have a file per kernel.
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryMulKernel.cu b/aten/src/ATen/native/cuda/BinaryMulKernel.cu
index 251221f7adc..40d1df9d3fd 100644
--- a/aten/src/ATen/native/cuda/BinaryMulKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryMulKernel.cu
@@ -16,7 +16,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char mul_name[] = "mul_kernel";
 void mul_kernel_cuda(TensorIteratorBase& iter) {
@@ -45,4 +45,4 @@ void mul_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(mul_stub, &mul_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu b/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu
index dfa2f7124b5..be16947c28e 100644
--- a/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu
@@ -11,7 +11,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void remainder_kernel_cuda(TensorIteratorBase& iter) {
   if (isIntegralType(iter.common_dtype(), /*includeBool*/ false)) {
@@ -58,4 +58,4 @@ void fmod_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(remainder_stub, &remainder_kernel_cuda);
 REGISTER_DISPATCH(fmod_stub, &fmod_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu
index c2578f8bbc2..d6bd145c4f5 100644
--- a/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu
@@ -8,7 +8,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 
 void lshift_kernel_cuda(TensorIteratorBase& iter) {
@@ -32,4 +32,4 @@ void rshift_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(lshift_stub, &lshift_kernel_cuda);
 REGISTER_DISPATCH(rshift_stub, &rshift_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Blas.cpp b/aten/src/ATen/native/cuda/Blas.cpp
index ce78f517a0b..01471f568a7 100644
--- a/aten/src/ATen/native/cuda/Blas.cpp
+++ b/aten/src/ATen/native/cuda/Blas.cpp
@@ -30,7 +30,7 @@
 #include <ATen/ops/vdot_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -672,4 +672,4 @@ TORCH_IMPL_FUNC(addmv_out_cuda)(const Tensor &self, const Tensor &mat, const Ten
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Bucketization.cu b/aten/src/ATen/native/cuda/Bucketization.cu
index 7a139ffac7e..5c960249f5c 100644
--- a/aten/src/ATen/native/cuda/Bucketization.cu
+++ b/aten/src/ATen/native/cuda/Bucketization.cu
@@ -15,7 +15,7 @@
 #include <ATen/ops/searchsorted_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 // Implement a numpy like searchsorted and a TF like bucketize function running on cuda
 // See details in ATen/nativate/Bucketization.cpp
@@ -218,4 +218,4 @@ Tensor bucketize_cuda(const Scalar& self, const Tensor& boundaries, bool out_int
   return bucketize_cuda(searchsorted_scalar_tensor(self, boundaries.device()), boundaries, out_int32, right);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CUDAScalar.cu b/aten/src/ATen/native/cuda/CUDAScalar.cu
index f298058106c..b71410ca5fe 100644
--- a/aten/src/ATen/native/cuda/CUDAScalar.cu
+++ b/aten/src/ATen/native/cuda/CUDAScalar.cu
@@ -10,7 +10,7 @@
 
 #include <ATen/cuda/CUDAContext.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 Scalar _local_scalar_dense_cuda(const Tensor& self) {
   Scalar r;
@@ -24,4 +24,4 @@ Scalar _local_scalar_dense_cuda(const Tensor& self) {
   return r;
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/Col2Im.cu b/aten/src/ATen/native/cuda/Col2Im.cu
index 4445e838990..90e3ebbea9a 100644
--- a/aten/src/ATen/native/cuda/Col2Im.cu
+++ b/aten/src/ATen/native/cuda/Col2Im.cu
@@ -20,7 +20,7 @@
 #include <ATen/ops/im2col_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void col2im_out_cuda_template(
@@ -168,4 +168,4 @@ Tensor col2im_cuda(
   return output;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CompareEQKernel.cu b/aten/src/ATen/native/cuda/CompareEQKernel.cu
index 088312fb6f1..f20d4abb38e 100644
--- a/aten/src/ATen/native/cuda/CompareEQKernel.cu
+++ b/aten/src/ATen/native/cuda/CompareEQKernel.cu
@@ -9,7 +9,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native { namespace {
+namespace at { namespace native { namespace {
 
 enum class EqOpType {EQ, NE};
 
@@ -47,4 +47,4 @@ void ne_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(eq_stub, &eq_kernel_cuda);
 REGISTER_DISPATCH(ne_stub, &ne_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CompareKernels.cu b/aten/src/ATen/native/cuda/CompareKernels.cu
index 8a1a97759f1..3229bbbcebc 100644
--- a/aten/src/ATen/native/cuda/CompareKernels.cu
+++ b/aten/src/ATen/native/cuda/CompareKernels.cu
@@ -9,7 +9,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native { namespace {
+namespace at { namespace native { namespace {
 
 enum class OpType {GE, GT, LE, LT};
 
@@ -100,4 +100,4 @@ REGISTER_DISPATCH(gt_stub, &gt_kernel_cuda);
 REGISTER_DISPATCH(le_stub, &le_kernel_cuda);
 REGISTER_DISPATCH(lt_stub, &lt_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ComplexKernel.cu b/aten/src/ATen/native/cuda/ComplexKernel.cu
index 2bf26722fbc..99125f311b7 100644
--- a/aten/src/ATen/native/cuda/ComplexKernel.cu
+++ b/aten/src/ATen/native/cuda/ComplexKernel.cu
@@ -7,7 +7,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void complex_kernel_cuda(TensorIterator& iter) {
@@ -33,4 +33,4 @@ void polar_kernel_cuda(TensorIterator& iter) {
 REGISTER_DISPATCH(complex_stub, &complex_kernel_cuda);
 REGISTER_DISPATCH(polar_stub, &polar_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ConvolutionMM2d.cu b/aten/src/ATen/native/cuda/ConvolutionMM2d.cu
index bca9d38e2ce..f77ff6bc546 100644
--- a/aten/src/ATen/native/cuda/ConvolutionMM2d.cu
+++ b/aten/src/ATen/native/cuda/ConvolutionMM2d.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/sum.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void slow_conv2d_shape_check(
@@ -499,4 +499,4 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv2d_backward_cuda(
       grad_bias);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Copy.cu b/aten/src/ATen/native/cuda/Copy.cu
index 07ead054603..772a33aae35 100644
--- a/aten/src/ATen/native/cuda/Copy.cu
+++ b/aten/src/ATen/native/cuda/Copy.cu
@@ -19,7 +19,7 @@
 #include <c10/cuda/CUDACachingAllocator.h>
 #include <c10/cuda/CUDAStream.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void neg_kernel_cuda(TensorIteratorBase &iter);
 void conj_kernel_cuda(TensorIteratorBase &iter);
@@ -282,4 +282,4 @@ static void copy_kernel_cuda(TensorIterator& iter, bool non_blocking) {
 
 REGISTER_DISPATCH(copy_stub, &copy_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CopysignKernel.cu b/aten/src/ATen/native/cuda/CopysignKernel.cu
index 38724d7e299..2f7b0cf3c4d 100644
--- a/aten/src/ATen/native/cuda/CopysignKernel.cu
+++ b/aten/src/ATen/native/cuda/CopysignKernel.cu
@@ -18,7 +18,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void copysign_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "copysign_cuda", [&]() {
@@ -30,4 +30,4 @@ void copysign_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(copysign_stub, &copysign_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CrossKernel.cu b/aten/src/ATen/native/cuda/CrossKernel.cu
index 01da1ffc3ba..8ded8d037bf 100644
--- a/aten/src/ATen/native/cuda/CrossKernel.cu
+++ b/aten/src/ATen/native/cuda/CrossKernel.cu
@@ -5,7 +5,7 @@
 #include <ATen/Dispatch.h>
 #include <ATen/core/Tensor.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename T, typename OffsetCalc, typename StrideType>
 __global__ void cross_kernel(
@@ -89,4 +89,4 @@ void cross_impl(const Tensor& result, const Tensor& x1, const Tensor& x2, int64_
 
 REGISTER_DISPATCH(cross_stub, &cross_impl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CumminmaxKernel.cu b/aten/src/ATen/native/cuda/CumminmaxKernel.cu
index d72f13f559c..ea73273e2d4 100644
--- a/aten/src/ATen/native/cuda/CumminmaxKernel.cu
+++ b/aten/src/ATen/native/cuda/CumminmaxKernel.cu
@@ -8,7 +8,7 @@
 #include <limits>
 #include <functional>
 
-namespace at::native {
+namespace at { namespace native {
 
 void launch_cummax_cuda_kernel(const TensorBase& self, const TensorBase& values, const TensorBase& indices, int64_t dim) {
   AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
@@ -26,4 +26,4 @@ void launch_cummin_cuda_kernel(const TensorBase& self, const TensorBase& values,
   });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CumprodKernel.cu b/aten/src/ATen/native/cuda/CumprodKernel.cu
index f44ec072b16..d1f3233abb1 100644
--- a/aten/src/ATen/native/cuda/CumprodKernel.cu
+++ b/aten/src/ATen/native/cuda/CumprodKernel.cu
@@ -5,7 +5,7 @@
 #include <ATen/native/cuda/ScanKernels.h>
 #include <ATen/native/cuda/ScanUtils.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 void launch_cumprod_cuda_kernel(const TensorBase& result, const TensorBase& self, int64_t dim) {
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
@@ -20,4 +20,4 @@ void launch_cumprod_cuda_kernel(const TensorBase& result, const TensorBase& self
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CumsumKernel.cu b/aten/src/ATen/native/cuda/CumsumKernel.cu
index 07db9cd2361..85866b3f0f3 100644
--- a/aten/src/ATen/native/cuda/CumsumKernel.cu
+++ b/aten/src/ATen/native/cuda/CumsumKernel.cu
@@ -5,7 +5,7 @@
 #include <ATen/native/cuda/ScanKernels.h>
 #include <ATen/native/cuda/ScanUtils.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 void launch_cumsum_cuda_kernel(const TensorBase& result, const TensorBase& self, int64_t dim) {
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
@@ -22,4 +22,4 @@ void launch_cumsum_cuda_kernel(const TensorBase& result, const TensorBase& self,
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DepthwiseConv2d.cu b/aten/src/ATen/native/cuda/DepthwiseConv2d.cu
index 7e2a0796ccb..8513862610d 100644
--- a/aten/src/ATen/native/cuda/DepthwiseConv2d.cu
+++ b/aten/src/ATen/native/cuda/DepthwiseConv2d.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/_conv_depthwise2d_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 using at::cuda::detail::CUDA_NUM_THREADS;
 using at::cuda::detail::GET_BLOCKS;
@@ -632,4 +632,4 @@ std::tuple<Tensor, Tensor> conv_depthwise2d_backward_cuda(
 
 REGISTER_CUDA_DISPATCH(conv_depthwise2d_backward_stub, &conv_depthwise2d_backward_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DepthwiseConv3d.cu b/aten/src/ATen/native/cuda/DepthwiseConv3d.cu
index f11bfc817ab..f0c540fc259 100644
--- a/aten/src/ATen/native/cuda/DepthwiseConv3d.cu
+++ b/aten/src/ATen/native/cuda/DepthwiseConv3d.cu
@@ -19,7 +19,7 @@
 #include <tuple>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t,
@@ -697,4 +697,4 @@ REGISTER_CUDA_DISPATCH(conv_depthwise3d_backward_stub, &conv_depthwise3d_backwar
 #undef NODEF_OR_EQUAL_3
 #undef NODEF_OR_EQUAL
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu b/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu
index 8b387585863..8ec3f1f0d04 100644
--- a/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu
+++ b/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu
@@ -21,7 +21,7 @@
 #include <ATen/ops/max_pool2d_with_indices_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -564,4 +564,4 @@ const Tensor& gradInput) {
   );
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu b/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu
index 6e2f79466e5..23f86bd5649 100644
--- a/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu
+++ b/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu
@@ -24,7 +24,7 @@
 #include <ATen/ops/zeros_like.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -650,4 +650,4 @@ Tensor max_pool3d_with_indices_backward_cuda(
   return gradInput;
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/DistanceKernel.cu b/aten/src/ATen/native/cuda/DistanceKernel.cu
index c1380dd7ff2..2ae4cd592e6 100644
--- a/aten/src/ATen/native/cuda/DistanceKernel.cu
+++ b/aten/src/ATen/native/cuda/DistanceKernel.cu
@@ -19,7 +19,7 @@
 
 #include <c10/macros/Macros.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -362,4 +362,4 @@ REGISTER_DISPATCH(pdist_backward_stub, &pdist_backward_kernel_impl);
 REGISTER_DISPATCH(cdist_stub, &cdist_kernel_impl);
 REGISTER_DISPATCH(cdist_backward_stub, &cdist_backward_kernel_impl);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/DistributionBernoulli.cu b/aten/src/ATen/native/cuda/DistributionBernoulli.cu
index 89a518267d2..a7967122db9 100644
--- a/aten/src/ATen/native/cuda/DistributionBernoulli.cu
+++ b/aten/src/ATen/native/cuda/DistributionBernoulli.cu
@@ -21,7 +21,7 @@
 #include <utility>
 #include <type_traits>
 
-namespace at::native {
+namespace at { namespace native {
 
 void bernoulli_tensor_kernel(const TensorBase &self, const TensorBase &p_, c10::optional<Generator> gen_) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen_, cuda::detail::getDefaultCUDAGenerator());
@@ -37,4 +37,4 @@ void bernoulli_scalar_kernel(const TensorBase &self, double p, c10::optional<Gen
 REGISTER_DISPATCH(bernoulli_tensor_stub, &bernoulli_tensor_kernel);
 REGISTER_DISPATCH(bernoulli_scalar_stub, &bernoulli_scalar_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu b/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu
index a66d3cf3288..27f316bc82b 100644
--- a/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu
@@ -3,7 +3,7 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void cauchy_kernel(TensorIteratorBase& iter, double median, double sigma, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +12,4 @@ void cauchy_kernel(TensorIteratorBase& iter, double median, double sigma, c10::o
 
 REGISTER_DISPATCH(cauchy_stub, &cauchy_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu b/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu
index 76cb94f6fd8..4dac756a2aa 100644
--- a/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu
@@ -3,7 +3,7 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void exponential_kernel(TensorIteratorBase& iter, double lambda, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +12,4 @@ void exponential_kernel(TensorIteratorBase& iter, double lambda, c10::optional<G
 
 REGISTER_DISPATCH(exponential_stub, &exponential_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu b/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu
index 0fe49d7bbd4..4bfe6cb692b 100644
--- a/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu
@@ -3,7 +3,7 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void geometric_kernel(TensorIteratorBase& iter, double p_, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +12,4 @@ void geometric_kernel(TensorIteratorBase& iter, double p_, c10::optional<Generat
 
 REGISTER_DISPATCH(geometric_stub, &geometric_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu b/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu
index f394d4fea39..f7b094ed625 100644
--- a/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu
@@ -3,7 +3,7 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void log_normal_kernel(TensorIteratorBase& iter, double mean, double std, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +12,4 @@ void log_normal_kernel(TensorIteratorBase& iter, double mean, double std, c10::o
 
 REGISTER_DISPATCH(log_normal_stub, &log_normal_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionNormal.cu b/aten/src/ATen/native/cuda/DistributionNormal.cu
index a17c3e3da05..28330dbd69a 100644
--- a/aten/src/ATen/native/cuda/DistributionNormal.cu
+++ b/aten/src/ATen/native/cuda/DistributionNormal.cu
@@ -3,7 +3,7 @@
 #include <ATen/cuda/CUDAGeneratorImpl.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void normal_kernel(const TensorBase &self, double mean, double std, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +12,4 @@ void normal_kernel(const TensorBase &self, double mean, double std, c10::optiona
 
 REGISTER_DISPATCH(normal_stub, &normal_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionRandomKernel.cu b/aten/src/ATen/native/cuda/DistributionRandomKernel.cu
index 034a19c512f..0607e4fa804 100644
--- a/aten/src/ATen/native/cuda/DistributionRandomKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionRandomKernel.cu
@@ -3,7 +3,7 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void random_from_to_kernel(TensorIteratorBase& iter, uint64_t range, int64_t base, c10::optional<Generator> gen_) {
   auto gen = get_generator_or_default<CUDAGeneratorImpl>(gen_, cuda::detail::getDefaultCUDAGenerator());
@@ -24,4 +24,4 @@ REGISTER_DISPATCH(random_from_to_stub, &random_from_to_kernel);
 REGISTER_DISPATCH(random_stub, &random_kernel);
 REGISTER_DISPATCH(random_full_64_bits_range_stub, &random_full_64_bits_range_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/DistributionUniform.cu b/aten/src/ATen/native/cuda/DistributionUniform.cu
index 2ebdfa44645..a848f0fd48f 100644
--- a/aten/src/ATen/native/cuda/DistributionUniform.cu
+++ b/aten/src/ATen/native/cuda/DistributionUniform.cu
@@ -3,7 +3,7 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void uniform_kernel(TensorIteratorBase& iter, double from, double to, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +12,4 @@ void uniform_kernel(TensorIteratorBase& iter, double from, double to, c10::optio
 
 REGISTER_DISPATCH(uniform_stub, &uniform_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Distributions.cpp b/aten/src/ATen/native/cuda/Distributions.cpp
index c0d5abb49bf..fc885d86744 100644
--- a/aten/src/ATen/native/cuda/Distributions.cpp
+++ b/aten/src/ATen/native/cuda/Distributions.cpp
@@ -16,7 +16,7 @@
 #include <ATen/ops/poisson_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor _s_poisson_cuda(const Tensor& lambda, c10::optional<Generator> gen_) {
   auto gen = get_generator_or_default<CUDAGeneratorImpl>(gen_, cuda::detail::getDefaultCUDAGenerator());
@@ -81,4 +81,4 @@ Tensor _dirichlet_grad_cuda(const Tensor& x, const Tensor& alpha, const Tensor&
   return ret;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Distributions.cu b/aten/src/ATen/native/cuda/Distributions.cu
index 1aac9fe8ba2..f45d745eb41 100644
--- a/aten/src/ATen/native/cuda/Distributions.cu
+++ b/aten/src/ATen/native/cuda/Distributions.cu
@@ -128,7 +128,7 @@ void gamma_cuda_kernel(
 
 } // namespace
 
-namespace at::native {
+namespace at { namespace native {
 
 void launch_dirichlet_kernel(at::TensorIteratorBase &iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
@@ -205,4 +205,4 @@ void launch_dirichlet_grad_kernel(TensorIteratorBase &iter) {
   });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Dropout.cu b/aten/src/ATen/native/cuda/Dropout.cu
index 67ea3e4f832..3a667113bdd 100644
--- a/aten/src/ATen/native/cuda/Dropout.cu
+++ b/aten/src/ATen/native/cuda/Dropout.cu
@@ -25,7 +25,7 @@
 #include <ATen/ops/zeros_like.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -413,4 +413,4 @@ Tensor masked_scale_cuda(const Tensor& self, const Tensor& mask, double scale){
   return dropout_backward_cuda<uint8_t>(self, mask, scale);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Embedding.cu b/aten/src/ATen/native/cuda/Embedding.cu
index cd4d8aae544..568669e10fd 100644
--- a/aten/src/ATen/native/cuda/Embedding.cu
+++ b/aten/src/ATen/native/cuda/Embedding.cu
@@ -31,7 +31,7 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -392,4 +392,4 @@ Tensor & embedding_renorm_cuda_(Tensor & self, const Tensor & indices,
 }
 
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu b/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu
index 30ee6abb46a..86f8115218a 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu
@@ -21,7 +21,7 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -361,4 +361,4 @@ Tensor embedding_backward_cuda_kernel(
   return grad_weight;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/EmbeddingBag.cu b/aten/src/ATen/native/cuda/EmbeddingBag.cu
index 5f448463ce4..d9060ec3efd 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBag.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBag.cu
@@ -34,7 +34,7 @@
 #include <thrust/iterator/reverse_iterator.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 #if !CUB_SUPPORTS_SCAN_BY_KEY()
 template<typename index_t>
@@ -560,4 +560,4 @@ Tensor _embedding_bag_per_sample_weights_backward_cuda(
   return output;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Equal.cpp b/aten/src/ATen/native/cuda/Equal.cpp
index 206d562efcd..ab8c9adef4e 100644
--- a/aten/src/ATen/native/cuda/Equal.cpp
+++ b/aten/src/ATen/native/cuda/Equal.cpp
@@ -10,7 +10,7 @@
 #include <ATen/ops/equal_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 bool cuda_equal(const Tensor& self, const Tensor &src) {
   if (!at::namedinference::are_names_equal(
@@ -29,4 +29,4 @@ bool cuda_equal(const Tensor& self, const Tensor &src) {
   return at::cuda::eq(self, src).all().item().to<bool>();
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/FillKernel.cu b/aten/src/ATen/native/cuda/FillKernel.cu
index 5090979d356..f40354757cc 100644
--- a/aten/src/ATen/native/cuda/FillKernel.cu
+++ b/aten/src/ATen/native/cuda/FillKernel.cu
@@ -6,7 +6,7 @@
 #include <ATen/native/Fill.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename scalar_t>
 struct FillFunctor {
@@ -26,4 +26,4 @@ void fill_kernel_cuda(TensorIterator& iter, const Scalar& value) {
 
 REGISTER_DISPATCH(fill_stub, &fill_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu b/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu
index f05d0f25783..1d74e91a904 100644
--- a/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu
+++ b/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu
@@ -17,7 +17,7 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename T, template<class> class Op>
 std::vector<Tensor> foreach_tensor_list_op(TensorList tensors1, TensorList tensors2, const Scalar& alpha = 1) {
@@ -133,4 +133,4 @@ FOREACH_BINARY_OP_LIST(all_types_complex_bool_half_bfloat16, div, std::divides,
 FOREACH_BINARY_OP_LIST(all_types_half_bfloat16, clamp_max, minimum, /*division_op*/ false);
 FOREACH_BINARY_OP_LIST(all_types_half_bfloat16, clamp_min, maximum, /*division_op*/ false);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu b/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu
index b1e7d84008c..219ee5c0dcb 100644
--- a/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu
+++ b/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename T, template<class> class Op>
 std::vector<Tensor> foreach_binary_op(TensorList tensors, const Scalar& scalar) {
@@ -142,4 +142,4 @@ std::vector<Tensor> foreach_tensor_sub_scalar_kernel_cuda(TensorList tensors, co
 FOREACH_BINARY_OP_SCALAR(all_types_half_bfloat16, clamp_max, minimum, false);
 FOREACH_BINARY_OP_SCALAR(all_types_half_bfloat16, clamp_min, maximum, false);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu b/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu
index f0c7cacd044..c855436a91e 100644
--- a/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu
+++ b/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename T, template<class> class Op>
 std::vector<Tensor> foreach_binary_op(TensorList tensors, at::ArrayRef<Scalar> scalars) {
@@ -145,4 +145,4 @@ std::vector<Tensor> foreach_tensor_sub_scalarlist_kernel_cuda(TensorList tensors
 FOREACH_BINARY_OP_SCALARLIST(all_types_half_bfloat16, clamp_max, minimum, false);
 FOREACH_BINARY_OP_SCALARLIST(all_types_half_bfloat16, clamp_min, maximum, false);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu b/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu
index 8a95da39697..c9cbac4b2ec 100644
--- a/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu
@@ -19,7 +19,7 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 template<template<class> class Op>
 std::vector<Tensor> foreach_pointwise_op(TensorList input, TensorList tensors1, TensorList tensors2, const Scalar& scalar) {
@@ -200,4 +200,4 @@ FOREACH_POINTWISE_OP_SCALARLIST(addcdiv, std::divides);
 FOREACH_POINTWISE_OP_TENSOR(addcdiv, std::divides);
 FOREACH_POINTWISE_OP_TENSOR(addcmul, std::multiplies);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachReduceOp.cu b/aten/src/ATen/native/cuda/ForeachReduceOp.cu
index 27d58e28cf1..7bb9fd3883d 100644
--- a/aten/src/ATen/native/cuda/ForeachReduceOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachReduceOp.cu
@@ -19,7 +19,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 
 template<typename T, int NormType, int depth=1, int r_args_depth=1, int res_arg_index=0>
 struct LpNormFunctor {
@@ -185,4 +185,4 @@ std::vector<Tensor> foreach_tensor_norm_cuda(TensorList tensors, const Scalar& o
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachTernaryOp.cu b/aten/src/ATen/native/cuda/ForeachTernaryOp.cu
index 26d3ff2160d..f2041c54ccf 100644
--- a/aten/src/ATen/native/cuda/ForeachTernaryOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachTernaryOp.cu
@@ -13,7 +13,7 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename T>
 struct LerpFunctor {
@@ -115,4 +115,4 @@ void foreach_tensor_lerp_list_cuda_(TensorList tensors1, TensorList tensors2, co
         }
   );
 }
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ForeachUnaryOp.cu b/aten/src/ATen/native/cuda/ForeachUnaryOp.cu
index 693a0d88b62..a7120aa5564 100644
--- a/aten/src/ATen/native/cuda/ForeachUnaryOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachUnaryOp.cu
@@ -39,7 +39,7 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, template<class> class Op> std::vector<Tensor> foreach_unary_op(TensorList tensors) {
     std::vector<std::vector<at::Tensor>> tensor_lists;
@@ -327,4 +327,4 @@ void foreach_tensor_zero_cuda_(TensorList tensors) {
     });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu b/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu
index df7a1ebdddf..4761b88efcc 100644
--- a/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu
+++ b/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu
@@ -24,7 +24,7 @@
 #include <cfloat>
 #include <cmath>
 
-namespace at::native {
+namespace at { namespace native {
 
 using namespace at::cuda::detail;
 
@@ -269,4 +269,4 @@ TORCH_IMPL_FUNC(fractional_max_pool2d_backward_cuda)(
   );
 }
 
-}// at::native
+}}// at::native
diff --git a/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu b/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu
index 4384b07556a..fb8b62711c3 100644
--- a/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu
+++ b/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu
@@ -27,7 +27,7 @@
 #include <cfloat>
 #include <cmath>
 
-namespace at::native {
+namespace at { namespace native {
 
 using namespace at::cuda::detail;
 
@@ -340,4 +340,4 @@ Tensor fractional_max_pool3d_backward_cuda(
     return gradInput;
  }
 
-}// namespace at::native
+}}// namespace at::native
diff --git a/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu b/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu
index 683c9c058a3..7c04ce4da35 100644
--- a/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu
+++ b/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu
@@ -7,7 +7,7 @@
 #include <ATen/cuda/Atomic.cuh>
 #include <ATen/cuda/CUDAContext.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -111,4 +111,4 @@ void _compute_linear_combination_cuda_kernel(
 
 REGISTER_DISPATCH(_compute_linear_combination_stub, &_compute_linear_combination_cuda_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/FusedAdamKernel.cu b/aten/src/ATen/native/cuda/FusedAdamKernel.cu
index b8f514e0f1c..43e4ec57309 100644
--- a/aten/src/ATen/native/cuda/FusedAdamKernel.cu
+++ b/aten/src/ATen/native/cuda/FusedAdamKernel.cu
@@ -6,7 +6,7 @@
 #include <c10/util/Exception.h>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 // note(crcrpar): To observe the CI rules, i.e. 20 minutes per file to compile, defensively split instantiations into _impl files.
 // this is only for CUDA 11.3 for which it took about 20 minutes and 28 minutes in my workstation and CI, respectively.
@@ -42,4 +42,4 @@ void _fused_adam_kernel_cuda_(
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/GcdLcmKernel.cu b/aten/src/ATen/native/cuda/GcdLcmKernel.cu
index c4a8cdfaf1f..3e9817acde1 100644
--- a/aten/src/ATen/native/cuda/GcdLcmKernel.cu
+++ b/aten/src/ATen/native/cuda/GcdLcmKernel.cu
@@ -11,7 +11,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 // See note [Jiterator]
 CONSTEXPR_EXCEPT_WIN_CUDA char gcd_name[] = "gcd";
@@ -55,4 +55,4 @@ void lcm_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(gcd_stub, &gcd_kernel_cuda);
 REGISTER_DISPATCH(lcm_stub, &lcm_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/GridSampler.cpp b/aten/src/ATen/native/cuda/GridSampler.cpp
index efe19edab44..d5e6c87a65b 100644
--- a/aten/src/ATen/native/cuda/GridSampler.cpp
+++ b/aten/src/ATen/native/cuda/GridSampler.cpp
@@ -14,7 +14,7 @@
 #include <ATen/ops/zeros_like.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor grid_sampler_2d_cuda(const Tensor& input, const Tensor& grid,
                             int64_t interpolation_mode, int64_t padding_mode,
@@ -79,4 +79,4 @@ grid_sampler_3d_backward_cuda(const Tensor& grad_output, const Tensor& input,
   return std::make_tuple(grad_input, grad_grid);
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/GridSampler.cu b/aten/src/ATen/native/cuda/GridSampler.cu
index e65f6d59c7d..fd21ee5bfb1 100644
--- a/aten/src/ATen/native/cuda/GridSampler.cu
+++ b/aten/src/ATen/native/cuda/GridSampler.cu
@@ -13,7 +13,7 @@
 #include <c10/macros/Macros.h>
 #include <cmath>
 
-namespace at::native {
+namespace at { namespace native {
 
 using namespace at::cuda::detail;
 
@@ -950,4 +950,4 @@ void launch_grid_sampler_3d_backward_kernel(
   }
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/IGammaKernel.cu b/aten/src/ATen/native/cuda/IGammaKernel.cu
index be3f7fc54a6..06fd3f7fa0c 100644
--- a/aten/src/ATen/native/cuda/IGammaKernel.cu
+++ b/aten/src/ATen/native/cuda/IGammaKernel.cu
@@ -531,7 +531,7 @@ struct CalcIgamma{
 
 // end of regularized lower & upper incomplete gamma
 
-namespace at::native {
+namespace at { namespace native {
 
 void igamma_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES(iter.common_dtype(), "igamma_cuda", [&]() {
@@ -551,4 +551,4 @@ REGISTER_DISPATCH(igammac_stub, &igammac_kernel_cuda);
 // DO NOT ADD ANY NEW KERNELS HERE
 // CUDA compilation times grow quickly.  It's perfectly acceptable to have a file per kernel.
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Im2Col.cu b/aten/src/ATen/native/cuda/Im2Col.cu
index 1a79db8dcde..fb987f4d564 100644
--- a/aten/src/ATen/native/cuda/Im2Col.cu
+++ b/aten/src/ATen/native/cuda/Im2Col.cu
@@ -20,7 +20,7 @@
 #include <ATen/ops/im2col_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 static void im2col_out_cuda_template(
@@ -162,4 +162,4 @@ Tensor im2col_cuda(
   return output;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/IndexKernel.cpp b/aten/src/ATen/native/cuda/IndexKernel.cpp
index b5e92a19770..70e01064629 100644
--- a/aten/src/ATen/native/cuda/IndexKernel.cpp
+++ b/aten/src/ATen/native/cuda/IndexKernel.cpp
@@ -19,7 +19,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 
 static Tensor & masked_select_out_cuda_impl(Tensor & result, const Tensor & self, const Tensor & mask) {
   NoNamesGuard guard;
@@ -84,4 +84,4 @@ Tensor & masked_scatter__cuda(Tensor& self, const Tensor& mask, const Tensor& so
   return self;
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/IndexKernel.cu b/aten/src/ATen/native/cuda/IndexKernel.cu
index 670b27ebbab..b2a12d3d0e3 100644
--- a/aten/src/ATen/native/cuda/IndexKernel.cu
+++ b/aten/src/ATen/native/cuda/IndexKernel.cu
@@ -16,7 +16,7 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 static constexpr int launch_bound2 = 4;
 
@@ -469,4 +469,4 @@ REGISTER_DISPATCH(flip_stub, &flip_kernel);
 
 REGISTER_CUDA_DISPATCH(index_put_kernel_quantized_stub, &index_put_kernel_quantized_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Indexing.cu b/aten/src/ATen/native/cuda/Indexing.cu
index 85ff7c38057..326f7a5ebac 100644
--- a/aten/src/ATen/native/cuda/Indexing.cu
+++ b/aten/src/ATen/native/cuda/Indexing.cu
@@ -184,7 +184,7 @@ __global__ void indexing_backward_kernel_quantized(
 }
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -1668,4 +1668,4 @@ Tensor index_select_sparse_cuda(const Tensor& self, int64_t dim, const Tensor& i
 }
 
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/KernelUtils.cuh b/aten/src/ATen/native/cuda/KernelUtils.cuh
index e1b9f380723..aacc3cf15dd 100644
--- a/aten/src/ATen/native/cuda/KernelUtils.cuh
+++ b/aten/src/ATen/native/cuda/KernelUtils.cuh
@@ -1,7 +1,8 @@
 #pragma once
 #include <ATen/cuda/Atomic.cuh>
 
-#if !(defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))
+// #if (!(defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))) && CUDA_VERSION >= 11000
+#if CUDA_VERSION >= 11000
 #include <cuda_bf16.h>
 #endif
 
@@ -78,7 +79,8 @@ __device__ __forceinline__ void fastSpecializedAtomicAdd(
     scalar_t value) {
 #if (                      \
     (defined(USE_ROCM)) || \
-    (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))
+    (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))) || \
+    CUDA_VERSION < 11000
   gpuAtomicAddNoReturn(
       reinterpret_cast<at::BFloat16*>(tensor) + index,
       static_cast<at::BFloat16>(value));
diff --git a/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu b/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
index ffcb5e50339..b12db98426c 100644
--- a/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
+++ b/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
@@ -17,7 +17,7 @@
 #include <thrust/device_ptr.h>
 #include <thrust/iterator/constant_iterator.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void index_put_with_sort_kernel_thrust_helper(Tensor &linearIndex, Tensor &orig_indices, Tensor &sorted_indices, int64_t num_indices) {
   sorted_indices.copy_(linearIndex);
@@ -110,4 +110,4 @@ int64_t embedding_backward_cuda_kernel_unique_by_key<int>(const Tensor &sorted_i
 template
 int64_t embedding_backward_cuda_kernel_unique_by_key<int64_t>(const Tensor &sorted_indices, Tensor &segment_offsets);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Lerp.cu b/aten/src/ATen/native/cuda/Lerp.cu
index 01053a3beea..6ecf513e2d2 100644
--- a/aten/src/ATen/native/cuda/Lerp.cu
+++ b/aten/src/ATen/native/cuda/Lerp.cu
@@ -6,7 +6,7 @@
 #include <ATen/native/cuda/JitLoops.cuh>
 #include <ATen/OpMathType.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char lerp_tensor_name[] = "lerp_tensor";
@@ -124,4 +124,4 @@ void lerp_scalar_kernel(at::TensorIteratorBase& iter, const c10::Scalar& weight)
 REGISTER_DISPATCH(lerp_kernel_tensor_weight, &lerp_tensor_kernel);
 REGISTER_DISPATCH(lerp_kernel_scalar_weight, &lerp_scalar_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/LinearAlgebra.cu b/aten/src/ATen/native/cuda/LinearAlgebra.cu
index fb59f976041..4d312a27b57 100644
--- a/aten/src/ATen/native/cuda/LinearAlgebra.cu
+++ b/aten/src/ATen/native/cuda/LinearAlgebra.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/ReduceOps.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -139,4 +139,4 @@ void unpack_pivots_cuda_kernel(TensorIterator& iter, const int64_t dim_size, con
 
 REGISTER_DISPATCH(unpack_pivots_stub, &unpack_pivots_cuda_kernel);
 REGISTER_DISPATCH(addr_stub, &addr_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp b/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp
index 045bfa8d1f9..1fc90333ebb 100644
--- a/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp
+++ b/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp
@@ -29,7 +29,7 @@ struct MagmaInitializer {
 }  // namespace (anonymous)
 #endif
 #endif
-namespace at::native {
+namespace at { namespace native {
 #if defined(BUILD_LAZY_CUDA_LINALG)
 namespace {
 cuda::detail::LinalgDispatch disp = {_cholesky_solve_helper_cuda};
@@ -175,4 +175,4 @@ Tensor _cholesky_solve_helper_cuda(const Tensor& self, const Tensor& A, bool upp
 
 #endif /*defined(BUILD_LAZY_CUDA_LINALG)*/
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/LogAddExpKernel.cu b/aten/src/ATen/native/cuda/LogAddExpKernel.cu
index c167ef698d3..795ee455006 100644
--- a/aten/src/ATen/native/cuda/LogAddExpKernel.cu
+++ b/aten/src/ATen/native/cuda/LogAddExpKernel.cu
@@ -10,7 +10,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void logaddexp_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(
@@ -54,4 +54,4 @@ void logaddexp2_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(logaddexp_stub, &logaddexp_kernel_cuda);
 REGISTER_DISPATCH(logaddexp2_stub, &logaddexp2_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu b/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu
index ea4188c970c..750c4edccd4 100644
--- a/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu
+++ b/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu
@@ -9,7 +9,7 @@
 #include <cmath>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 // custom min and max to be used in logcumsumexp for complex arguments
 template <typename scalar_t, bool min>
@@ -116,4 +116,4 @@ void launch_logcumsumexp_cuda_kernel(const TensorBase& result, const TensorBase&
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Loss.cu b/aten/src/ATen/native/cuda/Loss.cu
index c4f1f064614..4b0b19c89e8 100644
--- a/aten/src/ATen/native/cuda/Loss.cu
+++ b/aten/src/ATen/native/cuda/Loss.cu
@@ -58,7 +58,7 @@ void binary_cross_entropy_backward_out_kernel(Tensor& grad_input, const Tensor&
 
 } // namespace
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor binary_cross_entropy_cuda(const Tensor& input, const Tensor& target, const c10::optional<Tensor>& weight_opt, int64_t reduction) {
   // See [Note: hacky wrapper removal for optional tensor]
@@ -611,4 +611,4 @@ TORCH_IMPL_FUNC(nll_loss_backward_out_cuda)
       reduction,
       ignore_index);
 }
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/LossCTC.cu b/aten/src/ATen/native/cuda/LossCTC.cu
index bb70b8313e7..1cc9382c75c 100644
--- a/aten/src/ATen/native/cuda/LossCTC.cu
+++ b/aten/src/ATen/native/cuda/LossCTC.cu
@@ -36,7 +36,7 @@
 #include <type_traits>
 #include <numeric>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -780,4 +780,4 @@ Tensor ctc_loss_backward_gpu(const Tensor& grad, const Tensor& log_probs, const
     });
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu b/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu
index 51c82e95213..9c6f24677b9 100644
--- a/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu
+++ b/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu
@@ -9,7 +9,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void maximum_kernel_cuda(TensorIteratorBase& iter) {
   if (iter.dtype() == ScalarType::Bool) {
@@ -95,4 +95,4 @@ REGISTER_DISPATCH(minimum_stub, &minimum_kernel_cuda);
 REGISTER_DISPATCH(fmax_stub, &fmax_kernel_cuda);
 REGISTER_DISPATCH(fmin_stub, &fmin_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/MaxUnpooling.cu b/aten/src/ATen/native/cuda/MaxUnpooling.cu
index 3eedfe6ba9e..4690ba5dd00 100644
--- a/aten/src/ATen/native/cuda/MaxUnpooling.cu
+++ b/aten/src/ATen/native/cuda/MaxUnpooling.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/empty_like.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 using namespace at::cuda::detail;
 
@@ -609,4 +609,4 @@ at::Tensor max_unpooling3d_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu b/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu
index debe1bd6180..5de84f0f5fb 100644
--- a/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu
+++ b/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu
@@ -18,7 +18,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 const int MULTILABELMARGIN_THREADS = 128;
@@ -440,4 +440,4 @@ Tensor multilabel_margin_loss_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/MultiMarginLoss.cu b/aten/src/ATen/native/cuda/MultiMarginLoss.cu
index 96f41357f25..f9cf324ec99 100644
--- a/aten/src/ATen/native/cuda/MultiMarginLoss.cu
+++ b/aten/src/ATen/native/cuda/MultiMarginLoss.cu
@@ -16,7 +16,7 @@
 #include <ATen/ops/multi_margin_loss_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 constexpr int MULTIMARGIN_THREADS = 128;
 
@@ -391,4 +391,4 @@ Tensor multi_margin_loss_cuda_backward(
   return grad_input;
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/MultinomialKernel.cu b/aten/src/ATen/native/cuda/MultinomialKernel.cu
index 690b8e44439..eddc8aa5c42 100644
--- a/aten/src/ATen/native/cuda/MultinomialKernel.cu
+++ b/aten/src/ATen/native/cuda/MultinomialKernel.cu
@@ -26,7 +26,7 @@
 #include <curand_kernel.h>
 #include <curand_philox4x32_x.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -457,4 +457,4 @@ void multinomial_with_replacement_kernel_impl(
 REGISTER_DISPATCH(
     multinomial_with_replacement_stub,
     &multinomial_with_replacement_kernel_impl);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/NLLLoss2d.cu b/aten/src/ATen/native/cuda/NLLLoss2d.cu
index ba98f18427d..e46d216db75 100644
--- a/aten/src/ATen/native/cuda/NLLLoss2d.cu
+++ b/aten/src/ATen/native/cuda/NLLLoss2d.cu
@@ -22,7 +22,7 @@
 #include <ATen/ops/nll_loss2d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -525,4 +525,4 @@ Tensor nll_loss2d_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu
index e8b4122092e..87df9cdc4cc 100644
--- a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu
+++ b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu
@@ -23,7 +23,7 @@
 #include <ATen/ops/slow_conv_transpose2d_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 static inline void slow_conv_transpose2d_shape_check(
@@ -830,4 +830,4 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv_transpose2d_backward_cuda(
 
 REGISTER_CUDA_DISPATCH(slow_conv_transpose2d_backward_stub, &slow_conv_transpose2d_backward_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu
index c5067c5b1e1..37c41bd2d18 100644
--- a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu
+++ b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu
@@ -22,7 +22,7 @@
 #include <ATen/ops/slow_conv_transpose3d_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 static inline void slow_conv_transpose3d_shape_check(
@@ -1013,4 +1013,4 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv_transpose3d_backward_cuda(
 
 REGISTER_CUDA_DISPATCH(slow_conv_transpose3d_backward_stub, &slow_conv_transpose3d_backward_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu b/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu
index 1ef3268ddac..000b37f1da6 100644
--- a/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu
+++ b/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu
@@ -22,7 +22,7 @@
 
 #include <tuple>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -611,4 +611,4 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv_dilated3d_backward_cuda(
 REGISTER_CUDA_DISPATCH(slow_conv_dilated2d_backward_stub, &slow_conv_dilated2d_backward_cuda);
 REGISTER_CUDA_DISPATCH(slow_conv_dilated3d_backward_stub, &slow_conv_dilated3d_backward_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Nonzero.cu b/aten/src/ATen/native/cuda/Nonzero.cu
index 7cd6392bbfd..d5450c2d337 100644
--- a/aten/src/ATen/native/cuda/Nonzero.cu
+++ b/aten/src/ATen/native/cuda/Nonzero.cu
@@ -16,7 +16,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace{
 template<typename T>
@@ -127,4 +127,4 @@ Tensor nonzero_cuda(const Tensor& self){
   Tensor out = at::detail::empty_cuda({0}, self.options().dtype(kLong));
   return at::native::nonzero_out_cuda(self, out);
 }
-} //namespace at::native
+}} //namespace at::native
diff --git a/aten/src/ATen/native/cuda/Normalization.cu b/aten/src/ATen/native/cuda/Normalization.cu
index cd3f4543f29..6a2a91855c1 100644
--- a/aten/src/ATen/native/cuda/Normalization.cu
+++ b/aten/src/ATen/native/cuda/Normalization.cu
@@ -26,7 +26,7 @@
 #include <ATen/ops/scalar_tensor.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -736,4 +736,4 @@ std::tuple<Tensor, Tensor> batch_norm_update_stats_cuda(
   return std::tuple<Tensor, Tensor>(save_mean, save_var);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Normalization.cuh b/aten/src/ATen/native/cuda/Normalization.cuh
index cc79284fea4..eb6c0fa2216 100644
--- a/aten/src/ATen/native/cuda/Normalization.cuh
+++ b/aten/src/ATen/native/cuda/Normalization.cuh
@@ -1739,4 +1739,4 @@ at::Tensor batch_norm_backward_elemt_channels_last_cuda_template(
   return grad_input;
 }
 
-} } // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu b/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu
index 53b67125222..86a62817463 100644
--- a/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu
+++ b/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/PointwiseOps.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char addcmul_name[] = "addcmul";
 void addcmul_cuda_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -148,4 +148,4 @@ REGISTER_DISPATCH(addcmul_stub, &addcmul_cuda_kernel);
 REGISTER_DISPATCH(smooth_l1_backward_stub, &smooth_l1_backward_cuda_kernel);
 REGISTER_DISPATCH(huber_backward_stub, &huber_backward_cuda_kernel);
 REGISTER_DISPATCH(mse_backward_stub, &mse_backward_cuda_kernel);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/PowKernel.cu b/aten/src/ATen/native/cuda/PowKernel.cu
index eb56da722fb..7ec3660e795 100644
--- a/aten/src/ATen/native/cuda/PowKernel.cu
+++ b/aten/src/ATen/native/cuda/PowKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/Pow.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // Forward declare some unary kernels
 void rsqrt_kernel_cuda(TensorIteratorBase& iter);
@@ -206,4 +206,4 @@ void pow_tensor_scalar_kernel(TensorIteratorBase& iter, const Scalar& exp_scalar
 REGISTER_DISPATCH(pow_tensor_tensor_stub, &pow_tensor_tensor_kernel);
 REGISTER_DISPATCH(pow_tensor_scalar_stub, &pow_tensor_scalar_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/RNN.cu b/aten/src/ATen/native/cuda/RNN.cu
index ef384268c81..ed34bc78fba 100644
--- a/aten/src/ATen/native/cuda/RNN.cu
+++ b/aten/src/ATen/native/cuda/RNN.cu
@@ -19,7 +19,7 @@
 #include <ATen/ops/_thnn_fused_gru_cell_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -646,4 +646,4 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> _thnn_fused_gru_cell_backward
   return std::make_tuple(grad_input_gates, grad_hidden_gates, grad_hx, grad_input_bias, grad_hidden_bias);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Randperm.cu b/aten/src/ATen/native/cuda/Randperm.cu
index 46e14f9f2ea..0d02e635ea0 100644
--- a/aten/src/ATen/native/cuda/Randperm.cu
+++ b/aten/src/ATen/native/cuda/Randperm.cu
@@ -18,7 +18,7 @@
 
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 // [Algorithm of randperm]
 //
@@ -130,4 +130,4 @@ Tensor& randperm_out_cuda(int64_t n, c10::optional<Generator> generator, Tensor&
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/RangeFactories.cu b/aten/src/ATen/native/cuda/RangeFactories.cu
index 83a803d2185..af2a05325a5 100644
--- a/aten/src/ATen/native/cuda/RangeFactories.cu
+++ b/aten/src/ATen/native/cuda/RangeFactories.cu
@@ -67,7 +67,7 @@ void gpu_kernel_with_index(at::Tensor &output, func_t f) {
 
 }  // namespace
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor& linspace_cuda_out(const Scalar& start, const Scalar& end, int64_t steps, Tensor& result) {
   TORCH_CHECK(steps >= 0, "number of steps must be non-negative");
@@ -271,4 +271,4 @@ Tensor& arange_cuda_out(const Scalar& start, const Scalar& end, const Scalar& st
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/RecordStream.cu b/aten/src/ATen/native/cuda/RecordStream.cu
index cc7fc20d433..e98ce93f803 100644
--- a/aten/src/ATen/native/cuda/RecordStream.cu
+++ b/aten/src/ATen/native/cuda/RecordStream.cu
@@ -8,9 +8,9 @@
 #include <ATen/ops/record_stream_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 void record_stream_cuda(Tensor& self, c10::Stream stream) {
   struct c10::StreamData3 data = stream.pack3();
   c10::cuda::CUDACachingAllocator::recordStream(self.storage().data_ptr(), at::cuda::CUDAStream::unpack3(data.stream_id, data.device_index, data.device_type));
 }
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Reduce.cu b/aten/src/ATen/native/cuda/Reduce.cu
index 36a13134882..2de32f6d4a3 100644
--- a/aten/src/ATen/native/cuda/Reduce.cu
+++ b/aten/src/ATen/native/cuda/Reduce.cu
@@ -5,7 +5,7 @@
 #include <iostream>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 static inline std::ostream& operator<<(std::ostream& out, dim3 dim) {
   if (dim.y == 1 && dim.z == 1) {
@@ -53,4 +53,4 @@ std::ostream& operator<<(std::ostream& out, const ReduceConfig& config) {
   return out;
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu b/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu
index cdd5daab2d9..bfa598d2869 100644
--- a/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu
@@ -15,7 +15,7 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t>
 void _min_max_values_kernel_cuda_impl(TensorIterator& iter) {
@@ -46,4 +46,4 @@ void aminmax_launch_kernel(TensorIterator& iter) {
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu b/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu
index c5d763f3135..f97b1bb8f3d 100644
--- a/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu
@@ -15,7 +15,7 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, typename acc_t = scalar_t>
 void argmax_kernel_cuda_impl(TensorIterator& iter) {
@@ -43,4 +43,4 @@ void argmax_kernel_cuda(TensorIterator& iter) {
 
 REGISTER_DISPATCH(argmax_stub, &argmax_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu b/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu
index fc34c11c519..8678a1721a2 100644
--- a/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu
@@ -15,7 +15,7 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, typename acc_t = scalar_t>
 void argmin_kernel_cuda_impl(TensorIterator& iter) {
@@ -43,4 +43,4 @@ void argmin_kernel_cuda(TensorIterator& iter) {
 
 REGISTER_DISPATCH(argmin_stub, &argmin_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceLogicKernel.cu b/aten/src/ATen/native/cuda/ReduceLogicKernel.cu
index 3f65c745d7a..9f1e3db0d60 100644
--- a/aten/src/ATen/native/cuda/ReduceLogicKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceLogicKernel.cu
@@ -6,7 +6,7 @@
 #include <ATen/native/ReduceOps.h>
 #include <ATen/Dispatch.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void and_kernel_cuda(TensorIterator& iter) {
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
@@ -35,4 +35,4 @@ void or_kernel_cuda(TensorIterator& iter) {
 REGISTER_DISPATCH(and_stub, &and_kernel_cuda);
 REGISTER_DISPATCH(or_stub, &or_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu b/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu
index 883e8fe2149..86cfe872936 100644
--- a/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu
@@ -15,7 +15,7 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename acc_t>
 struct MaxNanFunctor {
@@ -58,4 +58,4 @@ void max_all_launch_kernel(TensorIterator &iter) {
 
 REGISTER_DISPATCH(max_values_stub, &max_values_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu b/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu
index a0ccf873be0..54d0f8499e5 100644
--- a/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu
@@ -16,7 +16,7 @@
 #include <ATen/cuda/NumericLimits.cuh>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename acc_t>
 struct MinNanFunctor {
@@ -55,4 +55,4 @@ void min_all_launch_kernel(TensorIterator &iter) {
 
 REGISTER_DISPATCH(min_values_stub, &min_values_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceMomentKernel.cu b/aten/src/ATen/native/cuda/ReduceMomentKernel.cu
index 980f7fa5c36..790e59852aa 100644
--- a/aten/src/ATen/native/cuda/ReduceMomentKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceMomentKernel.cu
@@ -8,7 +8,7 @@
 #include <ATen/Dispatch.h>
 #include <ATen/native/ReduceOps.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, typename out_t=scalar_t>
 void std_var_kernel_impl(TensorIterator& iter, int32_t correction, bool take_sqrt) {
@@ -70,4 +70,4 @@ static void mean_kernel_cuda(TensorIterator& iter) {
 REGISTER_DISPATCH(std_var_stub, &std_var_kernel_cuda);
 REGISTER_DISPATCH(mean_stub, &mean_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceNormKernel.cu b/aten/src/ATen/native/cuda/ReduceNormKernel.cu
index 5ad037f6618..15a747f2a86 100644
--- a/aten/src/ATen/native/cuda/ReduceNormKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceNormKernel.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/LinearAlgebra.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // This reduction accumulates results as the type `acc_t`. By default, when
 // `scalar_t` is complex, `acc_t` is the downgraded real number type.
@@ -48,4 +48,4 @@ void norm_launch_kernel(TensorIterator& iter, double ord) {
   });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceOps.cpp b/aten/src/ATen/native/cuda/ReduceOps.cpp
index d4ed5f7c010..ab878f82e3a 100644
--- a/aten/src/ATen/native/cuda/ReduceOps.cpp
+++ b/aten/src/ATen/native/cuda/ReduceOps.cpp
@@ -24,7 +24,7 @@
 #include <ATen/ops/where.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void norm_kernel_cuda(TensorIterator& iter, const Scalar& val) {
@@ -97,4 +97,4 @@ REGISTER_CUDA_DISPATCH(aminmax_stub, &aminmax_kernel_impl);
 
 REGISTER_CUDA_DISPATCH(norm_stub, &norm_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu b/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
index cf2f5064d36..58e539e057b 100644
--- a/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
@@ -8,7 +8,7 @@
 #include <ATen/jit_macros.h>
 #include <ATen/OpMathType.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, typename acc_t = scalar_t, typename out_t = scalar_t>
 struct sum_functor {
@@ -184,4 +184,4 @@ REGISTER_DISPATCH(sum_stub, &sum_kernel_cuda);
 REGISTER_DISPATCH(nansum_stub, &nansum_kernel_cuda);
 REGISTER_DISPATCH(prod_stub, &prod_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReflectionPad.cu b/aten/src/ATen/native/cuda/ReflectionPad.cu
index c798d73fe89..c9bc9194521 100644
--- a/aten/src/ATen/native/cuda/ReflectionPad.cu
+++ b/aten/src/ATen/native/cuda/ReflectionPad.cu
@@ -24,7 +24,7 @@
 
 #include <thrust/pair.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 using at::cuda::detail::canUse32BitIndexMath;
@@ -677,4 +677,4 @@ TORCH_IMPL_FUNC(reflection_pad3d_backward_out_cuda) (
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/RenormKernel.cu b/aten/src/ATen/native/cuda/RenormKernel.cu
index ef133761aed..579fb50d72b 100644
--- a/aten/src/ATen/native/cuda/RenormKernel.cu
+++ b/aten/src/ATen/native/cuda/RenormKernel.cu
@@ -5,7 +5,7 @@
 
 #include <ATen/Dispatch.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
@@ -26,4 +26,4 @@ void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
 
 REGISTER_DISPATCH(renorm_scale_factor_stub, &renorm_scale_factor_impl);
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Repeat.cu b/aten/src/ATen/native/cuda/Repeat.cu
index 65c6863745c..1717752a72d 100644
--- a/aten/src/ATen/native/cuda/Repeat.cu
+++ b/aten/src/ATen/native/cuda/Repeat.cu
@@ -50,7 +50,7 @@ static void compute_cuda(
   C10_CUDA_KERNEL_LAUNCH_CHECK();
 }
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor repeat_interleave_cuda(
     const Tensor& repeat,
@@ -64,4 +64,4 @@ Tensor repeat_interleave_cuda(
   return output;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReplicationPadding.cu b/aten/src/ATen/native/cuda/ReplicationPadding.cu
index 3ebd7f733b3..85f27fb7523 100644
--- a/aten/src/ATen/native/cuda/ReplicationPadding.cu
+++ b/aten/src/ATen/native/cuda/ReplicationPadding.cu
@@ -27,7 +27,7 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at { namespace native {
 __host__ __device__ __forceinline__ int imin(int a, int b) {
   return a > b ? b : a;
 }
@@ -749,4 +749,4 @@ Tensor replication_pad3d_backward_cuda(
   return gradInput;
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/Resize.cpp b/aten/src/ATen/native/cuda/Resize.cpp
index 89e5b286461..5fcf522be93 100644
--- a/aten/src/ATen/native/cuda/Resize.cpp
+++ b/aten/src/ATen/native/cuda/Resize.cpp
@@ -11,7 +11,7 @@
 #include <ATen/ops/resize_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 void resize_bytes_cuda(StorageImpl* storage, size_t size_bytes) {
   TORCH_CHECK(storage->resizable(), "Trying to resize storage that is not resizable");
@@ -65,4 +65,4 @@ const Tensor& resize_cuda_(
   }
   return self;
 }
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/RreluWithNoise.cu b/aten/src/ATen/native/cuda/RreluWithNoise.cu
index 5184f29562f..c97cd15e1e8 100644
--- a/aten/src/ATen/native/cuda/RreluWithNoise.cu
+++ b/aten/src/ATen/native/cuda/RreluWithNoise.cu
@@ -14,7 +14,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, int unroll_factor, typename F>
 #if __CUDA_ARCH__ >= 350 || defined USE_ROCM
@@ -192,4 +192,4 @@ Tensor& rrelu_with_noise_cuda_(
       self, noise, lower, upper, training, generator, self);
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ScanKernels.cpp b/aten/src/ATen/native/cuda/ScanKernels.cpp
index 463ceb23bad..69f86c00695 100644
--- a/aten/src/ATen/native/cuda/ScanKernels.cpp
+++ b/aten/src/ATen/native/cuda/ScanKernels.cpp
@@ -16,7 +16,7 @@
 #include <ATen/ops/empty_like.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 static c10::MaybeOwned<Tensor> contiguous_out_arg(const Tensor &tensor) {
   if (tensor.is_contiguous()) {
@@ -112,4 +112,4 @@ void cumprod_cuda_kernel(const Tensor& result, const Tensor& self, int64_t dim)
 REGISTER_CUDA_DISPATCH(cumsum_stub, &cumsum_cuda_kernel);
 REGISTER_CUDA_DISPATCH(cumprod_stub, &cumprod_cuda_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ScatterGatherKernel.cu b/aten/src/ATen/native/cuda/ScatterGatherKernel.cu
index 55ff12eb0b0..5291474f475 100644
--- a/aten/src/ATen/native/cuda/ScatterGatherKernel.cu
+++ b/aten/src/ATen/native/cuda/ScatterGatherKernel.cu
@@ -15,7 +15,7 @@
 #include <ATen/cuda/Atomic.cuh>
 #include <ATen/cuda/CUDAContext.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // Implement as functors since lambdas don't get optimized.
 class ReduceMultiply {
@@ -563,4 +563,4 @@ REGISTER_DISPATCH(scatter_reduce_stub, &scatter_reduce_cuda_kernel);
 REGISTER_DISPATCH(scatter_scalar_reduce_stub, &scatter_scalar_reduce_cuda_kernel);
 REGISTER_DISPATCH(scatter_reduce_two_stub, &scatter_reduce_two_cuda_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SegmentReduce.cu b/aten/src/ATen/native/cuda/SegmentReduce.cu
index 1917666c48b..2a2ca2742ee 100644
--- a/aten/src/ATen/native/cuda/SegmentReduce.cu
+++ b/aten/src/ATen/native/cuda/SegmentReduce.cu
@@ -17,7 +17,7 @@
 #include <ATen/ops/cumsum.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 struct CustomMax {
@@ -619,4 +619,4 @@ REGISTER_DISPATCH(
   _segment_reduce_offsets_backward_stub,
   &_segment_reduce_offsets_backward_cuda_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Shape.cu b/aten/src/ATen/native/cuda/Shape.cu
index feb1fd44a24..116727cd194 100644
--- a/aten/src/ATen/native/cuda/Shape.cu
+++ b/aten/src/ATen/native/cuda/Shape.cu
@@ -21,7 +21,7 @@
 #include <ATen/ops/narrow.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 constexpr int CAT_ARRAY_BATCH_SIZE = 128;
 constexpr int CAT_ARRAY_MAX_INPUT_DIMS = 4;
@@ -320,4 +320,4 @@ TORCH_IMPL_FUNC(cat_out_cuda)
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SoftMax.cu b/aten/src/ATen/native/cuda/SoftMax.cu
index 08050fb24f6..96ff12ae327 100644
--- a/aten/src/ATen/native/cuda/SoftMax.cu
+++ b/aten/src/ATen/native/cuda/SoftMax.cu
@@ -28,7 +28,7 @@
 #include <ATen/ops/_softmax_backward_data.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -1113,4 +1113,4 @@ Tensor masked_softmax_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Sort.cpp b/aten/src/ATen/native/cuda/Sort.cpp
index 87475923e51..7d4ff50645f 100644
--- a/aten/src/ATen/native/cuda/Sort.cpp
+++ b/aten/src/ATen/native/cuda/Sort.cpp
@@ -21,7 +21,7 @@
 
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 std::vector<int64_t> infer_dense_strides_dim_last(const Tensor & self, int64_t dim);
 
@@ -124,4 +124,4 @@ void sort_cuda_kernel(
 // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
 REGISTER_CUDA_DISPATCH(sort_stub, &sort_cuda_kernel);
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Sort.cu b/aten/src/ATen/native/cuda/Sort.cu
index cb66b6571c5..e5e3274fd69 100644
--- a/aten/src/ATen/native/cuda/Sort.cu
+++ b/aten/src/ATen/native/cuda/Sort.cu
@@ -13,7 +13,7 @@
 #include <limits>
 #include <c10/core/DeviceArray.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename T>
 static int minimum_grid_for_occupancy(T kernel, int max_block_size) {
@@ -280,4 +280,4 @@ void sortKeyValueInplace(
   }
 }
 
-}  // namespace at::native
+}}  // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SortImpl.cu b/aten/src/ATen/native/cuda/SortImpl.cu
index 5d779d0fd15..7d8f1526dee 100644
--- a/aten/src/ATen/native/cuda/SortImpl.cu
+++ b/aten/src/ATen/native/cuda/SortImpl.cu
@@ -3,7 +3,7 @@
 #include <thrust/execution_policy.h>
 #include <thrust/sort.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 std::vector<int64_t> infer_dense_strides_dim_last(const Tensor & self, int64_t dim) {
   int64_t ndim = self.dim();
@@ -34,4 +34,4 @@ std::vector<int64_t> infer_dense_strides_dim_last(const Tensor & self, int64_t d
   return new_strides_unsort;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SortStable.cu b/aten/src/ATen/native/cuda/SortStable.cu
index 008e1a08f45..0c4d5e3039e 100644
--- a/aten/src/ATen/native/cuda/SortStable.cu
+++ b/aten/src/ATen/native/cuda/SortStable.cu
@@ -15,7 +15,7 @@
 #include <c10/core/DeviceArray.h>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -295,4 +295,4 @@ void launch_stable_sort_kernel(
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Sorting.cpp b/aten/src/ATen/native/cuda/Sorting.cpp
index 9381c0e4f02..87437e17d79 100644
--- a/aten/src/ATen/native/cuda/Sorting.cpp
+++ b/aten/src/ATen/native/cuda/Sorting.cpp
@@ -23,7 +23,7 @@
 #include <ATen/ops/where.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 std::tuple<Tensor&, Tensor&> kthvalue_out_impl_cuda(
@@ -204,4 +204,4 @@ Tensor nanmedian_cuda(const Tensor& self) {
   return median_impl(self, /*ignore_nan=*/true);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Sorting.cu b/aten/src/ATen/native/cuda/Sorting.cu
index 313c6d1ea98..c70950a3b75 100644
--- a/aten/src/ATen/native/cuda/Sorting.cu
+++ b/aten/src/ATen/native/cuda/Sorting.cu
@@ -15,7 +15,7 @@
 #include <cassert>
 #include <cstdlib>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -277,4 +277,4 @@ void launch_median_kernel(
       });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu b/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu
index ead1ff6326e..3ebf0d6f39b 100644
--- a/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu
+++ b/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu
@@ -5,7 +5,7 @@
 #include <ATen/native/cuda/KernelUtils.cuh>
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -172,4 +172,4 @@ void sparse_mask_intersection_out_cuda_kernel(
 REGISTER_CUDA_DISPATCH(mul_sparse_sparse_out_stub, &mul_sparse_sparse_out_cuda_kernel);
 REGISTER_CUDA_DISPATCH(sparse_mask_intersection_out_stub, &sparse_mask_intersection_out_cuda_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SparseMM.cu b/aten/src/ATen/native/cuda/SparseMM.cu
index 78bc554b52e..922efa5f4fc 100644
--- a/aten/src/ATen/native/cuda/SparseMM.cu
+++ b/aten/src/ATen/native/cuda/SparseMM.cu
@@ -8,7 +8,7 @@
 #include <ATen/ops/sspaddmm_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 // sparse, sparse, sparse, dense, real, real -> sparse
 Tensor& _sspaddmm_out_only_sparse_cuda(const Tensor& self,
     const Tensor& mat1, const Tensor& mat2, const Scalar& beta, const Scalar& alpha, Tensor& result) {
@@ -18,4 +18,4 @@ Tensor& _sspaddmm_out_cuda(const Tensor& self,
     const Tensor& mat1, const Tensor& mat2, const Scalar& beta, const Scalar& alpha, Tensor& result) {
   AT_ERROR("NYI: CUDA sspaddmm is not implemented");
 }
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SpectralOps.cpp b/aten/src/ATen/native/cuda/SpectralOps.cpp
index 80bbbcbd89d..43ca4d9f384 100644
--- a/aten/src/ATen/native/cuda/SpectralOps.cpp
+++ b/aten/src/ATen/native/cuda/SpectralOps.cpp
@@ -30,7 +30,8 @@
 #include <vector>
 
 
-namespace at::native {
+namespace at { 
+namespace native {
 
 using namespace at::native::detail;
 
@@ -534,4 +535,4 @@ Tensor& _fft_c2c_cufft_out(const Tensor& self, IntArrayRef dim,
 }
 
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/SpectralOps.cu b/aten/src/ATen/native/cuda/SpectralOps.cu
index fac39dca008..2f5c1300657 100644
--- a/aten/src/ATen/native/cuda/SpectralOps.cu
+++ b/aten/src/ATen/native/cuda/SpectralOps.cu
@@ -11,7 +11,7 @@
 #include <vector>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 // Offset calculator for indexing in Hermitian mirrored order.
 // In mirrored dims, maps linear index i to (n - i) % n
@@ -121,4 +121,4 @@ void _fft_fill_with_conjugate_symmetry_cuda_(
 
 REGISTER_DISPATCH(fft_fill_with_conjugate_symmetry_stub, &_fft_fill_with_conjugate_symmetry_cuda_);
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/StepKernel.cu b/aten/src/ATen/native/cuda/StepKernel.cu
index 72ad8298287..2252987923c 100644
--- a/aten/src/ATen/native/cuda/StepKernel.cu
+++ b/aten/src/ATen/native/cuda/StepKernel.cu
@@ -9,7 +9,7 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at { namespace native {
 
 void nextafter_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND(kBFloat16, iter.common_dtype(), "nextafter_cuda", [&]() {
@@ -30,4 +30,4 @@ void heaviside_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(nextafter_stub, &nextafter_kernel_cuda);
 REGISTER_DISPATCH(heaviside_stub, &heaviside_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorCompare.cpp b/aten/src/ATen/native/cuda/TensorCompare.cpp
index 1b4d7490b03..b99df69f3b2 100644
--- a/aten/src/ATen/native/cuda/TensorCompare.cpp
+++ b/aten/src/ATen/native/cuda/TensorCompare.cpp
@@ -2,7 +2,7 @@
 #include <ATen/core/Tensor.h>
 #include <ATen/native/TensorCompare.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -20,4 +20,4 @@ void isin_default_kernel_gpu(
 
 REGISTER_CUDA_DISPATCH(isin_default_stub, &isin_default_kernel_gpu);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorCompare.cu b/aten/src/ATen/native/cuda/TensorCompare.cu
index 0accbf91b9c..70fecd4b6c1 100644
--- a/aten/src/ATen/native/cuda/TensorCompare.cu
+++ b/aten/src/ATen/native/cuda/TensorCompare.cu
@@ -7,7 +7,7 @@
 #include <c10/core/Scalar.h>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -125,4 +125,4 @@ void _assert_async_cuda(const Tensor& self_tensor) {
   });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorFactories.cu b/aten/src/ATen/native/cuda/TensorFactories.cu
index ac0eefeb657..e38f3bccfc6 100644
--- a/aten/src/ATen/native/cuda/TensorFactories.cu
+++ b/aten/src/ATen/native/cuda/TensorFactories.cu
@@ -28,7 +28,7 @@
 #include <cmath>
 #include <cstddef>
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor& eye_out_cuda(int64_t n, Tensor& result) {
   // the default value of `m` equals to `n`
@@ -383,4 +383,4 @@ Tensor triu_indices_cuda(
   return tensor;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorModeKernel.cpp b/aten/src/ATen/native/cuda/TensorModeKernel.cpp
index db15c9b76e3..acea19bcc0f 100644
--- a/aten/src/ATen/native/cuda/TensorModeKernel.cpp
+++ b/aten/src/ATen/native/cuda/TensorModeKernel.cpp
@@ -11,7 +11,7 @@ constexpr int MAX_BLOCK_SIZE = AT_ROCM_ENABLED() ? 256 : 1024;
 // Maximum size per grid dimension that we assume (compute capability >= 2.0)
 constexpr int64_t MAX_GRID_SIZE = 65535LL;
 
-namespace at::native {
+namespace at { namespace native {
 
 void mode_kernel_impl(
     Tensor& values,
@@ -97,4 +97,4 @@ void mode_kernel_impl(
 }
 
 REGISTER_CUDA_DISPATCH(mode_stub, &mode_kernel_impl);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorModeKernel.cu b/aten/src/ATen/native/cuda/TensorModeKernel.cu
index 77f88f984d4..e56aca874da 100644
--- a/aten/src/ATen/native/cuda/TensorModeKernel.cu
+++ b/aten/src/ATen/native/cuda/TensorModeKernel.cu
@@ -18,7 +18,7 @@
 #include <thrust/sequence.h>
 #include <thrust/sort.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t>
 struct ModeImpl {
@@ -282,4 +282,4 @@ void launch_apply_mode_kernel(const TensorBase &values, const TensorBase &indice
   });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp b/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp
index 8edfa1a6744..75f19a2ff91 100644
--- a/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp
+++ b/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp
@@ -10,7 +10,7 @@
 #include <ATen/ops/set_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 // this needs to be split along CPU/CUDA lines because we don't have a consistent
 // way of getting the allocator to use for a device (c10::GetAllocator is not
@@ -38,4 +38,4 @@ Tensor& set_storage_cuda_(Tensor& result, Storage storage, int64_t storage_offse
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorTopK.cpp b/aten/src/ATen/native/cuda/TensorTopK.cpp
index 36e45d4dae2..7a17f9a7064 100644
--- a/aten/src/ATen/native/cuda/TensorTopK.cpp
+++ b/aten/src/ATen/native/cuda/TensorTopK.cpp
@@ -17,7 +17,7 @@
 #include <ATen/ops/topk_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 // TODO: remove this when CUDA <11.6 is no longer supported
 void topk_out_with_sort(
@@ -94,4 +94,4 @@ TORCH_IMPL_FUNC(topk_out_cuda)
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorTopK.cu b/aten/src/ATen/native/cuda/TensorTopK.cu
index bd48c9b0580..bfd74640498 100644
--- a/aten/src/ATen/native/cuda/TensorTopK.cu
+++ b/aten/src/ATen/native/cuda/TensorTopK.cu
@@ -19,7 +19,7 @@
 
 using namespace at::native;
 
-namespace at::native {
+namespace at { namespace native {
 
 // TODO: remove this when CUDA <11.6 is no longer supported
 bool disable_sort_for_topk() {
@@ -617,7 +617,7 @@ int get_items_per_thread(uint64_t num_slices, uint64_t slice_size) {
   int max_blocks_per_mp = 32;
 #else
   int regs_per_mp = prop->regsPerMultiprocessor;
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   int max_blocks_per_mp = prop->maxBlocksPerMultiProcessor;
 #else
   int max_blocks_per_mp = 32;
@@ -904,4 +904,4 @@ void launch_gather_topk_kernel(
 #undef RUN_K
 }
 
-} // at::native
+}} // at::native
diff --git a/aten/src/ATen/native/cuda/TensorTransformations.cu b/aten/src/ATen/native/cuda/TensorTransformations.cu
index 4fda9d7a880..1ecc618b0cb 100644
--- a/aten/src/ATen/native/cuda/TensorTransformations.cu
+++ b/aten/src/ATen/native/cuda/TensorTransformations.cu
@@ -18,7 +18,7 @@
 #include <cstddef>
 #include <vector>
 
-namespace at::native {
+namespace at { namespace native {
 
 template <typename scalar_t, typename IndexType>
 #if __CUDA_ARCH__ >= 350 || defined(USE_ROCM)
@@ -151,4 +151,4 @@ Tensor roll_cuda(const Tensor& self, IntArrayRef shifts, IntArrayRef dims) {
   return out_tensor;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TriangularOps.cu b/aten/src/ATen/native/cuda/TriangularOps.cu
index 549ef15293e..e14238ae4b6 100644
--- a/aten/src/ATen/native/cuda/TriangularOps.cu
+++ b/aten/src/ATen/native/cuda/TriangularOps.cu
@@ -19,7 +19,7 @@
 
 #include <ATen/cuda/CUDAApplyUtils.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ triu/tril ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
@@ -106,4 +106,4 @@ Tensor trace_cuda(const Tensor& self) {
   return self.diagonal().sum();
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryComplexKernels.cu b/aten/src/ATen/native/cuda/UnaryComplexKernels.cu
index 688974db517..d970325dae6 100644
--- a/aten/src/ATen/native/cuda/UnaryComplexKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryComplexKernels.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/DispatchStub.h>
 #include <ATen/native/TensorIterator.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // We manually overload angle because std::arg does not work with types other than c10::complex.
 template<typename scalar_t>
@@ -96,4 +96,4 @@ void conj_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(angle_stub, &angle_kernel_cuda);
 REGISTER_DISPATCH(conj_physical_stub, &conj_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryFractionKernels.cu b/aten/src/ATen/native/cuda/UnaryFractionKernels.cu
index 2cc6b1b5581..ae4d4a01aa0 100644
--- a/aten/src/ATen/native/cuda/UnaryFractionKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryFractionKernels.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cuda/Math.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 // We manually overload ceil because std::ceil does not work with std::complex types.
 template <typename scalar_t>
@@ -196,4 +196,4 @@ REGISTER_DISPATCH(round_stub, &round_kernel_cuda);
 REGISTER_DISPATCH(round_decimals_stub, &round_decimals_kernel_cuda);
 REGISTER_DISPATCH(trunc_stub, &trunc_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGammaKernels.cu b/aten/src/ATen/native/cuda/UnaryGammaKernels.cu
index f4a540fcf93..ff0fd0f3268 100644
--- a/aten/src/ATen/native/cuda/UnaryGammaKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryGammaKernels.cu
@@ -10,7 +10,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/Math.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // See note [Jiterator]
 CONSTEXPR_EXCEPT_WIN_CUDA char digamma_name[] = "digamma";
@@ -105,4 +105,4 @@ REGISTER_DISPATCH(digamma_stub, &digamma_kernel_cuda);
 REGISTER_DISPATCH(polygamma_stub, &polygamma_kernel_cuda);
 REGISTER_DISPATCH(lgamma_stub, &lgamma_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu
index 329fd465d2f..3ccd6897750 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char acos_name[] = "acos";
 void acos_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void acos_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(acos_stub, &acos_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu
index ad48e51af3c..34e40d5f043 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char acosh_name[] = "acosh";
 void acosh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void acosh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(acosh_stub, &acosh_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu
index 6b3cec3b96c..67be42e4f54 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char asin_name[] = "asin";
 void asin_kernel_cuda(TensorIteratorBase& iter) {
@@ -47,4 +47,4 @@ void asin_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(asin_stub, &asin_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu
index 7ffe938181d..520b46c07c3 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char asinh_name[] = "asinh";
 void asinh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void asinh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(asinh_stub, &asinh_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu
index d56f75efd4e..0fe93cb82f6 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char atan_name[] = "atan";
 void atan_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void atan_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(atan_stub, &atan_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu
index 55c9919c2ca..255bd3b64dc 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char atanh_name[] = "atanh";
 void atanh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void atanh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(atanh_stub, &atanh_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu
index 1359d0a16ae..02c588b7669 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char cos_name[] = "cos";
 void cos_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +50,4 @@ void cos_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(cos_stub, &cos_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu
index c9608a1ba2a..4cb7501d71b 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char cosh_name[] = "cosh";
 void cosh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void cosh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(cosh_stub, &cosh_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu
index f7d6d5e3b42..1927da380bc 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char sin_name[] = "sin";
 void sin_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +50,4 @@ void sin_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(sin_stub, &sin_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu
index 22dd2bf2ab2..3078744653a 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char sinh_name[] = "sinh";
 void sinh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void sinh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(sinh_stub, &sinh_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu
index 91208b69e48..488383ba08a 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char tan_name[] = "tan";
 void tan_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +50,4 @@ void tan_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(tan_stub, &tan_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu
index 9e6184f7a3f..87f244e1284 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu
@@ -9,7 +9,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char tanh_name[] = "tanh";
 void tanh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +51,4 @@ void tanh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(tanh_stub, &tanh_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryLogKernels.cu b/aten/src/ATen/native/cuda/UnaryLogKernels.cu
index fb3d19baca3..8b9738bd14e 100644
--- a/aten/src/ATen/native/cuda/UnaryLogKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryLogKernels.cu
@@ -10,7 +10,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cuda/Math.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char log_name[] = "log_kernel";
 void log_kernel_cuda(TensorIteratorBase& iter) {
@@ -115,4 +115,4 @@ REGISTER_DISPATCH(log10_stub, &log10_kernel_cuda);
 REGISTER_DISPATCH(log2_stub, &log2_kernel_cuda);
 REGISTER_DISPATCH(log1p_stub, &log1p_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnaryOpsKernel.cu b/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
index 07d5527e87d..f45fe42c3e4 100644
--- a/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
@@ -18,7 +18,7 @@
 #include <c10/core/Scalar.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void bitwise_not_kernel_cuda(TensorIteratorBase& iter) {
   if (iter.dtype() == ScalarType::Bool) {
@@ -256,4 +256,4 @@ REGISTER_DISPATCH(sqrt_stub, &sqrt_kernel_cuda);
 REGISTER_DISPATCH(nan_to_num_stub, &nan_to_num_kernel_cuda);
 REGISTER_DISPATCH(frexp_stub, &frexp_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnarySignKernels.cu b/aten/src/ATen/native/cuda/UnarySignKernels.cu
index 83233f3143c..1dab14f5c5b 100644
--- a/aten/src/ATen/native/cuda/UnarySignKernels.cu
+++ b/aten/src/ATen/native/cuda/UnarySignKernels.cu
@@ -12,7 +12,7 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at { namespace native {
 
 void logical_not_kernel_cuda(TensorIteratorBase& iter) {
   // error check -- this is just ensuring we don't dispatch on types that aren't in ALL_TYPES_AND_COMPLEX_AND3(...)
@@ -134,4 +134,4 @@ REGISTER_DISPATCH(sign_stub, &sign_kernel_cuda);
 REGISTER_DISPATCH(signbit_stub, &signbit_kernel_cuda);
 REGISTER_DISPATCH(sgn_stub, &sgn_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu b/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu
index cd62641a80d..d59c798e723 100644
--- a/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu
+++ b/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu
@@ -17,7 +17,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char exp2_name[] = "exp2_kernel";
 void exp2_kernel_cuda(TensorIteratorBase& iter) {
@@ -394,4 +394,4 @@ REGISTER_DISPATCH(special_ndtri_stub, &ndtri_kernel_cuda);
 REGISTER_DISPATCH(special_log_ndtr_stub, &log_ndtr_kernel_cuda);
 REGISTER_DISPATCH(special_erfcx_stub, &erfcx_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu b/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu
index 2f48d4fc014..d75de2a6e90 100644
--- a/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu
+++ b/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu
@@ -15,7 +15,7 @@
 // unfold_backward, the algorithm is described in
 // /native/cpu/UnfoldBackwardKernel.cpp
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -159,4 +159,4 @@ void unfold_backward_cuda_kernel(
 
 REGISTER_DISPATCH(unfold_backward_stub, &unfold_backward_cuda_kernel);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Unique.cu b/aten/src/ATen/native/cuda/Unique.cu
index fd1dc2f5236..86313bb351e 100644
--- a/aten/src/ATen/native/cuda/Unique.cu
+++ b/aten/src/ATen/native/cuda/Unique.cu
@@ -29,7 +29,8 @@
 
 #include <ATen/native/cuda/UniqueCub.cuh>
 
-namespace at::native {
+namespace at {
+namespace native{
 
 namespace {
 
@@ -232,4 +233,6 @@ unique_consecutive_cuda(const Tensor& self, const bool return_inverse, const boo
   return unique_dim_consecutive_cuda(self, dim.value(), return_inverse, return_counts);
 }
 
-}  // namespace at::native
+}  // namespace native
+}  // namespace at
+
diff --git a/aten/src/ATen/native/cuda/UniqueCub.cu b/aten/src/ATen/native/cuda/UniqueCub.cu
index 5e259048fe8..83a2dccb19e 100644
--- a/aten/src/ATen/native/cuda/UniqueCub.cu
+++ b/aten/src/ATen/native/cuda/UniqueCub.cu
@@ -16,7 +16,7 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native::internal {
+namespace at { namespace native { namespace internal {
 
 namespace {
 
@@ -338,4 +338,4 @@ INSTANTIATE_UNIQUE_CUDA_TEMPLATE(at::Half);
 
 #undef INSTANTIATE
 
-} // namespace at::native::internal
+}}} // namespace at::native::internal
diff --git a/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu b/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu
index 3589e06b52f..49387c70d37 100644
--- a/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu
@@ -16,7 +16,7 @@
 #include <ATen/ops/upsample_bicubic2d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t>
@@ -294,4 +294,4 @@ TORCH_IMPL_FUNC(upsample_bicubic2d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu b/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu
index e7d1bb02eeb..586bc78ed5f 100644
--- a/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu
@@ -27,7 +27,7 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t>
@@ -914,4 +914,4 @@ TORCH_IMPL_FUNC(_upsample_bicubic2d_aa_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UpSampleLinear1d.cu b/aten/src/ATen/native/cuda/UpSampleLinear1d.cu
index fd29c2ec855..84489f00a1e 100644
--- a/aten/src/ATen/native/cuda/UpSampleLinear1d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleLinear1d.cu
@@ -19,7 +19,7 @@
 #include <ATen/ops/upsample_linear1d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t>
@@ -227,4 +227,4 @@ TORCH_IMPL_FUNC(upsample_linear1d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UpSampleNearest1d.cu b/aten/src/ATen/native/cuda/UpSampleNearest1d.cu
index 26048202a45..108e5749632 100644
--- a/aten/src/ATen/native/cuda/UpSampleNearest1d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleNearest1d.cu
@@ -18,7 +18,7 @@
 #include <ATen/ops/_upsample_nearest_exact1d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 #define MAX_THREADS 512
@@ -235,4 +235,4 @@ TORCH_IMPL_FUNC(_upsample_nearest_exact1d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, scales);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UpSampleNearest2d.cu b/aten/src/ATen/native/cuda/UpSampleNearest2d.cu
index 5f4f4100da5..ab2a9b10131 100644
--- a/aten/src/ATen/native/cuda/UpSampleNearest2d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleNearest2d.cu
@@ -22,7 +22,7 @@
 #include <ATen/ops/upsample_nearest2d_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 #define MAX_THREADS 512
@@ -484,4 +484,4 @@ TORCH_IMPL_FUNC(_upsample_nearest_exact2d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, scales_h, scales_w);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UpSampleNearest3d.cu b/aten/src/ATen/native/cuda/UpSampleNearest3d.cu
index d06fc571a2d..9b1161ae883 100644
--- a/aten/src/ATen/native/cuda/UpSampleNearest3d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleNearest3d.cu
@@ -24,7 +24,7 @@
 #include <ATen/ops/_upsample_nearest_exact3d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 #define MAX_THREADS 512
@@ -336,4 +336,4 @@ TORCH_IMPL_FUNC(_upsample_nearest_exact3d_backward_out_cuda) (
 using at::native::upsample::compute_output_size;
 using at::native::upsample_cuda::get_scale_value;
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu b/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu
index d443082c4a0..f754bc05490 100644
--- a/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu
@@ -21,7 +21,7 @@
 #include <ATen/ops/upsample_trilinear3d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 __device__ __forceinline__ size_t
@@ -393,4 +393,4 @@ TORCH_IMPL_FUNC(upsample_trilinear3d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu b/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu
index 9f7fb1cbd0a..4b042665959 100644
--- a/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu
+++ b/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu
@@ -2,7 +2,7 @@
 #include <ATen/native/sparse/ValidateCompressedIndicesCommon.h>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -26,4 +26,4 @@ void _validate_compressed_sparse_indices_cuda(
       is_crow, cidx, idx, cdim, dim, nnz);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/WeightNorm.cu b/aten/src/ATen/native/cuda/WeightNorm.cu
index d492633a1bb..e50e55eddf7 100644
--- a/aten/src/ATen/native/cuda/WeightNorm.cu
+++ b/aten/src/ATen/native/cuda/WeightNorm.cu
@@ -19,7 +19,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 // Block size for weight_norm_*_first_dim_kernel.
@@ -522,4 +522,4 @@ std::tuple<Tensor, Tensor> weight_norm_backward_cuda
 #undef TILE_W
 #undef TILE_H
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ZetaKernel.cu b/aten/src/ATen/native/cuda/ZetaKernel.cu
index 7459504f508..3d47994e8fa 100644
--- a/aten/src/ATen/native/cuda/ZetaKernel.cu
+++ b/aten/src/ATen/native/cuda/ZetaKernel.cu
@@ -7,7 +7,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 
 /*
@@ -36,4 +36,4 @@ void zeta_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(zeta_stub, &zeta_kernel_cuda);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/airy_ai.cu b/aten/src/ATen/native/cuda/airy_ai.cu
index 35e6b002260..43d822dfd45 100644
--- a/aten/src/ATen/native/cuda/airy_ai.cu
+++ b/aten/src/ATen/native/cuda/airy_ai.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char airy_ai_name[] = "airy_ai_forward";
 
@@ -39,4 +39,4 @@ void airy_ai_kernel_cuda(TensorIteratorBase& iterator) {
 } // anonymous namespace
 
 REGISTER_DISPATCH(special_airy_ai_stub, &airy_ai_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/bessel_j0.cu b/aten/src/ATen/native/cuda/bessel_j0.cu
index 2ebfe676e50..5a80e87d411 100644
--- a/aten/src/ATen/native/cuda/bessel_j0.cu
+++ b/aten/src/ATen/native/cuda/bessel_j0.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char bessel_j0_name[] = "bessel_j0_forward";
 
@@ -39,4 +39,4 @@ void bessel_j0_kernel_cuda(TensorIteratorBase& iterator) {
 } // anonymous namespace
 
 REGISTER_DISPATCH(special_bessel_j0_stub, &bessel_j0_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/bessel_j1.cu b/aten/src/ATen/native/cuda/bessel_j1.cu
index 42bd43321f4..7ea6fef7009 100644
--- a/aten/src/ATen/native/cuda/bessel_j1.cu
+++ b/aten/src/ATen/native/cuda/bessel_j1.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char bessel_j1_name[] = "bessel_j1_forward";
 
@@ -39,4 +39,4 @@ void bessel_j1_kernel_cuda(TensorIteratorBase& iterator) {
 } // anonymous namespace
 
 REGISTER_DISPATCH(special_bessel_j1_stub, &bessel_j1_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/bessel_y0.cu b/aten/src/ATen/native/cuda/bessel_y0.cu
index 631031d4e26..b2cf794bd50 100644
--- a/aten/src/ATen/native/cuda/bessel_y0.cu
+++ b/aten/src/ATen/native/cuda/bessel_y0.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char bessel_y0_name[] = "bessel_y0_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_bessel_y0_stub, &bessel_y0_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/bessel_y1.cu b/aten/src/ATen/native/cuda/bessel_y1.cu
index 1375061e43e..dbb1477709c 100644
--- a/aten/src/ATen/native/cuda/bessel_y1.cu
+++ b/aten/src/ATen/native/cuda/bessel_y1.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char bessel_y1_name[] = "bessel_y1_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_bessel_y1_stub, &bessel_y1_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu
index 7736d20e018..fc64f1b01ab 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_t_name[] = "chebyshev_polynomial_t_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_t_stub, &chebyshev_polynomial_t_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu
index 412479e11f4..2600c6a5906 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_u_name[] = "chebyshev_polynomial_u_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_u_stub, &chebyshev_polynomial_u_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu
index ca2e534e641..ad56cfb9e34 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_v_name[] = "chebyshev_polynomial_v_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_v_stub, &chebyshev_polynomial_v_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu
index 9d5a0e3a7bd..09eebc2343b 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_w_name[] = "chebyshev_polynomial_w_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_w_stub, &chebyshev_polynomial_w_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu b/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu
index ec8ac6b4f26..a887760b191 100644
--- a/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu
+++ b/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu
@@ -6,7 +6,7 @@
 #include <ATen/native/cuda/MultiTensorApply.cuh>
 #include <vector>
 
-namespace at::native {
+namespace at { namespace native {
 
 void _fused_adam_amsgrad_cuda_impl_(
     at::TensorList params,
@@ -49,4 +49,4 @@ void _fused_adam_amsgrad_cuda_impl_(
         });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/fused_adam_impl.cu b/aten/src/ATen/native/cuda/fused_adam_impl.cu
index d91be6bfc99..5b9684c55e6 100644
--- a/aten/src/ATen/native/cuda/fused_adam_impl.cu
+++ b/aten/src/ATen/native/cuda/fused_adam_impl.cu
@@ -6,7 +6,7 @@
 #include <ATen/native/cuda/MultiTensorApply.cuh>
 #include <vector>
 
-namespace at::native {
+namespace at { namespace native {
 
 void _fused_adam_cuda_impl_(
     at::TensorList params,
@@ -48,4 +48,4 @@ void _fused_adam_cuda_impl_(
         });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/group_norm_kernel.cu b/aten/src/ATen/native/cuda/group_norm_kernel.cu
index 04bdca8ad11..87d21858d37 100644
--- a/aten/src/ATen/native/cuda/group_norm_kernel.cu
+++ b/aten/src/ATen/native/cuda/group_norm_kernel.cu
@@ -21,7 +21,7 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -993,4 +993,4 @@ void GroupNormBackwardKernelImpl(
 REGISTER_DISPATCH(GroupNormKernel, &GroupNormKernelImpl);
 REGISTER_DISPATCH(GroupNormBackwardKernel, &GroupNormBackwardKernelImpl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/hermite_polynomial_h.cu b/aten/src/ATen/native/cuda/hermite_polynomial_h.cu
index f53253bcd09..7174c0b23c4 100644
--- a/aten/src/ATen/native/cuda/hermite_polynomial_h.cu
+++ b/aten/src/ATen/native/cuda/hermite_polynomial_h.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char hermite_polynomial_h_name[] = "hermite_polynomial_h_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(hermite_polynomial_h_stub, &hermite_polynomial_h_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/hermite_polynomial_he.cu b/aten/src/ATen/native/cuda/hermite_polynomial_he.cu
index bab37656585..90b3cb3899f 100644
--- a/aten/src/ATen/native/cuda/hermite_polynomial_he.cu
+++ b/aten/src/ATen/native/cuda/hermite_polynomial_he.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char hermite_polynomial_he_name[] = "hermite_polynomial_he_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(hermite_polynomial_he_stub, &hermite_polynomial_he_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu b/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu
index a98336dfcb6..057d45ef8ae 100644
--- a/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu
+++ b/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char laguerre_polynomial_l_name[] = "laguerre_polynomial_l_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(laguerre_polynomial_l_stub, &laguerre_polynomial_l_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/layer_norm_kernel.cu b/aten/src/ATen/native/cuda/layer_norm_kernel.cu
index 6d8008230f8..9c8814d4088 100644
--- a/aten/src/ATen/native/cuda/layer_norm_kernel.cu
+++ b/aten/src/ATen/native/cuda/layer_norm_kernel.cu
@@ -28,7 +28,7 @@
 #include <c10/util/env.h>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 
@@ -1446,4 +1446,4 @@ std::tuple<Tensor, Tensor, Tensor> layer_norm_backward_cuda(
 
 REGISTER_DISPATCH(LayerNormKernel, &LayerNormKernelImpl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/legendre_polynomial_p.cu b/aten/src/ATen/native/cuda/legendre_polynomial_p.cu
index 9f5efc9b451..dbf7efefdc5 100644
--- a/aten/src/ATen/native/cuda/legendre_polynomial_p.cu
+++ b/aten/src/ATen/native/cuda/legendre_polynomial_p.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             const char legendre_polynomial_p_name[] = "legendre_polynomial_p_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(legendre_polynomial_p_stub, &legendre_polynomial_p_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp
index 87260196a40..32901bae912 100644
--- a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp
+++ b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp
@@ -66,7 +66,7 @@ const bool use_magma_ = false;
 
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 #if defined(BUILD_LAZY_CUDA_LINALG)
 // All registrations with PyTorch runtime should be done dynamically
 // so if library is lazy loaded it must not export anything, otherwise
@@ -2767,6 +2767,6 @@ struct DispatchInitializer {
 
 }  // namespace lazy_linalg
 #endif
-}  // namespace at::native
+}}  // namespace at::native
 
 #undef ALLOCATE_ARRAY
diff --git a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp
index 823c20b1d48..5c4b32ac915 100644
--- a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp
+++ b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp
@@ -27,7 +27,7 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 cublasOperation_t to_cublas(TransposeType trans) {
   switch (trans) {
@@ -1743,4 +1743,4 @@ void lu_solve_looped_cusolver(const Tensor& LU, const Tensor& pivots, const Tens
 
 #endif  // USE_CUSOLVER
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/modified_bessel_i0.cu b/aten/src/ATen/native/cuda/modified_bessel_i0.cu
index 9f1f3ba98c6..3226e95842a 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_i0.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_i0.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_i0_name[] = "modified_bessel_i0_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_i0_stub, &modified_bessel_i0_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/modified_bessel_i1.cu b/aten/src/ATen/native/cuda/modified_bessel_i1.cu
index d51e7fefb0e..54336c1c9d6 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_i1.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_i1.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_i1_name[] = "modified_bessel_i1_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_i1_stub, &modified_bessel_i1_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/modified_bessel_k0.cu b/aten/src/ATen/native/cuda/modified_bessel_k0.cu
index 574268456c8..6f6c37b4cda 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_k0.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_k0.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_k0_name[] = "modified_bessel_k0_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_k0_stub, &modified_bessel_k0_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/modified_bessel_k1.cu b/aten/src/ATen/native/cuda/modified_bessel_k1.cu
index b3720d8e1ba..24559597672 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_k1.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_k1.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_k1_name[] = "modified_bessel_k1_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_k1_stub, &modified_bessel_k1_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu b/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu
index ac2355e409a..4250ac092cd 100644
--- a/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu
+++ b/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char scaled_modified_bessel_k0_name[] = "scaled_modified_bessel_k0_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_scaled_modified_bessel_k0_stub, &scaled_modified_bessel_k0_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu b/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu
index b1d8d2a41b6..9aab1908139 100644
--- a/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu
+++ b/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char scaled_modified_bessel_k1_name[] = "scaled_modified_bessel_k1_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_scaled_modified_bessel_k1_stub, &scaled_modified_bessel_k1_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu
index d86042030cd..6ff216f265a 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_t_name[] = "shifted_chebyshev_polynomial_t_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(shifted_chebyshev_polynomial_t_stub, &shifted_chebyshev_polynomial_t_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu
index a2e2cd485fd..90f6cdd1d3d 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_u_name[] = "shifted_chebyshev_polynomial_u_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(shifted_chebyshev_polynomial_u_stub, &shifted_chebyshev_polynomial_u_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu
index 6e5404179ab..277f820cd5e 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_v_name[] = "shifted_chebyshev_polynomial_v_forward";
 
@@ -29,4 +29,4 @@ void shifted_chebyshev_polynomial_v_kernel_cuda(TensorIteratorBase& iterator) {
 } // namespace (anonymous)
 
 REGISTER_DISPATCH(shifted_chebyshev_polynomial_v_stub, &shifted_chebyshev_polynomial_v_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu
index 3bfee57d14e..59fecfde9a6 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu
@@ -8,7 +8,7 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_w_name[] = "shifted_chebyshev_polynomial_w_forward";
 
@@ -28,4 +28,4 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(shifted_chebyshev_polynomial_w_stub, &shifted_chebyshev_polynomial_w_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/spherical_bessel_j0.cu b/aten/src/ATen/native/cuda/spherical_bessel_j0.cu
index d0bf46e6539..b412dbc1f40 100644
--- a/aten/src/ATen/native/cuda/spherical_bessel_j0.cu
+++ b/aten/src/ATen/native/cuda/spherical_bessel_j0.cu
@@ -18,7 +18,7 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at { namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char spherical_bessel_j0_name[] = "spherical_bessel_j0_forward";
 
@@ -38,4 +38,4 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_spherical_bessel_j0_stub, &spherical_bessel_j0_kernel_cuda);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mkldnn/RNN.cpp b/aten/src/ATen/native/mkldnn/RNN.cpp
index bb0251bc9d6..d59ae5c57b1 100644
--- a/aten/src/ATen/native/mkldnn/RNN.cpp
+++ b/aten/src/ATen/native/mkldnn/RNN.cpp
@@ -16,7 +16,7 @@
 
 #if !AT_MKLDNN_ENABLED()
 
-namespace at::native {
+namespace at { namespace native {
 
 
 std::tuple<Tensor, Tensor, Tensor, Tensor> mkldnn_rnn_layer(
@@ -68,14 +68,14 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> mkldnn_rnn_la
 
 REGISTER_NO_CPU_DISPATCH(lstm_mkldnn_stub);
 
-} // namespace at::native
+}} // namespace at::native
 
 #else // AT_MKLDNN_EBABLED
 
 #include <ATen/native/mkldnn/MKLDNNCommon.h>
 #include <ATen/native/mkldnn/Utils.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 struct RNNParams {
   ideep::rnn_kind mode;
@@ -571,6 +571,6 @@ void lstm_mkldnn(Tensor& output, Tensor& hy, Tensor& cy,
 
 REGISTER_ALL_CPU_DISPATCH(lstm_mkldnn_stub, &lstm_mkldnn);
 
-} // namespace at::native
+}} // namespace at::native
 
 #endif // AT_MKLDNN_EBABLED
diff --git a/aten/src/ATen/native/mps/OperationUtils.mm b/aten/src/ATen/native/mps/OperationUtils.mm
index c5e8b5d1fc1..26404bb6d41 100644
--- a/aten/src/ATen/native/mps/OperationUtils.mm
+++ b/aten/src/ATen/native/mps/OperationUtils.mm
@@ -438,4 +438,4 @@ void executeMPSAllocatorCallback(void* ptr, EventType event) override { }
 
 REGISTER_MPS_ALLOCATOR_CALLBACK("mps_graph_cache_callback", MPSGraphCacheCallback);
 
-} // namespace at::native::mps
+}} // namespace at::native::mps
diff --git a/aten/src/ATen/native/mps/TensorFactory.cpp b/aten/src/ATen/native/mps/TensorFactory.cpp
index 4639546e726..9c7655974b6 100644
--- a/aten/src/ATen/native/mps/TensorFactory.cpp
+++ b/aten/src/ATen/native/mps/TensorFactory.cpp
@@ -9,7 +9,7 @@
 #include <ATen/native/Resize.h>
 #include <ATen/native/mps/Copy.h>
 #include <ATen/native/mps/TensorFactory.h>
-namespace at::native {
+namespace at { namespace native {
 
 static inline void maybe_resize_storage_mps(TensorImpl* self, uint64_t new_size) {
   if (new_size == 0) {
@@ -134,4 +134,4 @@ Tensor& set_storage_mps_(Tensor& result, Storage storage, int64_t storage_offset
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Activation.mm b/aten/src/ATen/native/mps/operations/Activation.mm
index 198c13f3330..7a75da01b80 100644
--- a/aten/src/ATen/native/mps/operations/Activation.mm
+++ b/aten/src/ATen/native/mps/operations/Activation.mm
@@ -13,7 +13,7 @@
 
 using namespace at::mps;
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor relu_mps(const Tensor& self) {
   using namespace mps;
@@ -2470,4 +2470,4 @@ Tensor hardswish_backward_mps(const Tensor& grad_output, const Tensor& self) {
   }
   return grad_input;
 }
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/AdaptivePooling.mm b/aten/src/ATen/native/mps/operations/AdaptivePooling.mm
index d90545147e3..1c5745fe4b4 100644
--- a/aten/src/ATen/native/mps/operations/AdaptivePooling.mm
+++ b/aten/src/ATen/native/mps/operations/AdaptivePooling.mm
@@ -3,7 +3,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/Pool.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void set_kernel_params
   (int64_t isizeH, int64_t isizeW,
@@ -241,4 +241,4 @@
                                            indices);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/BinaryOps.mm b/aten/src/ATen/native/mps/operations/BinaryOps.mm
index 80113706746..5c5fffc4126 100644
--- a/aten/src/ATen/native/mps/operations/BinaryOps.mm
+++ b/aten/src/ATen/native/mps/operations/BinaryOps.mm
@@ -9,7 +9,7 @@
 #include <c10/util/Optional.h>
 #include <ATen/native/BinaryOps.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 struct BinaryOpCachedGraph : public MPSCachedGraph
@@ -390,4 +390,4 @@ Tensor floor_divide_mps(const Tensor& self, const Tensor& other) {
   mps::binaryOpTensor(self, other, Scalar(1.0), output, "logaddexp2_out_mps", logaddexp2_op_block);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Blas.mm b/aten/src/ATen/native/mps/operations/Blas.mm
index a5768d0d13a..b0ba7351ce0 100644
--- a/aten/src/ATen/native/mps/operations/Blas.mm
+++ b/aten/src/ATen/native/mps/operations/Blas.mm
@@ -13,7 +13,7 @@
 #endif
 
 
-namespace at::native {
+namespace at { namespace native {
 
 
 Tensor dot_mps(
@@ -217,4 +217,4 @@ Tensor dot_mps(
   addmv_out_mps_impl(self, mat, vec, beta_, alpha_, const_cast<Tensor&>(result));
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/ConstantOps.mm b/aten/src/ATen/native/mps/operations/ConstantOps.mm
index 12e86e14c63..a7f895e4330 100644
--- a/aten/src/ATen/native/mps/operations/ConstantOps.mm
+++ b/aten/src/ATen/native/mps/operations/ConstantOps.mm
@@ -2,7 +2,7 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor& fill_scalar_mps_impl(Tensor& self, const Scalar& value) {
   using namespace mps;
@@ -116,4 +116,4 @@ bool fill_mps_tensor_(Tensor& self, uint8_t value) {
   return fill_scalar_mps_impl(self, scalar_value);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Convolution.mm b/aten/src/ATen/native/mps/operations/Convolution.mm
index 601cbaec965..c272ede6136 100644
--- a/aten/src/ATen/native/mps/operations/Convolution.mm
+++ b/aten/src/ATen/native/mps/operations/Convolution.mm
@@ -9,7 +9,7 @@
 #include <ATen/native/ConvUtils.h>
 #include <torch/library.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void fill_depthwise_conv_desc(MPSGraphDepthwiseConvolution3DOpDescriptor* descriptor_,
                     NSUInteger strideInX, NSUInteger strideInY,
@@ -618,4 +618,4 @@ Tensor mps_convolution_transpose_backward_weight(
 }
 
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Copy.mm b/aten/src/ATen/native/mps/operations/Copy.mm
index 16f5718dd29..9e4c6d456e0 100644
--- a/aten/src/ATen/native/mps/operations/Copy.mm
+++ b/aten/src/ATen/native/mps/operations/Copy.mm
@@ -3,7 +3,7 @@
 #include <ATen/native/mps/Copy.h>
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 void* pageAlignedBlockPtr(
@@ -339,4 +339,4 @@ Tensor _copy_from_mps(const at::Tensor& self, const at::Tensor& dst, bool non_bl
   return mps::mps_copy_(const_cast<Tensor&>(dst), self, non_blocking);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/CrossKernel.mm b/aten/src/ATen/native/mps/operations/CrossKernel.mm
index f140bcb8f9b..36262101314 100644
--- a/aten/src/ATen/native/mps/operations/CrossKernel.mm
+++ b/aten/src/ATen/native/mps/operations/CrossKernel.mm
@@ -3,7 +3,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/Cross.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 static const char* METAL_CROSS = R"CROSS_METAL(
 
@@ -202,4 +202,4 @@ void cross_mps_impl(const Tensor& out, const Tensor& input, const Tensor& other,
 
 REGISTER_DISPATCH(cross_stub, &cross_mps_impl);
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Distributions.mm b/aten/src/ATen/native/mps/operations/Distributions.mm
index f047b9e524c..c497a5304c7 100644
--- a/aten/src/ATen/native/mps/operations/Distributions.mm
+++ b/aten/src/ATen/native/mps/operations/Distributions.mm
@@ -7,7 +7,7 @@
 #include <ATen/mps/MPSGeneratorImpl.h>
 #include <ATen/native/TensorFactories.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 struct RandomCachedGraph : public MPSCachedGraph
@@ -635,4 +635,4 @@ Tensor multinomial_mps(
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Eye.mm b/aten/src/ATen/native/mps/operations/Eye.mm
index 88edc9b1aa3..12896881a51 100644
--- a/aten/src/ATen/native/mps/operations/Eye.mm
+++ b/aten/src/ATen/native/mps/operations/Eye.mm
@@ -30,7 +30,7 @@
 //
 
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor& eye_out_mps(int64_t n, Tensor& result) {
   // the default value of `m` equals to `n`
@@ -114,4 +114,4 @@
 }
 
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Indexing.mm b/aten/src/ATen/native/mps/operations/Indexing.mm
index 8522ac92027..2437ed21747 100644
--- a/aten/src/ATen/native/mps/operations/Indexing.mm
+++ b/aten/src/ATen/native/mps/operations/Indexing.mm
@@ -27,7 +27,7 @@
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 static
 bool dispatchIndexKernel(TensorIteratorBase& iter,
@@ -944,4 +944,4 @@ Tensor embedding_dense_backward_mps(
 
 REGISTER_DISPATCH(index_stub, &index_kernel_mps);
 REGISTER_DISPATCH(index_put_stub, &index_put_kernel_mps);
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Inverse.mm b/aten/src/ATen/native/mps/operations/Inverse.mm
index 519de6afa3b..d059efbbfef 100644
--- a/aten/src/ATen/native/mps/operations/Inverse.mm
+++ b/aten/src/ATen/native/mps/operations/Inverse.mm
@@ -5,7 +5,7 @@
 #include <c10/util/Optional.h>
 
 
-namespace at::native {
+namespace at { namespace native {
 
 TORCH_IMPL_FUNC(linalg_inv_ex_out_mps)(const Tensor& A, bool check_errors, const Tensor& result, const Tensor& info)
 {
@@ -87,4 +87,4 @@
     }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Linear.mm b/aten/src/ATen/native/mps/operations/Linear.mm
index 529c26ded00..a82cb036dfa 100644
--- a/aten/src/ATen/native/mps/operations/Linear.mm
+++ b/aten/src/ATen/native/mps/operations/Linear.mm
@@ -2,7 +2,7 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 using namespace mps;
 
@@ -356,4 +356,4 @@ Tensor _mps_linear_backward_input(
   return std::tuple<Tensor, Tensor, Tensor>{grad_input, grad_weight, grad_bias};
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/LinearAlgebra.mm b/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
index 6e3f1bc594a..c3e26921b4e 100644
--- a/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
+++ b/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
@@ -4,7 +4,7 @@
 #include <ATen/native/LinearAlgebraUtils.h>
 #include <ATen/native/Resize.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 /*
  * Helper functions to be used for mm/addmm for detecting the Transpositions
@@ -842,4 +842,4 @@ Tensor linalg_solve_triangular_mps(const Tensor& A, const Tensor& B, bool upper,
   result.copy_(out);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/LossOps.mm b/aten/src/ATen/native/mps/operations/LossOps.mm
index 1a8c689003b..e6485e71739 100644
--- a/aten/src/ATen/native/mps/operations/LossOps.mm
+++ b/aten/src/ATen/native/mps/operations/LossOps.mm
@@ -2,7 +2,7 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 string reductionToString(int64_t reduction)
@@ -1552,4 +1552,4 @@ Tensor nll_loss2d_backward_mps(
 }
 
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Normalization.mm b/aten/src/ATen/native/mps/operations/Normalization.mm
index 34dd5f75211..2ec79439fa0 100644
--- a/aten/src/ATen/native/mps/operations/Normalization.mm
+++ b/aten/src/ATen/native/mps/operations/Normalization.mm
@@ -10,7 +10,7 @@
 #include <ATen/native/layer_norm.h>
 #include <torch/library.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 void get_shapes(MPSShape* input_shape_readonly,
                 NSMutableArray<NSNumber*>* &input_shape,
@@ -1308,4 +1308,4 @@ string get_mem_string(c10::MemoryFormat memory_format) {
 
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Pad.mm b/aten/src/ATen/native/mps/operations/Pad.mm
index d152dbe5eef..76008a8139a 100644
--- a/aten/src/ATen/native/mps/operations/Pad.mm
+++ b/aten/src/ATen/native/mps/operations/Pad.mm
@@ -3,7 +3,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <c10/util/Optional.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 // Pad operations (1D/2D/3D forward and backward)
@@ -397,4 +397,4 @@ Tensor constant_pad_nd_mps(const Tensor& self, IntArrayRef pad, const Scalar& va
   return mps::pad_out_template(output, self, pad, c10::nullopt, MPSGraphPaddingModeConstant, value.toDouble(), __func__);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/PointwiseOps.mm b/aten/src/ATen/native/mps/operations/PointwiseOps.mm
index 92109c64caf..b6dc837d8be 100644
--- a/aten/src/ATen/native/mps/operations/PointwiseOps.mm
+++ b/aten/src/ATen/native/mps/operations/PointwiseOps.mm
@@ -2,7 +2,7 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 // scope the MPS's internal methods to not expose them to at::native
 namespace mps {
 
@@ -114,4 +114,4 @@
   mps::addc_mul_div_out_mps(self, tensor1, tensor2, value, const_cast<Tensor&>(output), true, "addcdiv_out_mps");
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Pooling.mm b/aten/src/ATen/native/mps/operations/Pooling.mm
index ff26ff83518..660af7a73d7 100644
--- a/aten/src/ATen/native/mps/operations/Pooling.mm
+++ b/aten/src/ATen/native/mps/operations/Pooling.mm
@@ -3,7 +3,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/Pool.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 struct PoolingCachedGraph : public MPSCachedGraph
@@ -434,4 +434,4 @@ Tensor mps_max_pool2d_backward(
                            {1, 1}, ceil_mode, count_include_pad, divisor_override, "avg_pool2d_backward");
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/RangeFactories.mm b/aten/src/ATen/native/mps/operations/RangeFactories.mm
index 9cfd1423621..3f763a9bae4 100644
--- a/aten/src/ATen/native/mps/operations/RangeFactories.mm
+++ b/aten/src/ATen/native/mps/operations/RangeFactories.mm
@@ -10,7 +10,7 @@
 #include <cmath>
 #include <limits>
 
-namespace at::native {
+namespace at { namespace native {
 
 namespace {
 struct RangeCachedGraph : public mps::MPSCachedGraph {
@@ -268,4 +268,4 @@
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/ReduceOps.mm b/aten/src/ATen/native/mps/operations/ReduceOps.mm
index 5f8c0f64d8b..5e0d88ed831 100644
--- a/aten/src/ATen/native/mps/operations/ReduceOps.mm
+++ b/aten/src/ATen/native/mps/operations/ReduceOps.mm
@@ -12,7 +12,7 @@
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 typedef MPSGraphTensor* (^NormOpBlock)(mps::MPSBinaryCachedGraph*, MPSGraphTensor*, MPSGraphTensor*);
 #define NormOpFn(graph, primary, secondary) MPSGraphTensor* (mps::MPSBinaryCachedGraph* graph, MPSGraphTensor* primary, MPSGraphTensor* secondary)
@@ -2056,4 +2056,4 @@ void median_out_mps(
   return std::tuple<Tensor&, Tensor&>{values, indices};
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Repeat.mm b/aten/src/ATen/native/mps/operations/Repeat.mm
index d2155d2e7fe..3835b439177 100644
--- a/aten/src/ATen/native/mps/operations/Repeat.mm
+++ b/aten/src/ATen/native/mps/operations/Repeat.mm
@@ -15,7 +15,7 @@
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor permute_mps(const Tensor& self, IntArrayRef dims) {
   auto nDims = self.dim();
diff --git a/aten/src/ATen/native/mps/operations/RnnOps.mm b/aten/src/ATen/native/mps/operations/RnnOps.mm
index 9e59a6cf702..c205b049156 100644
--- a/aten/src/ATen/native/mps/operations/RnnOps.mm
+++ b/aten/src/ATen/native/mps/operations/RnnOps.mm
@@ -12,7 +12,7 @@
 #import <MetalPerformanceShadersGraph/MPSGraphRNNOps.h>
 #include <torch/library.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 std::vector<long long> getTensorShape(MPSGraphTensor* mpsTensor) {
     std::vector<long long> output_dimensions = {};
diff --git a/aten/src/ATen/native/mps/operations/Scalar.mm b/aten/src/ATen/native/mps/operations/Scalar.mm
index 73e099d1476..a6e70ea1bdd 100644
--- a/aten/src/ATen/native/mps/operations/Scalar.mm
+++ b/aten/src/ATen/native/mps/operations/Scalar.mm
@@ -16,7 +16,7 @@
 
 using namespace at::mps;
 
-namespace at::native {
+namespace at { namespace native {
 
 Scalar _local_scalar_dense_mps(const Tensor& self) {
   Scalar r;
@@ -34,4 +34,4 @@ Scalar _local_scalar_dense_mps(const Tensor& self) {
 }
 
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/ScatterGather.mm b/aten/src/ATen/native/mps/operations/ScatterGather.mm
index 62ae308cc25..7f3ac74e0f5 100644
--- a/aten/src/ATen/native/mps/operations/ScatterGather.mm
+++ b/aten/src/ATen/native/mps/operations/ScatterGather.mm
@@ -2,7 +2,7 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 TORCH_IMPL_FUNC(gather_out_mps)
 (const Tensor & self_arg,
@@ -420,4 +420,4 @@
   scatter_mps_general(self, dim, index, src, output, "scatter_add_mps_out", "add");
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Shape.mm b/aten/src/ATen/native/mps/operations/Shape.mm
index a4f70fe68ff..a8aafc8de9f 100644
--- a/aten/src/ATen/native/mps/operations/Shape.mm
+++ b/aten/src/ATen/native/mps/operations/Shape.mm
@@ -7,7 +7,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // Produces a shape with the `dim` dimension set to 0.
 std::vector<int64_t> getTopK0Shape(IntArrayRef sizes, const int64_t dim_) {
@@ -445,4 +445,4 @@ void check_shape_except_dim(const Tensor &first, const Tensor &second,
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/SoftMax.mm b/aten/src/ATen/native/mps/operations/SoftMax.mm
index 59d3fd61eea..c9277b65adb 100644
--- a/aten/src/ATen/native/mps/operations/SoftMax.mm
+++ b/aten/src/ATen/native/mps/operations/SoftMax.mm
@@ -13,7 +13,7 @@
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 #endif
 
-namespace at::native {
+namespace at { namespace native {
 
 void get_shapes(MPSShape* input_shape_readonly,
                 NSMutableArray<NSNumber*>* &input_shape,
@@ -271,4 +271,4 @@ void get_shapes(MPSShape* input_shape_readonly,
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Sort.mm b/aten/src/ATen/native/mps/operations/Sort.mm
index 4b3bb692ac0..daaf6765748 100644
--- a/aten/src/ATen/native/mps/operations/Sort.mm
+++ b/aten/src/ATen/native/mps/operations/Sort.mm
@@ -7,7 +7,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 // sort
 TORCH_IMPL_FUNC(sort_stable_out_mps)
diff --git a/aten/src/ATen/native/mps/operations/SummaryOps.mm b/aten/src/ATen/native/mps/operations/SummaryOps.mm
index 8677e14b8b7..9e802649294 100644
--- a/aten/src/ATen/native/mps/operations/SummaryOps.mm
+++ b/aten/src/ATen/native/mps/operations/SummaryOps.mm
@@ -2,7 +2,7 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 Tensor& bincount_mps_impl(const Tensor& self,
                           const Tensor& weights,
@@ -150,4 +150,4 @@ Tensor _bincount_mps(const Tensor& self, const c10::optional<Tensor>& weights_op
   return bincount_mps_impl(self, weights_, output);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/TensorCompare.mm b/aten/src/ATen/native/mps/operations/TensorCompare.mm
index 4f8def1cbb7..821aae80958 100644
--- a/aten/src/ATen/native/mps/operations/TensorCompare.mm
+++ b/aten/src/ATen/native/mps/operations/TensorCompare.mm
@@ -4,7 +4,7 @@
 #include <ATen/native/TensorCompare.h>
 #include <ATen/native/Resize.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 struct CachedGraph : public MPSCachedGraph
@@ -541,4 +541,4 @@ Tensor where_mps(const Tensor& condition,
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/TriangularOps.mm b/aten/src/ATen/native/mps/operations/TriangularOps.mm
index a4b0db98b0f..d2f1429c4d2 100644
--- a/aten/src/ATen/native/mps/operations/TriangularOps.mm
+++ b/aten/src/ATen/native/mps/operations/TriangularOps.mm
@@ -11,7 +11,7 @@
 
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 
-namespace at::native {
+namespace at { namespace native {
 
 TORCH_IMPL_FUNC(triu_mps_out)
 (const Tensor& self,
@@ -179,4 +179,4 @@
 
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/UnaryOps.mm b/aten/src/ATen/native/mps/operations/UnaryOps.mm
index 210c214834a..d425344eb6c 100644
--- a/aten/src/ATen/native/mps/operations/UnaryOps.mm
+++ b/aten/src/ATen/native/mps/operations/UnaryOps.mm
@@ -3,7 +3,7 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 typedef MPSGraphTensor* (^UnaryOpBlock)(MPSGraph*, MPSGraphTensor*);
@@ -285,4 +285,4 @@ void unary_op(const Tensor& self, const Tensor& output, std::string op_name, Una
     });
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/Unique.mm b/aten/src/ATen/native/mps/operations/Unique.mm
index eac16a74564..190cb018ccf 100644
--- a/aten/src/ATen/native/mps/operations/Unique.mm
+++ b/aten/src/ATen/native/mps/operations/Unique.mm
@@ -4,7 +4,7 @@
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 #include <ATen/native/Resize.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 struct UniqueCachedGraph : public MPSCachedGraph
@@ -358,4 +358,4 @@ void runUniqueGraph(UniqueCachedGraph *uniqueGraph, const Tensor& input, Tensor&
   return _unique_impl_mps(self, return_inverse, return_counts, false, c10::nullopt);
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/UpSample.mm b/aten/src/ATen/native/mps/operations/UpSample.mm
index 3b781dea08f..0ff02d3e27f 100644
--- a/aten/src/ATen/native/mps/operations/UpSample.mm
+++ b/aten/src/ATen/native/mps/operations/UpSample.mm
@@ -4,7 +4,7 @@
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 #include <ATen/native/UpSample.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 // Upsampling operations (1D/2D forward and backward)
@@ -397,4 +397,4 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:
   }
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mps/operations/View.mm b/aten/src/ATen/native/mps/operations/View.mm
index b6df6e3f654..63b9e799475 100644
--- a/aten/src/ATen/native/mps/operations/View.mm
+++ b/aten/src/ATen/native/mps/operations/View.mm
@@ -7,7 +7,7 @@
 #include <torch/library.h>
 #include <ATen/mps/IndexKernels.h>
 
-namespace at::native {
+namespace at { namespace native {
 namespace mps {
 
 struct ViewCachedGraph : public MPSCachedGraph
@@ -948,4 +948,4 @@ Tensor as_strided_tensorimpl_mps(const Tensor& self, IntArrayRef size, IntArrayR
   return result;
 }
 
-} // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp b/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp
index 5dccb15060f..041613f7a43 100644
--- a/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp
+++ b/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp
@@ -612,7 +612,7 @@ void spmm(
 
   // CUDA < 11.0 doesn't support 64-bit indices and doesn't raise an error about this
   // silently returning incorrect results
-#if defined(USE_ROCM)
+#if defined(USE_ROCM) || (defined(CUDA_VERSION) && CUDA_VERSION < 11000)  
   auto mat1_32 = at::native::_sparse_csr_tensor_unsafe(
       mat1.crow_indices().to(kInt),
       mat1.col_indices().to(kInt),
@@ -689,7 +689,8 @@ void spgemm(
     const Scalar& beta,
     const Scalar& alpha,
     const at::sparse_csr::SparseCsrTensor& C) {
-#if defined(USE_ROCM) && ROCM_VERSION < 50200
+// #if defined(USE_ROCM) && ROCM_VERSION < 50200
+#if defined(CUDA_VERSION) && CUDA_VERSION < 11000
   TORCH_CHECK(
       false,
       "Calling addmm with sparse GPU tensors requires compiling ",
diff --git a/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu b/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu
index 91466770a92..98904bbb677 100644
--- a/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu
+++ b/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu
@@ -748,9 +748,11 @@ cudaDataType getTensorCudaDataType(Tensor self) {
 #endif
 
 Tensor& bmm_out_sparse_cuda(const SparseTensor& self, const Tensor& mat2, Tensor& result) {
-#if defined(_MSC_VER) && (CUSPARSE_VERSION < 11000)
+#if defined(USE_ROCM)
+  TORCH_CHECK(false, "bmm sparse-dense is not supported on HIP");
+#elif defined(_MSC_VER) && (CUSPARSE_VERSION < 11000)
   TORCH_CHECK(false, "bmm sparse-dense CUDA is not supported on Windows with cuda before 11.0");
-#elif defined(USE_ROCM) || (defined(CUDART_VERSION) && (CUDART_VERSION >= 10010))  // linux cuda >= 10.1 or windows cuda >= 11.0
+#elif defined(CUDART_VERSION) && (CUDART_VERSION >= 10010)  // linux cuda >= 10.1 or windows cuda >= 11.0
 
   TORCH_CHECK(!mat2.is_sparse(), "bmm_sparse: Tensor 'mat2' must be dense");
   TORCH_CHECK(self.dense_dim() == 0, "bmm_sparse: Tensor 'self' must have 0 dense dims, but has ", self.dense_dim());
@@ -832,7 +834,10 @@ Tensor& bmm_out_sparse_cuda(const SparseTensor& self, const Tensor& mat2, Tensor
 
   // See Note [Enabling Deterministic Operations]
   bool deterministic =  globalContext().deterministicAlgorithms();
-  cusparseSpMMAlg_t mm_alg = deterministic ? CUSPARSE_SPMM_COO_ALG2 : CUSPARSE_SPMM_COO_ALG1;
+  // https://petsc.org/release/src/mat/impls/aij/seq/seqcusparse/aijcusparse.cu.html
+  int CUSPARSE_SPMM_COO_ALG2 = 2;
+  int CUSPARSE_SPMM_COO_ALG1 = 1;
+  cusparseSpMMAlg_t mm_alg = deterministic ? (cusparseSpMMAlg_t)CUSPARSE_SPMM_COO_ALG2 : (cusparseSpMMAlg_t)CUSPARSE_SPMM_COO_ALG1;
 
   // Iterate through each set of 2D matrices within the 3D
   // tensor inputs, performing a matrix multiply with each
diff --git a/third_party/nvfuser/CMakeLists.txt b/third_party/nvfuser/CMakeLists.txt
index 6dec9136271..126422be5fc 100644
--- a/third_party/nvfuser/CMakeLists.txt
+++ b/third_party/nvfuser/CMakeLists.txt
@@ -317,7 +317,12 @@ if(BUILD_TEST AND USE_CUDA)
   target_include_directories(${NVFUSER_TESTS} PRIVATE "${NVFUSER_ROOT}" "${TORCH_ROOT}/torch/csrc/api/include/")
   target_link_libraries(${NVFUSER_TESTS} PRIVATE ${NVFUSER_CODEGEN} torch ${TORCHLIB_FLAVOR} gtest_main gmock_main)
   if(NOT MSVC)
-    target_compile_options(${NVFUSER_TESTS} PRIVATE -Wno-unused-variable)
+    if (USE_CUDA)
+      # see: nvvc doesn't support -Wno-unused-variable
+      target_compile_options(${NVFUSER_TESTS} PRIVATE "")
+    else()
+      target_compile_options(${NVFUSER_TESTS} PRIVATE -Wno-unused-variable)
+    endif()
   endif()
 
   install(TARGETS ${NVFUSER_TESTS} DESTINATION bin)
diff --git a/third_party/nvfuser/csrc/executor.cpp b/third_party/nvfuser/csrc/executor.cpp
index 0ab2951bda6..321b3386859 100644
--- a/third_party/nvfuser/csrc/executor.cpp
+++ b/third_party/nvfuser/csrc/executor.cpp
@@ -25,7 +25,7 @@
 #include <cmath>
 #include <fstream>
 
-namespace torch {
+namespace torch { 
 namespace jit {
 namespace fuser {
 namespace cuda {
@@ -363,7 +363,7 @@ void fillTensorWithNan(at::Tensor& t) {
       t.fill_(0x7FFFFFFF);
       break;
     case at::ScalarType::Long:
-      t.fill_(0x7FFFFFFFFFFFFFFFL);
+      t.fill_((int64_t)0x7FFFFFFFFFFFFFFFl);
       break;
     case at::ScalarType::Bool:
       t.fill_(true);
-- 
2.17.2 (Apple Git-113)


From dd8ff4d221bf7f1f8510d6fea9905636800e0b06 Mon Sep 17 00:00:00 2001
From: Orlando Ding <xiandao.airs@gmail.com>
Date: Sat, 18 Mar 2023 14:16:01 -0700
Subject: [PATCH 5/5] orlando - for updates implementation

---
 torch/utils/cpp_extension.py | 14 ++++++++++++--
 1 file changed, 12 insertions(+), 2 deletions(-)

diff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py
index 11b233f2712..76618bc55d2 100644
--- a/torch/utils/cpp_extension.py
+++ b/torch/utils/cpp_extension.py
@@ -538,7 +538,8 @@ class BuildExtension(build_ext):
             # overriding the option if the user explicitly passed it.
             cpp_format_prefix = '/{}:' if self.compiler.compiler_type == 'msvc' else '-{}='
             cpp_flag_prefix = cpp_format_prefix.format('std')
-            cpp_flag = cpp_flag_prefix + 'c++17'
+            import platform
+            cpp_flag = cpp_flag_prefix + 'c++17' if not platform.platform().startswith('macOS-10.13.6') else cpp_flag_prefix + 'c++14'
             if not any(flag.startswith(cpp_flag_prefix) for flag in cflags):
                 cflags.append(cpp_flag)
 
@@ -2116,11 +2117,20 @@ def _write_ninja_file(path,
             nvcc = _join_cuda_home('bin', 'nvcc')
         config.append(f'nvcc = {nvcc}')
 
+    def replace_std17_with_std14(options):
+        options = [c for c in options if c != "-std=c++17"]
+        options.append("-std=c++14")
+        return options
+
     if IS_HIP_EXTENSION:
         post_cflags = COMMON_HIP_FLAGS + post_cflags
     flags = [f'cflags = {" ".join(cflags)}']
+    # orlando: customized for cuda version, filtering -std=c++17
+    post_cflags = replace_std17_with_std14(post_cflags)
     flags.append(f'post_cflags = {" ".join(post_cflags)}')
-    if with_cuda:
+    if with_cuda:	
+        # orlando: customized for cuda version, filtering -std=c++17
+        cuda_post_cflags = replace_std17_with_std14(cuda_post_cflags)
         flags.append(f'cuda_cflags = {" ".join(cuda_cflags)}')
         flags.append(f'cuda_post_cflags = {" ".join(cuda_post_cflags)}')
     flags.append(f'cuda_dlink_post_cflags = {" ".join(cuda_dlink_post_cflags)}')
-- 
2.17.2 (Apple Git-113)

